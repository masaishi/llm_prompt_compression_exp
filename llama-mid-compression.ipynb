{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "690aa1cc",
   "metadata": {
    "papermill": {
     "duration": 0.010932,
     "end_time": "2024-05-21T21:00:39.794613",
     "exception": false,
     "start_time": "2024-05-21T21:00:39.783681",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Experiment of Calculation amount Reduction Method Using embed_compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1983513b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-21T21:00:39.816428Z",
     "iopub.status.busy": "2024-05-21T21:00:39.816114Z",
     "iopub.status.idle": "2024-05-21T21:04:37.600612Z",
     "shell.execute_reply": "2024-05-21T21:04:37.599325Z"
    },
    "papermill": {
     "duration": 237.798293,
     "end_time": "2024-05-21T21:04:37.603348",
     "exception": false,
     "start_time": "2024-05-21T21:00:39.805055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip uninstall transformers -y\n",
    "\n",
    "!pip install quanto\n",
    "!pip install -i https://pypi.org/simple/ bitsandbytes\n",
    "!pip install git+https://github.com/huggingface/accelerate.git\n",
    "!pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46edf877",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T21:04:37.626438Z",
     "iopub.status.busy": "2024-05-21T21:04:37.626148Z",
     "iopub.status.idle": "2024-05-21T21:04:39.706112Z",
     "shell.execute_reply": "2024-05-21T21:04:39.705113Z"
    },
    "papermill": {
     "duration": 2.093645,
     "end_time": "2024-05-21T21:04:39.708619",
     "exception": false,
     "start_time": "2024-05-21T21:04:37.614974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\r\n",
      "Token is valid (permission: read).\r\n",
      "Your token has been saved to /root/.cache/huggingface/token\r\n",
      "Login successful\r\n"
     ]
    }
   ],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "hf_api_key = user_secrets.get_secret(\"hf_api_key\")\n",
    "\n",
    "!huggingface-cli login --token $hf_api_key\n",
    "\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a0c6929",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T21:04:39.730718Z",
     "iopub.status.busy": "2024-05-21T21:04:39.730422Z",
     "iopub.status.idle": "2024-05-21T21:04:41.625530Z",
     "shell.execute_reply": "2024-05-21T21:04:41.624146Z"
    },
    "papermill": {
     "duration": 1.908726,
     "end_time": "2024-05-21T21:04:41.627955",
     "exception": false,
     "start_time": "2024-05-21T21:04:39.719229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf src/\n",
    "!mkdir -p src/\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/kaggle/working/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2d3f515",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T21:04:41.651891Z",
     "iopub.status.busy": "2024-05-21T21:04:41.651322Z",
     "iopub.status.idle": "2024-05-21T21:04:41.657741Z",
     "shell.execute_reply": "2024-05-21T21:04:41.656869Z"
    },
    "papermill": {
     "duration": 0.019716,
     "end_time": "2024-05-21T21:04:41.659787",
     "exception": false,
     "start_time": "2024-05-21T21:04:41.640071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/config.py\n",
    "import torch\n",
    "\n",
    "class CFG:\n",
    "    model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "    dtype = torch.float16\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51781b1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T21:04:41.682421Z",
     "iopub.status.busy": "2024-05-21T21:04:41.682152Z",
     "iopub.status.idle": "2024-05-21T21:04:41.694503Z",
     "shell.execute_reply": "2024-05-21T21:04:41.693664Z"
    },
    "papermill": {
     "duration": 0.026247,
     "end_time": "2024-05-21T21:04:41.696522",
     "exception": false,
     "start_time": "2024-05-21T21:04:41.670275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/custom_models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/custom_models.py\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers.cache_utils import Cache, DynamicCache, StaticCache\n",
    "from transformers.modeling_attn_mask_utils import AttentionMaskConverter\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPast\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM, LlamaModel\n",
    "from config import CFG\n",
    "\n",
    "class CustomLlamaForCausalLM(LlamaForCausalLM):\n",
    "     def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = CustomLlama(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "class CustomLlama(LlamaModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        \n",
    "        self.next_token_i = 0\n",
    "        self.embed_layers = 12\n",
    "        self.mid_hidden_states = []\n",
    "        self.is_get_mid = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if (input_ids is None) ^ (inputs_embeds is not None):\n",
    "            raise ValueError(\n",
    "                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n",
    "            )\n",
    "\n",
    "        if self.gradient_checkpointing and self.training and use_cache:\n",
    "            logger.warning_once(\n",
    "                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n",
    "            )\n",
    "            use_cache = False\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "        past_seen_tokens = 0\n",
    "        if use_cache:  # kept for BC (cache positions)\n",
    "            if not isinstance(past_key_values, StaticCache):\n",
    "                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n",
    "                past_seen_tokens = past_key_values.get_seq_length()\n",
    "        if cache_position is None:\n",
    "            if isinstance(past_key_values, StaticCache):\n",
    "                raise ValueError(\"cache_position is a required argument when using StaticCache.\")\n",
    "            cache_position = torch.arange(\n",
    "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "            )\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "        causal_mask = self._update_causal_mask(attention_mask, inputs_embeds, cache_position, past_seen_tokens)\n",
    "\n",
    "        # embed positions\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # decoder layers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        next_decoder_cache = None\n",
    "\n",
    "        for i, decoder_layer in enumerate(self.layers):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                layer_outputs = self._gradient_checkpointing_func(\n",
    "                    decoder_layer.__call__,\n",
    "                    hidden_states,\n",
    "                    causal_mask,\n",
    "                    position_ids,\n",
    "                    past_key_values,\n",
    "                    output_attentions,\n",
    "                    use_cache,\n",
    "                    cache_position,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = decoder_layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask=causal_mask,\n",
    "                    position_ids=position_ids,\n",
    "                    past_key_value=past_key_values,\n",
    "                    output_attentions=output_attentions,\n",
    "                    use_cache=use_cache,\n",
    "                    cache_position=cache_position,\n",
    "                )\n",
    "            hidden_states = layer_outputs[0]\n",
    "            \n",
    "            if self.is_get_mid and self.next_token_i == 0:\n",
    "                self.mid_hidden_states.append(hidden_states)\n",
    "            \n",
    "            if not self.is_get_mid and i < self.embed_layers and self.next_token_i==0:\n",
    "                mid_hidden_state = self.mid_hidden_states[i]\n",
    "                if i > 16:\n",
    "                    mid_hidden_state = mid_hidden_state.to(\"cuda:1\")\n",
    "                hidden_states = torch.cat((hidden_states[:, :6, :], mid_hidden_state, hidden_states[:, 6+mid_hidden_state.size(1):, :]), dim=1)\n",
    "\n",
    "            if use_cache:\n",
    "                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attns += (layer_outputs[1],)\n",
    "        \n",
    "        if self.is_get_mid and self.next_token_i > 1:\n",
    "            return None\n",
    "        \n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        # add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        next_cache = None\n",
    "        if use_cache:\n",
    "            next_cache = (\n",
    "                next_decoder_cache.to_legacy_cache() if isinstance(next_decoder_cache, Cache) else next_decoder_cache\n",
    "            )\n",
    "        \n",
    "        self.next_token_i += 1\n",
    "        \n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n",
    "        return BaseModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attns,\n",
    "        )\n",
    "    \n",
    "    def _update_causal_mask(\n",
    "        self,\n",
    "        attention_mask: torch.Tensor,\n",
    "        input_tensor: torch.Tensor,\n",
    "        cache_position: torch.Tensor,\n",
    "        past_seen_tokens: int,\n",
    "    ):\n",
    "        # TODO: As of torch==2.2.0, the `attention_mask` passed to the model in `generate` is 2D and of dynamic length even when the static\n",
    "        # KV cache is used. This is an issue for torch.compile which then recaptures cudagraphs at each decode steps due to the dynamic shapes.\n",
    "        # (`recording cudagraph tree for symint key 13`, etc.), which is VERY slow. A workaround is `@torch.compiler.disable`, but this prevents using\n",
    "        # `fullgraph=True`. See more context in https://github.com/huggingface/transformers/pull/29114\n",
    "\n",
    "        if self.config._attn_implementation == \"flash_attention_2\":\n",
    "            if attention_mask is not None and 0.0 in attention_mask:\n",
    "                return attention_mask\n",
    "            return None\n",
    "\n",
    "        if self.config._attn_implementation == \"sdpa\":\n",
    "            # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument,\n",
    "            # in order to dispatch on Flash Attention 2.\n",
    "            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n",
    "                attention_mask, inputs_embeds=input_tensor, past_key_values_length=past_seen_tokens\n",
    "            ):\n",
    "                return None\n",
    "\n",
    "        dtype, device = input_tensor.dtype, input_tensor.device\n",
    "        min_dtype = torch.finfo(dtype).min\n",
    "        sequence_length = input_tensor.shape[1]\n",
    "        if hasattr(getattr(self.layers[0], \"self_attn\", {}), \"past_key_value\"):  # static cache\n",
    "            target_length = self.config.max_position_embeddings\n",
    "        else:  # dynamic cache\n",
    "            target_length = (\n",
    "                attention_mask.shape[-1]\n",
    "                if isinstance(attention_mask, torch.Tensor)\n",
    "                else past_seen_tokens + sequence_length + 1\n",
    "            )\n",
    "\n",
    "        causal_mask = torch.full((sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device)\n",
    "        if sequence_length != 1:\n",
    "            causal_mask = torch.triu(causal_mask, diagonal=1)\n",
    "        causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n",
    "        causal_mask = causal_mask[None, None, :, :].expand(input_tensor.shape[0], 1, -1, -1)\n",
    "        if attention_mask is not None:\n",
    "            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n",
    "            if attention_mask.dim() == 2:\n",
    "                mask_length = attention_mask.shape[-1]\n",
    "                padding_mask = causal_mask[..., :mask_length].eq(0.0) * attention_mask[:, None, None, :].eq(0.0)\n",
    "                causal_mask[..., :mask_length] = causal_mask[..., :mask_length].masked_fill(padding_mask, min_dtype)\n",
    "            elif attention_mask.dim() == 4:\n",
    "                # backwards compatibility: we allow passing a 4D attention mask shorter than the input length with\n",
    "                # cache. In that case, the 4D attention mask attends to the newest tokens only.\n",
    "                if attention_mask.shape[-2] < cache_position[0] + sequence_length:\n",
    "                    offset = cache_position[0]\n",
    "                else:\n",
    "                    offset = 0\n",
    "                mask_shape = attention_mask.shape\n",
    "                mask_slice = (attention_mask.eq(0.0)).to(dtype=dtype) * min_dtype\n",
    "                causal_mask[\n",
    "                    : mask_shape[0], : mask_shape[1], offset : mask_shape[2] + offset, : mask_shape[3]\n",
    "                ] = mask_slice\n",
    "\n",
    "        if (\n",
    "            self.config._attn_implementation == \"sdpa\"\n",
    "            and attention_mask is not None\n",
    "            and attention_mask.device.type == \"cuda\"\n",
    "        ):\n",
    "            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n",
    "            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n",
    "            # Details: https://github.com/pytorch/pytorch/issues/110213\n",
    "            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n",
    "\n",
    "        return causal_mask\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    model = CustomLlamaForCausalLM.from_pretrained(\n",
    "        CFG.model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31e17ffc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T21:04:41.717973Z",
     "iopub.status.busy": "2024-05-21T21:04:41.717731Z",
     "iopub.status.idle": "2024-05-21T21:04:41.725327Z",
     "shell.execute_reply": "2024-05-21T21:04:41.724475Z"
    },
    "papermill": {
     "duration": 0.020699,
     "end_time": "2024-05-21T21:04:41.727414",
     "exception": false,
     "start_time": "2024-05-21T21:04:41.706715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/pipeline.py\n",
    "from typing import Tuple, List\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from transformers import (\n",
    "    AutoTokenizer, BitsAndBytesConfig, PreTrainedTokenizerBase, PreTrainedModel\n",
    ")\n",
    "\n",
    "from config import CFG\n",
    "from custom_models import CustomLlamaForCausalLM\n",
    "\n",
    "class EmbedCompressionPipeline:\n",
    "    \"\"\"Pipeline for compressing embeddings of input prompts using a pre-trained language model.\"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizerBase, model: PreTrainedModel):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_name):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "        \n",
    "        device_map = [('model.embed_tokens', 0)]\n",
    "        device_map.extend([(f'model.layers.{i}', 0) for i in range(0, 16)])\n",
    "        device_map.extend([(f'model.layers.{i}', 1) for i in range(16, 32)])\n",
    "        device_map.extend([\n",
    "            ('model.norm', 1),\n",
    "            ('lm_head', 1),\n",
    "        ])\n",
    "        device_map = {ii:jj for (ii,jj) in device_map}\n",
    "\n",
    "        model = CustomLlamaForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=device_map,\n",
    "        )\n",
    "        \n",
    "        return cls(tokenizer, model)\n",
    "\n",
    "\n",
    "    def set_mid_hidden_states(self, prompt, block_size: int = 2) -> Tensor:\n",
    "        \"\"\"Compress embeddings\"\"\"\n",
    "        self.model.model.next_token_i = 0\n",
    "        self.model.model.mid_hidden_states = []\n",
    "        \n",
    "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").to(CFG.device)\n",
    "        try:\n",
    "            self.model.model.is_get_mid = True\n",
    "            outputs = self.model.generate(\n",
    "                **input_ids,\n",
    "                max_new_tokens=200,\n",
    "                temperature=1.0,\n",
    "            )\n",
    "        except:\n",
    "            self.model.model.is_get_mid = False\n",
    "            \n",
    "        self.model.model.mid_hidden_states = [self.embed_compression(state, block_size=block_size) for state in self.model.model.mid_hidden_states]\n",
    "    \n",
    "    def embed_compression(self, inputs_embeds: Tensor, block_size: int = 2) -> Tensor:\n",
    "        \"\"\"Compress embeddings by averaging over blocks of specified size.\"\"\"\n",
    "        means = [inputs_embeds[:, i:i+block_size, :].mean(dim=1, keepdim=True)\n",
    "                 for i in range(0, inputs_embeds.size(1), block_size)]\n",
    "        compression_inputs_embeds = torch.cat(means, dim=1)\n",
    "        return compression_inputs_embeds\n",
    "    \n",
    "\n",
    "    def generate(self, prompt, embed_layers=12):\n",
    "        self.model.model.next_token_i = 0\n",
    "        self.model.model.is_get_mid = False\n",
    "        self.model.model.embed_layers = embed_layers\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"assistant\", \"content\": \"embed\"*self.model.model.mid_hidden_states[0].size(1)},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "        prompt = self.tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").to(CFG.device)\n",
    "\n",
    "        outputs = self.model.generate(\n",
    "            **input_ids,\n",
    "            max_new_tokens=50,\n",
    "            temperature=1.0,\n",
    "        )\n",
    "        return self.tokenizer.decode(outputs[0, input_ids['input_ids'].size(1):])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pipe = EmbedCompressionPipeline.from_pretrained(CFG.model_name)\n",
    "    pipe.set_mid_hidden_states(\"My name is Embed Compression Pipeline.\")\n",
    "    print(pipe.generate(\"What is your name?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1e6377",
   "metadata": {
    "papermill": {
     "duration": 0.011936,
     "end_time": "2024-05-21T21:04:41.750016",
     "exception": false,
     "start_time": "2024-05-21T21:04:41.738080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fde006c",
   "metadata": {
    "papermill": {
     "duration": 0.009872,
     "end_time": "2024-05-21T21:04:41.774435",
     "exception": false,
     "start_time": "2024-05-21T21:04:41.764563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afacdc17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T21:04:41.796084Z",
     "iopub.status.busy": "2024-05-21T21:04:41.795545Z",
     "iopub.status.idle": "2024-05-21T21:06:50.290671Z",
     "shell.execute_reply": "2024-05-21T21:06:50.289560Z"
    },
    "papermill": {
     "duration": 128.508524,
     "end_time": "2024-05-21T21:06:50.293086",
     "exception": false,
     "start_time": "2024-05-21T21:04:41.784562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dad83436fb647dc8ee2db69ad622031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec376bb70d314fbfb53a0c7172d85ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6e5f59ecc94b7ea138f5b92d28f968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead7ef6f172d403e9e7e8612e9410cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54db929edc0a4352a2efa4ace0d47f34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e41baff9eed4eac8c265ad7637958db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c69bedb5c4d74f44b383109b7469dc47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9253280d05e540b2a1e11505148edeb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e87e4d6455490080277ff1a48e972a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b2614167d94885992650030b425b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43be4379941748219078abe7e411cfe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde883abf9a24945ad31cd4f2475e55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from config import CFG\n",
    "from pipeline import EmbedCompressionPipeline\n",
    "\n",
    "pipe = EmbedCompressionPipeline.from_pretrained(CFG.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "783a2dfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T21:06:50.319898Z",
     "iopub.status.busy": "2024-05-21T21:06:50.319447Z",
     "iopub.status.idle": "2024-05-21T21:06:50.326292Z",
     "shell.execute_reply": "2024-05-21T21:06:50.325368Z"
    },
    "papermill": {
     "duration": 0.022075,
     "end_time": "2024-05-21T21:06:50.328168",
     "exception": false,
     "start_time": "2024-05-21T21:06:50.306093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import re\n",
    "import gc\n",
    "import time\n",
    "import torch\n",
    "\n",
    "def clear_cache():\n",
    "    for _ in range(10):\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        time.sleep(0.1)\n",
    "\n",
    "def get_gpu_memory():\n",
    "    \"\"\"Function to get current GPU memory usage via nvidia-smi command.\"\"\"\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used', '--format=csv'], capture_output=True, text=True)\n",
    "    # Parse the output to extract memory usage\n",
    "    memory_used = [int(re.findall(r'\\d+', line)[0]) for line in result.stdout.split('\\n') if re.search(r'\\d+ MiB', line)]\n",
    "    return memory_used\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9380150e",
   "metadata": {
    "papermill": {
     "duration": 0.011987,
     "end_time": "2024-05-21T21:06:50.352559",
     "exception": false,
     "start_time": "2024-05-21T21:06:50.340572",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Get sample data from Haoxiang Shi, Jiaan Wang, Jiarong Xu, Cen Wang, Tetsuya Sakai: “CT-Eval: Benchmarking Chinese Text-to-Table Performance in Large Language Models”, 2024; <a href='http://arxiv.org/abs/2405.12174'>arXiv:2405.12174</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88ccc701",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-21T21:06:50.379885Z",
     "iopub.status.busy": "2024-05-21T21:06:50.379610Z",
     "iopub.status.idle": "2024-05-21T21:06:50.401014Z",
     "shell.execute_reply": "2024-05-21T21:06:50.400127Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.037614,
     "end_time": "2024-05-21T21:06:50.402934",
     "exception": false,
     "start_time": "2024-05-21T21:06:50.365320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "comp_prompt = \"\"\"1 Introduction\n",
    "Information extraction (IE) aims to identify and extract structured information from unstructured text.\n",
    "1Codes and data will be publicly available once accepted.\n",
    "Basic IE sub-tasks, such as, named entity recognition and entity linking, generally operate at the\n",
    "sentence level, which ignore models’ ability to understand the document-level meaning. In contrast,\n",
    "Text-to-Table, an emerging sub-task of IE, requires\n",
    "models to understand information within a given\n",
    "document and then generate structured tables. Despite great success in the domain Wu et al. (2022);\n",
    "Li et al. (2023), previous studies are oriented to\n",
    "English, limiting the research in other languages.\n",
    "Recently, large language models (LLMs) have\n",
    "demonstrated powerful performance across various NLP tasks (Zhao et al., 2023; Wang et al.,\n",
    "2023a,b,c). Some studies (González-Gallardo et al.,\n",
    "2023; Gao et al., 2023) have also utilized LLMs for\n",
    "several IE sub-tasks, and found that compared to\n",
    "supervised baselines, the performance of LLMs is\n",
    "sub-optimal. However, their performance of LLMs\n",
    "on Text-to-Table remains largely unexplored. Additionally, the rapid advancements in LLMs have facilitated their widespread adoption in multi-lingual\n",
    "settings, enabling language modeling abilities to be\n",
    "shared across different languages. Consequently, it\n",
    "is theoretically possible to feasible to employ LLMs\n",
    "for text-to-table in other languages, an area that\n",
    "has yet to be thoroughly investigated.\n",
    "Motivated by the aforementioned considerations,\n",
    "we decide to benchmark LLMs on Chinese text-totable. A heuristic approach involves translating existing English datasets into Chinese for subsequent\n",
    "performance evaluation. There are four datasets\n",
    "widely used in text-to-table: E2E (Novikova et al.,\n",
    "2017) is a restaurant domain dataset encompassing\n",
    "51.5K samples about restaurant information like\n",
    "names, addresses, rate scores, etc. Rotowire (Wiseman et al., 2017) is a sports domain dataset derived\n",
    "from NBA basketball games with 4.9K samples\n",
    "about NBA team information. WikiBio (Lebret\n",
    "et al., 2016), compiled from Wikipedia, containing\n",
    "72.6K documents and corresponding structured tables in the biography domain. WikiTableText (Bao\n",
    "arXiv:2405.12174v1 [cs.CL] 20 May 2024\n",
    "et al., 2018) is also derived from Wikipedia, and\n",
    "involves 13.3K multidisciplinary samples spanning\n",
    "fields like finance and politics. However, according to our preliminary analysis, these datasets also\n",
    "suffer from the issues of less diversity or high hallucination, making them unsuitable to benchmark\n",
    "LLMs. Specifically, (1) E2E, Rotowire, and WikiBio exhibit a lack of diversity as they focus on a\n",
    "single domain, violating the core principle of instruction tuning in LLMs, that is, diversity. (2)\n",
    "Although WikiTableText incorporates multiple domains for diversity, our analysis (§ 3.5) indicates\n",
    "that 18.83% of samples exhibit hallucination in\n",
    "the golden tables, that is, containing additional information beyond the provided documents. This\n",
    "arises from treating Wikipedia infoboxes as golden\n",
    "tables, created collaboratively by online users and\n",
    "potentially containing additional basic information.\n",
    "In this paper, we propose the Chinese Text-toTable Evaluation (CT-Eval) dataset, which is constructed through three steps to ensure data diversity\n",
    "and minimize hallucination. To ensure diversity,\n",
    "the first step involves collecting multidisciplinary\n",
    "document-table pairs. We choose the Baidu Baike\n",
    "as the data source, which is a popular Chinese multidisciplinary online encyclopedia. Each page in\n",
    "this source contains text and an infobox summarizing the corresponding key information. Second, to\n",
    "minimize data hallucination, we train an LLM, as a\n",
    "hallucination judger, to filter out task samples with\n",
    "hallucination in their golden tables (infoboxes). Finally, we obtain 88.6K samples with an average\n",
    "length of 911.46 Chinese characters. We split them\n",
    "into 86.6K, 1K and 1K for training, validation and\n",
    "testing. For validation and testing samples, human\n",
    "annotators further clean data hallucination in the\n",
    "golden tables to ensure evaluation reliability.\n",
    "Based on the proposed CT-Eval, we benchmark\n",
    "various mainstream LLMs in both zero-shot (for\n",
    "both open- and closed-source LLMs) and finetuning (only for open-source LLMs) scenarios. Our\n",
    "experiments reveal that (1) GPT-4 achieves the best\n",
    "zero-shot performance among all LLMs. However,\n",
    "its performance remains a discernible disparity\n",
    "compared to human judgment. (2) After fine-tuning\n",
    "on the training set of CT-Eval, all open-source\n",
    "LLMs demonstrate a significant performance improvement, outperforming zero-shot GPT-4 by a\n",
    "large margin, indicating the effectiveness of CTEval. In-depth analyses of LLM-generated tables\n",
    "reveal the persistence of hallucination issues in\n",
    "both zero-shot and fine-tuned LLMs, highlighting\n",
    "a challenge in using LLMs as text-to-table systems.\n",
    "Future work could not only evaluate LLMs’ performance on text-to-table via our CT-Eval benchmark\n",
    "dataset, but also leverage its training data to improve LLMs’ text-to-table ability via fine-tuning.\n",
    "2 Related Work\n",
    "2.1 Text-to-table Tasks\n",
    "Wu et al. (2022) pioneer the text-to-table task.\n",
    "Given the absence of a dedicated text-to-table\n",
    "dataset, Wu et al. (2022) repurpose existing tableto-text datasets, i.e., WikiBio (Lebret et al., 2016),\n",
    "E2E (Novikova et al., 2017), Rotowire (Wiseman et al., 2017) and WikiTableText (Bao et al.,\n",
    "2018), for text-to-table tasks by reversing their\n",
    "input-output pairs. They fine-tune BART (Lewis\n",
    "et al., 2019) to perform text-to-table in a sequenceto-sequence manner, and find that the fine-tuned\n",
    "BART outperforms the pipeline baselines using\n",
    "relation extraction and named entity extraction.\n",
    "STable (Pietruszka et al., 2022) employs two pretrained language models (PLMs) (T5 (Raffel et al.,\n",
    "2020) and TILT (Powalski et al., 2021)) for text-totable, and designs a permutation-based decoder to\n",
    "enhance the PLMs’ table generation ability. Subsequently, Li et al. (2023) find that the predefined row\n",
    "order in golden tables introduces bias into text-totable models. Consequently, they train table header\n",
    "and table body generators separately to produce\n",
    "final tables. While these studies achieve notable\n",
    "success, they primarily explored text-to-table performance before the LLM era. In addition, their\n",
    "evaluation datasets generally focus on a single domain, and adapt from the table-to-text datasets, resulting in hallucination issues. Thus, they are unsuitable for benchmarking LLMs in text-to-table.\n",
    "2.2 Large Language Models\n",
    "The advent of advanced LLMs ,e.g., ChatGPT (OpenAI, 2022), GPT-4 (OpenAI, 2023), marks a pivotal\n",
    "moment that propels the field of NLP into a boom\n",
    "phase. Zhong et al. (2023) show that LLMs can\n",
    "achieve decent performance on benchmarks like\n",
    "GLUE (Wang et al., 2018), which spans eight representative NLP understanding tasks. Concurrently,\n",
    "several studies have scrutinized the performance\n",
    "of LLMs across various IE tasks. For example,\n",
    "Gao et al. (2023) test the capability of ChatGPT\n",
    "in event extraction. Similarly, González-Gallardo\n",
    "et al. (2023) employ ChatGPT on historical entity\n",
    "recognition. However, the results of these IE subtasks consistently show that LLMs underperform\n",
    "compared to state-of-the-art supervised approaches.\n",
    "The text-to-table task we focused on is more complex than the previous basic IE sub-tasks, however,\n",
    "it remains an unexplored area for evaluation on\n",
    "LLMs.\n",
    "3 CT-Eval\n",
    "In this section, we first discuss the data source for\n",
    "building CT-Eval (§ 3.1). Then, we give the details\n",
    "of how to control the hallucination in the preliminary collected data, including LLM hallucination\n",
    "judger (§ 3.2) and human cleaning processes (§ 3.3).\n",
    "Finally, we formulate the text-to-table task (§ 3.4)\n",
    "and provide the details of data statistics (§ 3.5).\n",
    "3.1 Data Source\n",
    "Following the success of WikiTableText (Bao et al.,\n",
    "2018), we also choose a multidisciplinary online\n",
    "encyclopedia as the data source to ensure data diversity. After carefully comparing existing Chinese\n",
    "online encyclopedias, we choose Baidu Baike2\n",
    ",\n",
    "which is one of the Chinese encyclopedias with\n",
    "the most entries in the world.\n",
    "We obtain the Baidu Baike data from the dumps\n",
    "provided by Xu et al. (2017). The data contains\n",
    "over 9M entity pages, each of which includes an\n",
    "infobox and the corresponding textual description,\n",
    "forming a text-to-table sample. Utilizing this data,\n",
    "we implement the following rule-based strategy\n",
    "for preliminary data cleaning: (1) Each page must\n",
    "contain at least one infobox; otherwise, the golden\n",
    "table is missing. (2) The number of tabular cells in\n",
    "the infobox should exceed three to ensure validity.\n",
    "(3) The length of the textual description should\n",
    "surpass 200 tokens. After that, 200K documenttable pairs are remaining for further processing.\n",
    "3.2 LLM Data Cleaning\n",
    "After the initial cleaning process, the hallucination issue persists due to Baidu Baike’s submission\n",
    "rules, wherein most of the page text and infoboxes\n",
    "are edited and maintained by individuals. Thus,\n",
    "the contents in the infoboxes may not always align\n",
    "precisely with the textual documents. For instance,\n",
    "some infoboxes may include additional knowledge\n",
    "unrelated to the text, potentially misleading the\n",
    "model from learning the text-to-table task.\n",
    "2\n",
    "https://baike.baidu.com/\n",
    "§ There is a document and a golden table that\n",
    "summarize the information contained in this\n",
    "document. Please help me identify whether the\n",
    "golden table contains additional information\n",
    "beyond the document. Give the reason first and\n",
    "then provide your judgment.\n",
    "Figure 1: Illustration of judgment prompt.\n",
    "To control the hallucination in the task samples,\n",
    "we decide to employ an LLM as a hallucination\n",
    "judger to filter out samples exhibiting hallucination.\n",
    "While using GPT-4 directly as the LLM hallucination judger is a straightforward approach, utilizing\n",
    "official APIs can be costly. Therefore, we first\n",
    "randomly select 5K samples from the data before\n",
    "cleaning. Then, we use GPT-4 to assess whether the\n",
    "golden tables contain additional information. The\n",
    "judgment prompt is illustrated in Figure 1, where\n",
    "the LLM is tasked with evaluating hallucination in\n",
    "a chain-of-thought manner. Next, we use the GPT4 judgment results to train an open-source LLM,\n",
    "employing the trained model to evaluate the remaining samples. Samples containing hallucinations are\n",
    "discarded. Given the capacity for understanding\n",
    "lengthy documents of existing Chinese LLMs (Bai\n",
    "et al., 2023b), we select ChatGLM3-6B-32k3\n",
    "as\n",
    "the open-source LLM hallucination judger. Finally,\n",
    "there are 88.6K samples after the data cleaning\n",
    "by the LLM hallucination judger. These samples\n",
    "totally cover 28 domains, e.g., physics and religion.\n",
    "3.3 Human Data Cleaning\n",
    "We split the LLM-cleaned samples into the training, validation, and test sets with 84.6K, 1K and\n",
    "1K samples, respectively. In the validation and test\n",
    "sets, we balance the number of samples in each\n",
    "domain to ensure a comprehensive evaluation. Furthermore, to mitigate the hallucination issue in the\n",
    "validation and test sets, we employ human annotators to manually cleanse their golden tables.\n",
    "In this phase, we employ five human annotators\n",
    "and one data expert, all of whom are native Chinese\n",
    "speakers with advanced educational qualifications.\n",
    "The data expert is a researcher with extensive experience in IE research. We first organize a tutorial\n",
    "for the five annotators to ensure alignment of annotation requirements. This includes clarifying the\n",
    "concept of the hallucination issue, emphasizing the\n",
    "additional information present in tables that cannot\n",
    "be directly extracted from the corresponding documents. Next, for each document-table pair in the\n",
    "3\n",
    "https://huggingface.co/THUDM/chatglm3-6b-32k\n",
    "CT-Eval\n",
    "Industry\n",
    "Informatics\n",
    "Physics\n",
    "Astronomy\n",
    "Traffic Engineering\n",
    "Biology\n",
    "Medical Science\n",
    "Culture\n",
    "Religion\n",
    "History\n",
    "Geography\n",
    "Education\n",
    "Business\n",
    "Finance\n",
    "Management\n",
    "Movie and Book\n",
    "Science Fiction\n",
    "Music\n",
    "Animation\n",
    "Dwelling\n",
    "Traveling\n",
    "Career\n",
    "Healthy\n",
    "Food\n",
    "Cosmetics\n",
    "Profile\n",
    "Security\n",
    "Military\n",
    "STEM\n",
    "24.0%\n",
    "Social Science\n",
    "60.9%\n",
    "Life\n",
    "Support\n",
    "11.6%\n",
    "O.\n",
    "3.5\n",
    "%\n",
    "Figure 2: Domain distribution in CT-EVAL\n",
    "validation and test sets, three annotators are asked\n",
    "to remove the hallucination information from tables. The data expert reviews 10% of the manually\n",
    "cleaned samples from each annotator. If the accuracy rate falls below 95%, the respective annotator\n",
    "will be required to redo the annotation. Finally,\n",
    "for each sample, if the three manually cleaned results are consistent, the results are saved as the final\n",
    "data. Otherwise, the results are decided by a group\n",
    "meeting among all annotators and the data expert.\n",
    "3.4 Task Overview\n",
    "Given a document D = {w1, w2, ..., w|D|}, where\n",
    "wi\n",
    "is the i-th word in D, the text-to-table task aims\n",
    "to extract key information from D and outputs a table T = {c0,0, c0,1, ..., c0,n, c1,0, c1,1, ..., c1,n, ...,\n",
    "ci,j , ..., cm,n}, where c0,k(k ∈ {0, 1, ..., n}) represents the column header and ck,0(k ∈ {0, 1, ..., n})\n",
    "denotes as the row header. ci,j (i > 0, j > 0) represents the text of the cell in the i-th row and j-th\n",
    "column in the table with m rows and n columns.\n",
    "3.5 Data Statistics\n",
    "We compare CT-Eval with the previous datasets\n",
    "across several metrics including language, number of entries, average length, number of domains,\n",
    "hallucination rate, and average number of cells.\n",
    "As shown in Table 1, compared to previous\n",
    "datasets that mostly focus on one domain, CT-Eval\n",
    "covers 28 domains, offering a valuable resource\n",
    "to evaluate the text-to-table capabilities of LLMs\n",
    "across multiple domains. These 28 domains can\n",
    "be categorized into four branches: STEM, social\n",
    "science, life support and others. To provide a comprehensive understanding of domain coverage in\n",
    "CT-Eval, we present the distributions of each domain in Figure 2. Notably, the “Social Science”\n",
    "Original Item\n",
    "Translation\n",
    "材料 做法 厨师一点通\n",
    "炖牛肉时，可以放几个\n",
    "山楂进去，这样牛肉会\n",
    "烂得快些，而且有股山\n",
    "楂的清香。\n",
    "1.牛肉洗净放入清水锅\n",
    "中煮至七成熟，捞出切\n",
    "成方块\n",
    "2. …\n",
    "材料：\n",
    "牛肉，胡萝卜，\n",
    "白萝卜，大料，\n",
    "香叶\n",
    "东坡牛肉\n",
    "Original Item\n",
    "Ingredients Cooking method Cooking Tips\n",
    "When stewing beef, you\n",
    "can put a few hawthorn\n",
    "into it, so that the beef\n",
    "will rot faster and have a\n",
    "hawthorn fragrance.\n",
    "1. Wash the beef and\n",
    "put it into a pot of\n",
    "water to boil until\n",
    "it is seven mature,\n",
    "fish out and cut it\n",
    "into squares…\n",
    "2. …\n",
    "Ingredients:\n",
    "Beef, carrots, white\n",
    "radish, Chinese\n",
    "spices, allspice\n",
    "Dongpo\n",
    "Beef\n",
    "“标题”: “东坡牛肉”, “ ”章节“: [{”标题“: ”东坡牛肉的做法“},\n",
    "内容“: ”葱段、姜片、蒜头、辣椒、香叶、大料、盐、白糖、番茄酱、排骨\n",
    "酱、辣椒酱、高汤（用煮牛肉的汤即可）。\\n做法：\\n1、先来处理一下牛肉，\n",
    "将牛腩在冷水中多泡一会…“}\n",
    "Translation\n",
    "“Title”: “Dongpo Beef”, “ ”Chapter“: [{”Title “: ”Dongpo Beef Practice“}, :\n",
    "Ingredients：Diced green onion, ginger, garlic, chili pepper, coriander, dashi, salt,\n",
    "sugar, tomato sauce, rib sauce, chili sauce, broth. Cooking method:1, first, let's deal\n",
    "with the beef, soak the brisket in cold water for a while longer...\"}\n",
    "Figure 3: Text-to-table example from CT-EVAL.\n",
    "branch exhibits the highest proportion, encompassing domains such as culture and religion. Conversely, the “Other” branch, which includes topics\n",
    "related to cosmetics and profiles, represents the\n",
    "smallest portion. The “STEM” (science, technology, engineering, and mathematics) branch features\n",
    "data related to topics such as physical sciences and\n",
    "astronomy. The “Life Support” branch is highly\n",
    "relevant to everyday life, comprises 11.60% of the\n",
    "data, and primarily focuses on domains such as\n",
    "dwelling and health. An illustration of a data sample from CT-Eval is depicted in Figure 3. Each\n",
    "document-table pair, akin to the example shown,\n",
    "consists of a textual description and a golden table\n",
    "with varying numbers of columns and rows.\n",
    "To assess the quality of our dataset compared\n",
    "to previous ones, we randomly select 200 samples\n",
    "from the training, validation and test sets of CTEval, and previous E2E, Rotowire, WikiBio, and\n",
    "WikiTableText, respectively. Then, we compute the\n",
    "hallucination rate for each dataset through human\n",
    "annotation. Specifically, three annotators proficient\n",
    "in both English and Chinese judged whether the\n",
    "golden tables contained hallucination information\n",
    "following the guidance outlined in Section 3.3. The\n",
    "hallucination rate for each dataset is determined\n",
    "by the average proportion of hallucinated samples\n",
    "judged by all three annotators. We observe that\n",
    "the hallucination rate of the CT-Eval training set\n",
    "is 6.83%, similar to Rotowire (6.17%). Moreover,\n",
    "with the help of our human cleaning, the hallucination rates in our test and validation sets decrease to\n",
    "1.00% and 1.50%, respectively, significantly lower\n",
    "Dataset Language N. Entries Avg. Length N. Domains Hallu. R Avg. Cells\n",
    "E2E English 42,061 90.58 Restaurant 4.17% 4.46\n",
    "Rotowire English 3,398 1311.01 Sports 6.17% 40.49\n",
    "WikiBio English 582,659 416.71 Biography 8.67% 4.19\n",
    "WikiTableText English 10,000 59.76 Multiple Subclasses 18.83% 4.25\n",
    "CT-Eval(train) Chinese 84,603 911.46 28 Subclasses 6.83% 11.40\n",
    "CT-Eval(val.) Chinese 1,000 813.32 28 Subclasses 1.00% 10.78\n",
    "CT-Eval(test) Chinese 1,000 845.22 28 Subclasses 1.50% 10.86\n",
    "Table 1: Data Statistics of CT-Eval and previous text-to-table datasets. “N. Entries” denotes the number of entries.\n",
    "“Hallu. R” indicates hallucination rate. “Avg.Cells” indicates the average number of table cells\n",
    "than those in other datasets. Therefore, the reliability of our dataset can be verified. Regarding\n",
    "domains, the E2E, Rotowire and WikiBio datasets\n",
    "focus on single domains, whereas WikiTableText\n",
    "and CT-Eval encompass multiple domains to ensure data diversity. Concerning the length of the input documents, we note that previous E2E and WikiTableText primarily contain short documents with\n",
    "an average length of under 100 words, whereas documents in our dataset and Rotowire exceed words\n",
    "or characters, presenting more complex inputs for\n",
    "text-to-table models. In summary, CT-Eval stands\n",
    "as the sole dataset fulfilling criteria of data diversity,\n",
    "lengthy documents, and low hallucination.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e734c0a5",
   "metadata": {
    "papermill": {
     "duration": 0.012347,
     "end_time": "2024-05-21T21:06:50.427926",
     "exception": false,
     "start_time": "2024-05-21T21:06:50.415579",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## block_size=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e47cae2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T21:06:50.455337Z",
     "iopub.status.busy": "2024-05-21T21:06:50.454943Z",
     "iopub.status.idle": "2024-05-21T21:07:30.492239Z",
     "shell.execute_reply": "2024-05-21T21:07:30.491176Z"
    },
    "papermill": {
     "duration": 40.066347,
     "end_time": "2024-05-21T21:07:30.507253",
     "exception": false,
     "start_time": "2024-05-21T21:06:50.440906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2024-05-21 21:07:17.269938: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-21 21:07:17.270070: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-21 21:07:17.441797: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.7 s, sys: 3.78 s, total: 33.5 s\n",
      "Wall time: 40 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipe.set_mid_hidden_states(comp_prompt, block_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9759d070",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T21:07:30.534123Z",
     "iopub.status.busy": "2024-05-21T21:07:30.533539Z",
     "iopub.status.idle": "2024-05-21T21:07:30.540349Z",
     "shell.execute_reply": "2024-05-21T21:07:30.539536Z"
    },
    "papermill": {
     "duration": 0.022393,
     "end_time": "2024-05-21T21:07:30.542403",
     "exception": false,
     "start_time": "2024-05-21T21:07:30.520010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1506, 4096])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mid_hidden_state = pipe.model.model.mid_hidden_states[0]\n",
    "mid_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e984b0b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T21:07:30.569206Z",
     "iopub.status.busy": "2024-05-21T21:07:30.568909Z",
     "iopub.status.idle": "2024-05-21T21:09:42.078308Z",
     "shell.execute_reply": "2024-05-21T21:09:42.077267Z"
    },
    "papermill": {
     "duration": 131.524997,
     "end_time": "2024-05-21T21:09:42.080286",
     "exception": false,
     "start_time": "2024-05-21T21:07:30.555289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_layers= 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT-Eval is an acronym for \"Chinese Text Evaluation\", which is a dataset and evaluation benchmark for text-based conversational AI models, particularly Chinese-language models. The CT-Eval dataset is a collection of annotated Chinese texts, which includes text classification,\n",
      "\n",
      "embed_layers= 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think there may be a typo!\n",
      "\n",
      "I'm assuming you meant to ask \"What is CT-ET?\"\n",
      "\n",
      "CT-ET stands for Chinese Textual Evaluation, which is a dataset used for evaluating the performance of Natural Language Processing (NLP) models\n",
      "\n",
      "embed_layers= 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think you meant to ask \"What is CT-Eval?\"\n",
      "\n",
      "CT-Eval is an evaluation metric for natural language processing (NLP) models, particularly in the context of language translation and text generation. It is used to measure the quality of the\n",
      "\n",
      "embed_layers= 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think you meant to ask \"What is CT-Eval?\".\n",
      "\n",
      "CT-Eval is short for \"Chinese Text Evaluation\", which is a dataset and a benchmark for evaluating the performance of machine learning models on text-related tasks, particularly in the context of\n",
      "\n",
      "embed_layers= 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize for the mistake earlier. After re-reading the text, I realized that CT-Eval is not mentioned. It seems that the correct abbreviation is actually \"CT-Eval\" or \"CT-Eval dataset\", which is a dataset used for\n",
      "\n",
      "embed_layers= 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT-Eval is the Chinese Text Evaluation dataset, which is a benchmark dataset for evaluating the performance of Chinese language models, specifically in the domain of text evaluation. The dataset consists of a collection of Chinese text documents, along with their corresponding annotations and labels\n",
      "\n",
      "embed_layers= 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize, but I couldn't find any information on \"CT-Eval\". It's possible that it's a specific tool or evaluation method used in a particular field or industry, or it might be a misspelling or acronym that I couldn't\n",
      "\n",
      "embed_layers= 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT-Eval is a benchmarking and evaluation platform for natural language processing (NLP) models, specifically designed for the task of text-to-text generation, such as language translation, text summarization, and text generation. CT-Eval provides a standardized\n",
      "\n",
      "embed_layers= 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT-Eval is an abbreviation that stands for \"Chinese Text Evaluation\".<|eot_id|>\n",
      "\n",
      "embed_layers= 30\n",
      "CT-Eval is the short name for the Chinese Treebank Evaluation. It is a Chinese treebank dataset and evaluation metric used to evaluate the performance of natural language processing (NLP) models, particularly those that are designed to process Chinese text.\n",
      "\n",
      "The\n",
      "\n",
      "CPU times: user 1min 45s, sys: 26.5 s, total: 2min 11s\n",
      "Wall time: 2min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"embed_layers=\", ((i+1)*3))\n",
    "    print(pipe.generate(\"What is CT-Eval?\", embed_layers=((i+5)*2)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1a32b1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T21:09:42.111447Z",
     "iopub.status.busy": "2024-05-21T21:09:42.110762Z",
     "iopub.status.idle": "2024-05-21T21:10:00.040226Z",
     "shell.execute_reply": "2024-05-21T21:10:00.039198Z"
    },
    "papermill": {
     "duration": 17.947447,
     "end_time": "2024-05-21T21:10:00.042397",
     "exception": false,
     "start_time": "2024-05-21T21:09:42.094950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Usage Before Inference:\n",
      "[3539, 3447] MiB\n",
      "CT-Eval stands for \"Chinese Text Evaluation\" or \"Computational Text Evaluation\". It is a research dataset and evaluation metric specifically designed for natural language processing (NLP) and text-to-text generation tasks, such as machine translation, text summarization\n",
      "GPU Usage After Inference:\n",
      "[3765, 4799] MiB\n",
      "Difference in GPU Memory Usage:\n",
      "[226, 1352] MiB\n"
     ]
    }
   ],
   "source": [
    "clear_cache()\n",
    "\n",
    "# GPU usage before inference\n",
    "memory_before = get_gpu_memory()\n",
    "print(\"GPU Usage Before Inference:\")\n",
    "print(memory_before, \"MiB\")\n",
    "\n",
    "# Inference code here\n",
    "print(pipe.generate(\"What is CT-Eval?\", embed_layers=12))\n",
    "\n",
    "# GPU usage after inference\n",
    "memory_after = get_gpu_memory()\n",
    "print(\"GPU Usage After Inference:\")\n",
    "print(memory_after, \"MiB\")\n",
    "\n",
    "# Calculate the difference in memory usage\n",
    "memory_difference = [after - before for before, after in zip(memory_before, memory_after)]\n",
    "print(\"Difference in GPU Memory Usage:\")\n",
    "print(memory_difference, \"MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eacdf24",
   "metadata": {
    "papermill": {
     "duration": 0.015157,
     "end_time": "2024-05-21T21:10:00.073428",
     "exception": false,
     "start_time": "2024-05-21T21:10:00.058271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b2bdcc2",
   "metadata": {
    "papermill": {
     "duration": 0.014693,
     "end_time": "2024-05-21T21:10:00.103209",
     "exception": false,
     "start_time": "2024-05-21T21:10:00.088516",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## block_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3778299e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T21:10:00.135146Z",
     "iopub.status.busy": "2024-05-21T21:10:00.134804Z",
     "iopub.status.idle": "2024-05-21T21:10:29.591632Z",
     "shell.execute_reply": "2024-05-21T21:10:29.590611Z"
    },
    "papermill": {
     "duration": 29.475588,
     "end_time": "2024-05-21T21:10:29.594067",
     "exception": false,
     "start_time": "2024-05-21T21:10:00.118479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.1 s, sys: 8.4 s, total: 29.5 s\n",
      "Wall time: 29.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipe.set_mid_hidden_states(comp_prompt, block_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ceb6b026",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T21:10:29.628899Z",
     "iopub.status.busy": "2024-05-21T21:10:29.628573Z",
     "iopub.status.idle": "2024-05-21T21:10:29.634931Z",
     "shell.execute_reply": "2024-05-21T21:10:29.634035Z"
    },
    "papermill": {
     "duration": 0.025012,
     "end_time": "2024-05-21T21:10:29.637504",
     "exception": false,
     "start_time": "2024-05-21T21:10:29.612492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2258, 4096])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mid_hidden_state = pipe.model.model.mid_hidden_states[0]\n",
    "mid_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3e7f043",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T21:10:29.674547Z",
     "iopub.status.busy": "2024-05-21T21:10:29.673590Z",
     "iopub.status.idle": "2024-05-21T21:13:36.049717Z",
     "shell.execute_reply": "2024-05-21T21:13:36.048680Z"
    },
    "papermill": {
     "duration": 186.473727,
     "end_time": "2024-05-21T21:13:36.130263",
     "exception": false,
     "start_time": "2024-05-21T21:10:29.656536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_layers= 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT-Eval, also known as Chinese Text-to-Table (CTT), is a benchmark dataset and evaluation metric for text-to-table (T2T) models, which is a type of natural language processing (NLP) task that aims to\n",
      "\n",
      "embed_layers= 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT-Eval is the abbreviation for Chinese Text-to-Table (CT-Eval). It is a dataset and evaluation benchmark for Chinese text-to-table (TT) tasks, specifically for Chinese text-to-table generation models. The dataset is designed to evaluate the\n",
      "\n",
      "embed_layers= 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think you meant to ask \"What is CT-Eval (Chinese Text Evaluation) dataset?\"\n",
      "\n",
      "CT-Eval (Chinese Text Evaluation) is a benchmarking dataset for evaluating the performance of text-to-table and text-to-text models in the Chinese language.\n",
      "\n",
      "embed_layers= 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm glad you asked!\n",
      "\n",
      "CT-Eval is the dataset I introduced in my previous response, specifically designed for evaluating the performance of Large Language Models (LLMs) on the text-to-table task. It's a collection of 88,600 text\n",
      "\n",
      "embed_layers= 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT-Eval is an abbreviation for \"Chinese Text-to-Table\" Evaluation dataset, which is a benchmark dataset for evaluating the performance of text-to-table models in the Chinese language. It is a collection of pairs of texts and corresponding tables that can be\n",
      "\n",
      "embed_layers= 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT-Eval is the Chinese text-to-table (CT-Eval) dataset, which is a new dataset designed to evaluate the performance of large language models (LLMs) on the task of text-to-table (T2T) generation.<|eot_id|>\n",
      "\n",
      "embed_layers= 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT-Eval is the Chinese Text-to-Table (CT-Eval) dataset and benchmark for evaluating the performance of Large Language Models (LLMs) on the task of generating tables from natural language text. It is a dataset specifically designed to assess the\n",
      "\n",
      "embed_layers= 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT-Eval stands for Chinese Text-to-Text Evaluation.<|eot_id|>\n",
      "\n",
      "embed_layers= 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT-Eval is the Chinese Text-to-Table (Text-to-Table) Evaluation dataset that I described in my previous response. It is a dataset specifically designed to evaluate the performance of language models in text-to-table tasks, particularly in the context of\n",
      "\n",
      "embed_layers= 30\n",
      "CT-Eval is the dataset and evaluation framework for the Text-to-Text (T2T) task, specifically designed for Chinese Text-to-Text (C2T) and English Text-to-Text (E2T).<|eot_id|>\n",
      "\n",
      "CPU times: user 2min 23s, sys: 43.2 s, total: 3min 6s\n",
      "Wall time: 3min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"embed_layers=\", ((i+1)*3))\n",
    "    print(pipe.generate(\"What is CT-Eval?\", embed_layers=((i+5)*2)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a045d93d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T21:13:36.167391Z",
     "iopub.status.busy": "2024-05-21T21:13:36.166956Z",
     "iopub.status.idle": "2024-05-21T21:13:57.690363Z",
     "shell.execute_reply": "2024-05-21T21:13:57.689223Z"
    },
    "papermill": {
     "duration": 21.543533,
     "end_time": "2024-05-21T21:13:57.692429",
     "exception": false,
     "start_time": "2024-05-21T21:13:36.148896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Usage Before Inference:\n",
      "[3643, 3897] MiB\n",
      "CT-Eval, short for Chinese Text-to-Table (CT-Eval), is a text-to-table evaluation dataset for text-based dialogue and question-answering tasks.<|eot_id|>\n",
      "GPU Usage After Inference:\n",
      "[3869, 5577] MiB\n",
      "Difference in GPU Memory Usage:\n",
      "[226, 1680] MiB\n"
     ]
    }
   ],
   "source": [
    "clear_cache()\n",
    "\n",
    "# GPU usage before inference\n",
    "memory_before = get_gpu_memory()\n",
    "print(\"GPU Usage Before Inference:\")\n",
    "print(memory_before, \"MiB\")\n",
    "\n",
    "# Inference code here\n",
    "print(pipe.generate(\"What is CT-Eval?\", embed_layers=12))\n",
    "\n",
    "# GPU usage after inference\n",
    "memory_after = get_gpu_memory()\n",
    "print(\"GPU Usage After Inference:\")\n",
    "print(memory_after, \"MiB\")\n",
    "\n",
    "# Calculate the difference in memory usage\n",
    "memory_difference = [after - before for before, after in zip(memory_before, memory_after)]\n",
    "print(\"Difference in GPU Memory Usage:\")\n",
    "print(memory_difference, \"MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442dff13",
   "metadata": {
    "papermill": {
     "duration": 0.016517,
     "end_time": "2024-05-21T21:13:57.727006",
     "exception": false,
     "start_time": "2024-05-21T21:13:57.710489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2e77362",
   "metadata": {
    "papermill": {
     "duration": 0.016338,
     "end_time": "2024-05-21T21:13:57.759974",
     "exception": false,
     "start_time": "2024-05-21T21:13:57.743636",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Compare to normal way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3534514e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T21:13:57.794768Z",
     "iopub.status.busy": "2024-05-21T21:13:57.794433Z",
     "iopub.status.idle": "2024-05-21T21:14:01.689865Z",
     "shell.execute_reply": "2024-05-21T21:14:01.688975Z"
    },
    "papermill": {
     "duration": 3.916241,
     "end_time": "2024-05-21T21:14:01.692787",
     "exception": false,
     "start_time": "2024-05-21T21:13:57.776546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del pipe\n",
    "\n",
    "clear_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ae0054a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T21:14:01.729407Z",
     "iopub.status.busy": "2024-05-21T21:14:01.728801Z",
     "iopub.status.idle": "2024-05-21T21:14:20.094633Z",
     "shell.execute_reply": "2024-05-21T21:14:20.093743Z"
    },
    "papermill": {
     "duration": 18.386132,
     "end_time": "2024-05-21T21:14:20.096905",
     "exception": false,
     "start_time": "2024-05-21T21:14:01.710773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90774a4511414330a5df0e8ccc6edee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "device_map = [('model.embed_tokens', 0)]\n",
    "device_map.extend([(f'model.layers.{i}', 0) for i in range(0, 16)])\n",
    "device_map.extend([(f'model.layers.{i}', 1) for i in range(16, 32)])\n",
    "device_map.extend([\n",
    "    ('model.norm', 1),\n",
    "    ('lm_head', 1),\n",
    "])\n",
    "device_map = {ii:jj for (ii,jj) in device_map}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CFG.model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=device_map,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d8b081",
   "metadata": {
    "papermill": {
     "duration": 0.017546,
     "end_time": "2024-05-21T21:14:20.132238",
     "exception": false,
     "start_time": "2024-05-21T21:14:20.114692",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## No Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f41a7129",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T21:14:20.168960Z",
     "iopub.status.busy": "2024-05-21T21:14:20.168129Z",
     "iopub.status.idle": "2024-05-21T21:14:20.178834Z",
     "shell.execute_reply": "2024-05-21T21:14:20.177939Z"
    },
    "papermill": {
     "duration": 0.031227,
     "end_time": "2024-05-21T21:14:20.180822",
     "exception": false,
     "start_time": "2024-05-21T21:14:20.149595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is CT-Eval?\"},\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a72e6db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T21:14:20.216946Z",
     "iopub.status.busy": "2024-05-21T21:14:20.216606Z",
     "iopub.status.idle": "2024-05-21T21:15:15.390580Z",
     "shell.execute_reply": "2024-05-21T21:15:15.389640Z"
    },
    "papermill": {
     "duration": 55.194883,
     "end_time": "2024-05-21T21:15:15.392840",
     "exception": false,
     "start_time": "2024-05-21T21:14:20.197957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is CT-Eval?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "CT-Eval is a performance evaluation software developed by the European Commission's Joint Research Centre (JRC) for assessing the performance of computer systems and algorithms in the field of Computer-Aided Translation (CAT).\n",
      "\n",
      "CT-Eval is designed to measure the quality\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is CT-Eval?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "CT-Eval is a type of computer-aided diagnosis (CAD) system designed to evaluate and analyze medical images, such as computed tomography (CT) scans. The system is used to help radiologists and healthcare professionals diagnose and detect various medical\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is CT-Eval?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "CT-Eval is a software tool used to evaluate the performance of a Computer Tomography (CT) scanner. It is a phantom-based quality control tool that is used to assess the accuracy and quality of the CT scanner's imaging capabilities.\n",
      "\n",
      "CT-Eval\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is CT-Eval?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "CT-Eval is a software package used to evaluate the performance of computer systems, particularly in the context of computer-aided design (CAD) and computer-aided engineering (CAE) applications. It is a widely used tool in various industries\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is CT-Eval?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "CT-Eval is a popular software tool used in the field of Computational Thinking (CT) and Computer Science (CS) education. It's a comprehensive evaluation system designed to assess students' understanding and skills in various programming concepts, algorithms, and problem-solving\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is CT-Eval?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "CT-Eval is a performance evaluation tool used in the context of computer networks and communication systems. It stands for \"Computerized Testing and Evaluation\" or \"Computerized Testing and Evaluation of Network Performance\".\n",
      "\n",
      "CT-Eval is a software tool used to test\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is CT-Eval?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "CT-Eval is a popular testing framework for evaluating and validating computer network protocols and devices. It's a network traffic generator and analyzer that can be used to simulate network scenarios, send and receive network packets, and analyze network performance.\n",
      "\n",
      "CT-Eval is\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is CT-Eval?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "CT-Eval is a performance evaluation and analysis tool used to measure the speed and accuracy of computer vision algorithms and deep learning models. It is specifically designed to evaluate the performance of computer vision models on a wide range of tasks, such as object detection,\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is CT-Eval?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "CT-Eval is a comprehensive evaluation tool for medical imaging software, commonly used in the medical imaging industry. It's an acronym for \"Computerized Tomography Evaluation\".\n",
      "\n",
      "CT-Eval is a standardized method for evaluating the performance of Computed Tomography (\n",
      "\n",
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is CT-Eval?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "CT-Eval is a popular open-source evaluation metric for comparing and assessing the performance of Natural Language Processing (NLP) and Text-to-Text (T2T) models, particularly in the context of machine translation, text summarization, and text\n",
      "\n",
      "CPU times: user 53.4 s, sys: 1.78 s, total: 55.2 s\n",
      "Wall time: 55.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(10):\n",
    "    outputs = model.generate(\n",
    "        **input_ids,\n",
    "        max_new_tokens=50,\n",
    "        temperature=1.0,\n",
    "    )\n",
    "    print(tokenizer.decode(outputs[0]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85f7a45c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T21:15:15.433743Z",
     "iopub.status.busy": "2024-05-21T21:15:15.432938Z",
     "iopub.status.idle": "2024-05-21T21:15:24.928679Z",
     "shell.execute_reply": "2024-05-21T21:15:24.927652Z"
    },
    "papermill": {
     "duration": 9.518831,
     "end_time": "2024-05-21T21:15:24.930886",
     "exception": false,
     "start_time": "2024-05-21T21:15:15.412055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Usage Before Inference:\n",
      "[2883, 2947] MiB\n",
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is CT-Eval?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "CT-Eval is a method for evaluating the quality of a CT (Computed Tomography) scan in medical imaging. It is a standardized scoring system used to assess the diagnostic accuracy and quality of a CT scan, particularly in the evaluation of chest and abdominal\n",
      "GPU Usage After Inference:\n",
      "[3181, 3185] MiB\n",
      "Difference in GPU Memory Usage:\n",
      "[298, 238] MiB\n"
     ]
    }
   ],
   "source": [
    "clear_cache()\n",
    "\n",
    "# GPU usage before inference\n",
    "memory_before = get_gpu_memory()\n",
    "print(\"GPU Usage Before Inference:\")\n",
    "print(memory_before, \"MiB\")\n",
    "\n",
    "# Inference code here\n",
    "outputs = model.generate(\n",
    "    **input_ids,\n",
    "    max_new_tokens=50,\n",
    "    temperature=1.0,\n",
    ")\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "# GPU usage after inference\n",
    "memory_after = get_gpu_memory()\n",
    "print(\"GPU Usage After Inference:\")\n",
    "print(memory_after, \"MiB\")\n",
    "\n",
    "# Calculate the difference in memory usage\n",
    "memory_difference = [after - before for before, after in zip(memory_before, memory_after)]\n",
    "print(\"Difference in GPU Memory Usage:\")\n",
    "print(memory_difference, \"MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581fe377",
   "metadata": {
    "papermill": {
     "duration": 0.018563,
     "end_time": "2024-05-21T21:15:24.969737",
     "exception": false,
     "start_time": "2024-05-21T21:15:24.951174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1df43dc",
   "metadata": {
    "papermill": {
     "duration": 0.0195,
     "end_time": "2024-05-21T21:15:25.008122",
     "exception": false,
     "start_time": "2024-05-21T21:15:24.988622",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Using Original Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0d72960",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T21:15:25.049547Z",
     "iopub.status.busy": "2024-05-21T21:15:25.049184Z",
     "iopub.status.idle": "2024-05-21T21:15:25.069716Z",
     "shell.execute_reply": "2024-05-21T21:15:25.068944Z"
    },
    "papermill": {
     "duration": 0.043422,
     "end_time": "2024-05-21T21:15:25.071967",
     "exception": false,
     "start_time": "2024-05-21T21:15:25.028545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"assistant\", \"content\": comp_prompt},\n",
    "    {\"role\": \"user\", \"content\": \"What is CT-Eval?\"},\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4678d36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T21:15:25.112591Z",
     "iopub.status.busy": "2024-05-21T21:15:25.112297Z",
     "iopub.status.idle": "2024-05-21T21:20:53.600166Z",
     "shell.execute_reply": "2024-05-21T21:20:53.599122Z"
    },
    "papermill": {
     "duration": 328.538864,
     "end_time": "2024-05-21T21:20:53.630214",
     "exception": false,
     "start_time": "2024-05-21T21:15:25.091350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "1 Introduction\n",
      "Information extraction (IE) aims to identify and extract structured information from unstructured text.\n",
      "1Codes and data will be publicly available once accepted.\n",
      "Basic IE sub-tasks, such as, named entity recognition and entity linking, generally operate at the\n",
      "sentence level, which ignore models’ ability to understand the document-level meaning. In contrast,\n",
      "Text-to-Table, an emerging sub-task of IE, requires\n",
      "models to understand information within a given\n",
      "document and then generate structured tables. Despite great success in the domain Wu et al. (2022);\n",
      "Li et al. (2023), previous studies are oriented to\n",
      "English, limiting the research in other languages.\n",
      "Recently, large language models (LLMs) have\n",
      "demonstrated powerful performance across various NLP tasks (Zhao et al., 2023; Wang et al.,\n",
      "2023a,b,c). Some studies (González-Gallardo et al.,\n",
      "2023; Gao et al., 2023) have also utilized LLMs for\n",
      "several IE sub-tasks, and found that compared to\n",
      "supervised baselines, the performance of LLMs is\n",
      "sub-optimal. However, their performance of LLMs\n",
      "on Text-to-Table remains largely unexplored. Additionally, the rapid advancements in LLMs have facilitated their widespread adoption in multi-lingual\n",
      "settings, enabling language modeling abilities to be\n",
      "shared across different languages. Consequently, it\n",
      "is theoretically possible to feasible to employ LLMs\n",
      "for text-to-table in other languages, an area that\n",
      "has yet to be thoroughly investigated.\n",
      "Motivated by the aforementioned considerations,\n",
      "we decide to benchmark LLMs on Chinese text-totable. A heuristic approach involves translating existing English datasets into Chinese for subsequent\n",
      "performance evaluation. There are four datasets\n",
      "widely used in text-to-table: E2E (Novikova et al.,\n",
      "2017) is a restaurant domain dataset encompassing\n",
      "51.5K samples about restaurant information like\n",
      "names, addresses, rate scores, etc. Rotowire (Wiseman et al., 2017) is a sports domain dataset derived\n",
      "from NBA basketball games with 4.9K samples\n",
      "about NBA team information. WikiBio (Lebret\n",
      "et al., 2016), compiled from Wikipedia, containing\n",
      "72.6K documents and corresponding structured tables in the biography domain. WikiTableText (Bao\n",
      "arXiv:2405.12174v1 [cs.CL] 20 May 2024\n",
      "et al., 2018) is also derived from Wikipedia, and\n",
      "involves 13.3K multidisciplinary samples spanning\n",
      "fields like finance and politics. However, according to our preliminary analysis, these datasets also\n",
      "suffer from the issues of less diversity or high hallucination, making them unsuitable to benchmark\n",
      "LLMs. Specifically, (1) E2E, Rotowire, and WikiBio exhibit a lack of diversity as they focus on a\n",
      "single domain, violating the core principle of instruction tuning in LLMs, that is, diversity. (2)\n",
      "Although WikiTableText incorporates multiple domains for diversity, our analysis (§ 3.5) indicates\n",
      "that 18.83% of samples exhibit hallucination in\n",
      "the golden tables, that is, containing additional information beyond the provided documents. This\n",
      "arises from treating Wikipedia infoboxes as golden\n",
      "tables, created collaboratively by online users and\n",
      "potentially containing additional basic information.\n",
      "In this paper, we propose the Chinese Text-toTable Evaluation (CT-Eval) dataset, which is constructed through three steps to ensure data diversity\n",
      "and minimize hallucination. To ensure diversity,\n",
      "the first step involves collecting multidisciplinary\n",
      "document-table pairs. We choose the Baidu Baike\n",
      "as the data source, which is a popular Chinese multidisciplinary online encyclopedia. Each page in\n",
      "this source contains text and an infobox summarizing the corresponding key information. Second, to\n",
      "minimize data hallucination, we train an LLM, as a\n",
      "hallucination judger, to filter out task samples with\n",
      "hallucination in their golden tables (infoboxes). Finally, we obtain 88.6K samples with an average\n",
      "length of 911.46 Chinese characters. We split them\n",
      "into 86.6K, 1K and 1K for training, validation and\n",
      "testing. For validation and testing samples, human\n",
      "annotators further clean data hallucination in the\n",
      "golden tables to ensure evaluation reliability.\n",
      "Based on the proposed CT-Eval, we benchmark\n",
      "various mainstream LLMs in both zero-shot (for\n",
      "both open- and closed-source LLMs) and finetuning (only for open-source LLMs) scenarios. Our\n",
      "experiments reveal that (1) GPT-4 achieves the best\n",
      "zero-shot performance among all LLMs. However,\n",
      "its performance remains a discernible disparity\n",
      "compared to human judgment. (2) After fine-tuning\n",
      "on the training set of CT-Eval, all open-source\n",
      "LLMs demonstrate a significant performance improvement, outperforming zero-shot GPT-4 by a\n",
      "large margin, indicating the effectiveness of CTEval. In-depth analyses of LLM-generated tables\n",
      "reveal the persistence of hallucination issues in\n",
      "both zero-shot and fine-tuned LLMs, highlighting\n",
      "a challenge in using LLMs as text-to-table systems.\n",
      "Future work could not only evaluate LLMs’ performance on text-to-table via our CT-Eval benchmark\n",
      "dataset, but also leverage its training data to improve LLMs’ text-to-table ability via fine-tuning.\n",
      "2 Related Work\n",
      "2.1 Text-to-table Tasks\n",
      "Wu et al. (2022) pioneer the text-to-table task.\n",
      "Given the absence of a dedicated text-to-table\n",
      "dataset, Wu et al. (2022) repurpose existing tableto-text datasets, i.e., WikiBio (Lebret et al., 2016),\n",
      "E2E (Novikova et al., 2017), Rotowire (Wiseman et al., 2017) and WikiTableText (Bao et al.,\n",
      "2018), for text-to-table tasks by reversing their\n",
      "input-output pairs. They fine-tune BART (Lewis\n",
      "et al., 2019) to perform text-to-table in a sequenceto-sequence manner, and find that the fine-tuned\n",
      "BART outperforms the pipeline baselines using\n",
      "relation extraction and named entity extraction.\n",
      "STable (Pietruszka et al., 2022) employs two pretrained language models (PLMs) (T5 (Raffel et al.,\n",
      "2020) and TILT (Powalski et al., 2021)) for text-totable, and designs a permutation-based decoder to\n",
      "enhance the PLMs’ table generation ability. Subsequently, Li et al. (2023) find that the predefined row\n",
      "order in golden tables introduces bias into text-totable models. Consequently, they train table header\n",
      "and table body generators separately to produce\n",
      "final tables. While these studies achieve notable\n",
      "success, they primarily explored text-to-table performance before the LLM era. In addition, their\n",
      "evaluation datasets generally focus on a single domain, and adapt from the table-to-text datasets, resulting in hallucination issues. Thus, they are unsuitable for benchmarking LLMs in text-to-table.\n",
      "2.2 Large Language Models\n",
      "The advent of advanced LLMs,e.g., ChatGPT (OpenAI, 2022), GPT-4 (OpenAI, 2023), marks a pivotal\n",
      "moment that propels the field of NLP into a boom\n",
      "phase. Zhong et al. (2023) show that LLMs can\n",
      "achieve decent performance on benchmarks like\n",
      "GLUE (Wang et al., 2018), which spans eight representative NLP understanding tasks. Concurrently,\n",
      "several studies have scrutinized the performance\n",
      "of LLMs across various IE tasks. For example,\n",
      "Gao et al. (2023) test the capability of ChatGPT\n",
      "in event extraction. Similarly, González-Gallardo\n",
      "et al. (2023) employ ChatGPT on historical entity\n",
      "recognition. However, the results of these IE subtasks consistently show that LLMs underperform\n",
      "compared to state-of-the-art supervised approaches.\n",
      "The text-to-table task we focused on is more complex than the previous basic IE sub-tasks, however,\n",
      "it remains an unexplored area for evaluation on\n",
      "LLMs.\n",
      "3 CT-Eval\n",
      "In this section, we first discuss the data source for\n",
      "building CT-Eval (§ 3.1). Then, we give the details\n",
      "of how to control the hallucination in the preliminary collected data, including LLM hallucination\n",
      "judger (§ 3.2) and human cleaning processes (§ 3.3).\n",
      "Finally, we formulate the text-to-table task (§ 3.4)\n",
      "and provide the details of data statistics (§ 3.5).\n",
      "3.1 Data Source\n",
      "Following the success of WikiTableText (Bao et al.,\n",
      "2018), we also choose a multidisciplinary online\n",
      "encyclopedia as the data source to ensure data diversity. After carefully comparing existing Chinese\n",
      "online encyclopedias, we choose Baidu Baike2\n",
      ",\n",
      "which is one of the Chinese encyclopedias with\n",
      "the most entries in the world.\n",
      "We obtain the Baidu Baike data from the dumps\n",
      "provided by Xu et al. (2017). The data contains\n",
      "over 9M entity pages, each of which includes an\n",
      "infobox and the corresponding textual description,\n",
      "forming a text-to-table sample. Utilizing this data,\n",
      "we implement the following rule-based strategy\n",
      "for preliminary data cleaning: (1) Each page must\n",
      "contain at least one infobox; otherwise, the golden\n",
      "table is missing. (2) The number of tabular cells in\n",
      "the infobox should exceed three to ensure validity.\n",
      "(3) The length of the textual description should\n",
      "surpass 200 tokens. After that, 200K documenttable pairs are remaining for further processing.\n",
      "3.2 LLM Data Cleaning\n",
      "After the initial cleaning process, the hallucination issue persists due to Baidu Baike’s submission\n",
      "rules, wherein most of the page text and infoboxes\n",
      "are edited and maintained by individuals. Thus,\n",
      "the contents in the infoboxes may not always align\n",
      "precisely with the textual documents. For instance,\n",
      "some infoboxes may include additional knowledge\n",
      "unrelated to the text, potentially misleading the\n",
      "model from learning the text-to-table task.\n",
      "2\n",
      "https://baike.baidu.com/\n",
      "§ There is a document and a golden table that\n",
      "summarize the information contained in this\n",
      "document. Please help me identify whether the\n",
      "golden table contains additional information\n",
      "beyond the document. Give the reason first and\n",
      "then provide your judgment.\n",
      "Figure 1: Illustration of judgment prompt.\n",
      "To control the hallucination in the task samples,\n",
      "we decide to employ an LLM as a hallucination\n",
      "judger to filter out samples exhibiting hallucination.\n",
      "While using GPT-4 directly as the LLM hallucination judger is a straightforward approach, utilizing\n",
      "official APIs can be costly. Therefore, we first\n",
      "randomly select 5K samples from the data before\n",
      "cleaning. Then, we use GPT-4 to assess whether the\n",
      "golden tables contain additional information. The\n",
      "judgment prompt is illustrated in Figure 1, where\n",
      "the LLM is tasked with evaluating hallucination in\n",
      "a chain-of-thought manner. Next, we use the GPT4 judgment results to train an open-source LLM,\n",
      "employing the trained model to evaluate the remaining samples. Samples containing hallucinations are\n",
      "discarded. Given the capacity for understanding\n",
      "lengthy documents of existing Chinese LLMs (Bai\n",
      "et al., 2023b), we select ChatGLM3-6B-32k3\n",
      "as\n",
      "the open-source LLM hallucination judger. Finally,\n",
      "there are 88.6K samples after the data cleaning\n",
      "by the LLM hallucination judger. These samples\n",
      "totally cover 28 domains, e.g., physics and religion.\n",
      "3.3 Human Data Cleaning\n",
      "We split the LLM-cleaned samples into the training, validation, and test sets with 84.6K, 1K and\n",
      "1K samples, respectively. In the validation and test\n",
      "sets, we balance the number of samples in each\n",
      "domain to ensure a comprehensive evaluation. Furthermore, to mitigate the hallucination issue in the\n",
      "validation and test sets, we employ human annotators to manually cleanse their golden tables.\n",
      "In this phase, we employ five human annotators\n",
      "and one data expert, all of whom are native Chinese\n",
      "speakers with advanced educational qualifications.\n",
      "The data expert is a researcher with extensive experience in IE research. We first organize a tutorial\n",
      "for the five annotators to ensure alignment of annotation requirements. This includes clarifying the\n",
      "concept of the hallucination issue, emphasizing the\n",
      "additional information present in tables that cannot\n",
      "be directly extracted from the corresponding documents. Next, for each document-table pair in the\n",
      "3\n",
      "https://huggingface.co/THUDM/chatglm3-6b-32k\n",
      "CT-Eval\n",
      "Industry\n",
      "Informatics\n",
      "Physics\n",
      "Astronomy\n",
      "Traffic Engineering\n",
      "Biology\n",
      "Medical Science\n",
      "Culture\n",
      "Religion\n",
      "History\n",
      "Geography\n",
      "Education\n",
      "Business\n",
      "Finance\n",
      "Management\n",
      "Movie and Book\n",
      "Science Fiction\n",
      "Music\n",
      "Animation\n",
      "Dwelling\n",
      "Traveling\n",
      "Career\n",
      "Healthy\n",
      "Food\n",
      "Cosmetics\n",
      "Profile\n",
      "Security\n",
      "Military\n",
      "STEM\n",
      "24.0%\n",
      "Social Science\n",
      "60.9%\n",
      "Life\n",
      "Support\n",
      "11.6%\n",
      "O.\n",
      "3.5\n",
      "%\n",
      "Figure 2: Domain distribution in CT-EVAL\n",
      "validation and test sets, three annotators are asked\n",
      "to remove the hallucination information from tables. The data expert reviews 10% of the manually\n",
      "cleaned samples from each annotator. If the accuracy rate falls below 95%, the respective annotator\n",
      "will be required to redo the annotation. Finally,\n",
      "for each sample, if the three manually cleaned results are consistent, the results are saved as the final\n",
      "data. Otherwise, the results are decided by a group\n",
      "meeting among all annotators and the data expert.\n",
      "3.4 Task Overview\n",
      "Given a document D = {w1, w2,..., w|D|}, where\n",
      "wi\n",
      "is the i-th word in D, the text-to-table task aims\n",
      "to extract key information from D and outputs a table T = {c0,0, c0,1,..., c0,n, c1,0, c1,1,..., c1,n,...,\n",
      "ci,j,..., cm,n}, where c0,k(k ∈ {0, 1,..., n}) represents the column header and ck,0(k ∈ {0, 1,..., n})\n",
      "denotes as the row header. ci,j (i > 0, j > 0) represents the text of the cell in the i-th row and j-th\n",
      "column in the table with m rows and n columns.\n",
      "3.5 Data Statistics\n",
      "We compare CT-Eval with the previous datasets\n",
      "across several metrics including language, number of entries, average length, number of domains,\n",
      "hallucination rate, and average number of cells.\n",
      "As shown in Table 1, compared to previous\n",
      "datasets that mostly focus on one domain, CT-Eval\n",
      "covers 28 domains, offering a valuable resource\n",
      "to evaluate the text-to-table capabilities of LLMs\n",
      "across multiple domains. These 28 domains can\n",
      "be categorized into four branches: STEM, social\n",
      "science, life support and others. To provide a comprehensive understanding of domain coverage in\n",
      "CT-Eval, we present the distributions of each domain in Figure 2. Notably, the “Social Science”\n",
      "Original Item\n",
      "Translation\n",
      "材料 做法 厨师一点通\n",
      "炖牛肉时，可以放几个\n",
      "山楂进去，这样牛肉会\n",
      "烂得快些，而且有股山\n",
      "楂的清香。\n",
      "1.牛肉洗净放入清水锅\n",
      "中煮至七成熟，捞出切\n",
      "成方块\n",
      "2. …\n",
      "材料：\n",
      "牛肉，胡萝卜，\n",
      "白萝卜，大料，\n",
      "香叶\n",
      "东坡牛肉\n",
      "Original Item\n",
      "Ingredients Cooking method Cooking Tips\n",
      "When stewing beef, you\n",
      "can put a few hawthorn\n",
      "into it, so that the beef\n",
      "will rot faster and have a\n",
      "hawthorn fragrance.\n",
      "1. Wash the beef and\n",
      "put it into a pot of\n",
      "water to boil until\n",
      "it is seven mature,\n",
      "fish out and cut it\n",
      "into squares…\n",
      "2. …\n",
      "Ingredients:\n",
      "Beef, carrots, white\n",
      "radish, Chinese\n",
      "spices, allspice\n",
      "Dongpo\n",
      "Beef\n",
      "“标题”: “东坡牛肉”, “ ”章节“: [{”标题“: ”东坡牛肉的做法“},\n",
      "内容“: ”葱段、姜片、蒜头、辣椒、香叶、大料、盐、白糖、番茄酱、排骨\n",
      "酱、辣椒酱、高汤（用煮牛肉的汤即可）。\n",
      "做法：\n",
      "1、先来处理一下牛肉，\n",
      "将牛腩在冷水中多泡一会…“}\n",
      "Translation\n",
      "“Title”: “Dongpo Beef”, “ ”Chapter“: [{”Title “: ”Dongpo Beef Practice“}, :\n",
      "Ingredients：Diced green onion, ginger, garlic, chili pepper, coriander, dashi, salt,\n",
      "sugar, tomato sauce, rib sauce, chili sauce, broth. Cooking method:1, first, let's deal\n",
      "with the beef, soak the brisket in cold water for a while longer...\"}\n",
      "Figure 3: Text-to-table example from CT-EVAL.\n",
      "branch exhibits the highest proportion, encompassing domains such as culture and religion. Conversely, the “Other” branch, which includes topics\n",
      "related to cosmetics and profiles, represents the\n",
      "smallest portion. The “STEM” (science, technology, engineering, and mathematics) branch features\n",
      "data related to topics such as physical sciences and\n",
      "astronomy. The “Life Support” branch is highly\n",
      "relevant to everyday life, comprises 11.60% of the\n",
      "data, and primarily focuses on domains such as\n",
      "dwelling and health. An illustration of a data sample from CT-Eval is depicted in Figure 3. Each\n",
      "document-table pair, akin to the example shown,\n",
      "consists of a textual description and a golden table\n",
      "with varying numbers of columns and rows.\n",
      "To assess the quality of our dataset compared\n",
      "to previous ones, we randomly select 200 samples\n",
      "from the training, validation and test sets of CTEval, and previous E2E, Rotowire, WikiBio, and\n",
      "WikiTableText, respectively. Then, we compute the\n",
      "hallucination rate for each dataset through human\n",
      "annotation. Specifically, three annotators proficient\n",
      "in both English and Chinese judged whether the\n",
      "golden tables contained hallucination information\n",
      "following the guidance outlined in Section 3.3. The\n",
      "hallucination rate for each dataset is determined\n",
      "by the average proportion of hallucinated samples\n",
      "judged by all three annotators. We observe that\n",
      "the hallucination rate of the CT-Eval training set\n",
      "is 6.83%, similar to Rotowire (6.17%). Moreover,\n",
      "with the help of our human cleaning, the hallucination rates in our test and validation sets decrease to\n",
      "1.00% and 1.50%, respectively, significantly lower\n",
      "Dataset Language N. Entries Avg. Length N. Domains Hallu. R Avg. Cells\n",
      "E2E English 42,061 90.58 Restaurant 4.17% 4.46\n",
      "Rotowire English 3,398 1311.01 Sports 6.17% 40.49\n",
      "WikiBio English 582,659 416.71 Biography 8.67% 4.19\n",
      "WikiTableText English 10,000 59.76 Multiple Subclasses 18.83% 4.25\n",
      "CT-Eval(train) Chinese 84,603 911.46 28 Subclasses 6.83% 11.40\n",
      "CT-Eval(val.) Chinese 1,000 813.32 28 Subclasses 1.00% 10.78\n",
      "CT-Eval(test) Chinese 1,000 845.22 28 Subclasses 1.50% 10.86\n",
      "Table 1: Data Statistics of CT-Eval and previous text-to-table datasets. “N. Entries” denotes the number of entries.\n",
      "“Hallu. R” indicates hallucination rate. “Avg.Cells” indicates the average number of table cells\n",
      "than those in other datasets. Therefore, the reliability of our dataset can be verified. Regarding\n",
      "domains, the E2E, Rotowire and WikiBio datasets\n",
      "focus on single domains, whereas WikiTableText\n",
      "and CT-Eval encompass multiple domains to ensure data diversity. Concerning the length of the input documents, we note that previous E2E and WikiTableText primarily contain short documents with\n",
      "an average length of under 100 words, whereas documents in our dataset and Rotowire exceed words\n",
      "or characters, presenting more complex inputs for\n",
      "text-to-table models. In summary, CT-Eval stands\n",
      "as the sole dataset fulfilling criteria of data diversity,\n",
      "lengthy documents, and low hallucination.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is CT-Eval?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "CT-Eval is a dataset designed to benchmark the performance of Large Language Models (LLMs) on the text-to-table task, specifically for Chinese. It is a text-to-table evaluation dataset that is constructed through a multi-step process to ensure data diversity\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "1 Introduction\n",
      "Information extraction (IE) aims to identify and extract structured information from unstructured text.\n",
      "1Codes and data will be publicly available once accepted.\n",
      "Basic IE sub-tasks, such as, named entity recognition and entity linking, generally operate at the\n",
      "sentence level, which ignore models’ ability to understand the document-level meaning. In contrast,\n",
      "Text-to-Table, an emerging sub-task of IE, requires\n",
      "models to understand information within a given\n",
      "document and then generate structured tables. Despite great success in the domain Wu et al. (2022);\n",
      "Li et al. (2023), previous studies are oriented to\n",
      "English, limiting the research in other languages.\n",
      "Recently, large language models (LLMs) have\n",
      "demonstrated powerful performance across various NLP tasks (Zhao et al., 2023; Wang et al.,\n",
      "2023a,b,c). Some studies (González-Gallardo et al.,\n",
      "2023; Gao et al., 2023) have also utilized LLMs for\n",
      "several IE sub-tasks, and found that compared to\n",
      "supervised baselines, the performance of LLMs is\n",
      "sub-optimal. However, their performance of LLMs\n",
      "on Text-to-Table remains largely unexplored. Additionally, the rapid advancements in LLMs have facilitated their widespread adoption in multi-lingual\n",
      "settings, enabling language modeling abilities to be\n",
      "shared across different languages. Consequently, it\n",
      "is theoretically possible to feasible to employ LLMs\n",
      "for text-to-table in other languages, an area that\n",
      "has yet to be thoroughly investigated.\n",
      "Motivated by the aforementioned considerations,\n",
      "we decide to benchmark LLMs on Chinese text-totable. A heuristic approach involves translating existing English datasets into Chinese for subsequent\n",
      "performance evaluation. There are four datasets\n",
      "widely used in text-to-table: E2E (Novikova et al.,\n",
      "2017) is a restaurant domain dataset encompassing\n",
      "51.5K samples about restaurant information like\n",
      "names, addresses, rate scores, etc. Rotowire (Wiseman et al., 2017) is a sports domain dataset derived\n",
      "from NBA basketball games with 4.9K samples\n",
      "about NBA team information. WikiBio (Lebret\n",
      "et al., 2016), compiled from Wikipedia, containing\n",
      "72.6K documents and corresponding structured tables in the biography domain. WikiTableText (Bao\n",
      "arXiv:2405.12174v1 [cs.CL] 20 May 2024\n",
      "et al., 2018) is also derived from Wikipedia, and\n",
      "involves 13.3K multidisciplinary samples spanning\n",
      "fields like finance and politics. However, according to our preliminary analysis, these datasets also\n",
      "suffer from the issues of less diversity or high hallucination, making them unsuitable to benchmark\n",
      "LLMs. Specifically, (1) E2E, Rotowire, and WikiBio exhibit a lack of diversity as they focus on a\n",
      "single domain, violating the core principle of instruction tuning in LLMs, that is, diversity. (2)\n",
      "Although WikiTableText incorporates multiple domains for diversity, our analysis (§ 3.5) indicates\n",
      "that 18.83% of samples exhibit hallucination in\n",
      "the golden tables, that is, containing additional information beyond the provided documents. This\n",
      "arises from treating Wikipedia infoboxes as golden\n",
      "tables, created collaboratively by online users and\n",
      "potentially containing additional basic information.\n",
      "In this paper, we propose the Chinese Text-toTable Evaluation (CT-Eval) dataset, which is constructed through three steps to ensure data diversity\n",
      "and minimize hallucination. To ensure diversity,\n",
      "the first step involves collecting multidisciplinary\n",
      "document-table pairs. We choose the Baidu Baike\n",
      "as the data source, which is a popular Chinese multidisciplinary online encyclopedia. Each page in\n",
      "this source contains text and an infobox summarizing the corresponding key information. Second, to\n",
      "minimize data hallucination, we train an LLM, as a\n",
      "hallucination judger, to filter out task samples with\n",
      "hallucination in their golden tables (infoboxes). Finally, we obtain 88.6K samples with an average\n",
      "length of 911.46 Chinese characters. We split them\n",
      "into 86.6K, 1K and 1K for training, validation and\n",
      "testing. For validation and testing samples, human\n",
      "annotators further clean data hallucination in the\n",
      "golden tables to ensure evaluation reliability.\n",
      "Based on the proposed CT-Eval, we benchmark\n",
      "various mainstream LLMs in both zero-shot (for\n",
      "both open- and closed-source LLMs) and finetuning (only for open-source LLMs) scenarios. Our\n",
      "experiments reveal that (1) GPT-4 achieves the best\n",
      "zero-shot performance among all LLMs. However,\n",
      "its performance remains a discernible disparity\n",
      "compared to human judgment. (2) After fine-tuning\n",
      "on the training set of CT-Eval, all open-source\n",
      "LLMs demonstrate a significant performance improvement, outperforming zero-shot GPT-4 by a\n",
      "large margin, indicating the effectiveness of CTEval. In-depth analyses of LLM-generated tables\n",
      "reveal the persistence of hallucination issues in\n",
      "both zero-shot and fine-tuned LLMs, highlighting\n",
      "a challenge in using LLMs as text-to-table systems.\n",
      "Future work could not only evaluate LLMs’ performance on text-to-table via our CT-Eval benchmark\n",
      "dataset, but also leverage its training data to improve LLMs’ text-to-table ability via fine-tuning.\n",
      "2 Related Work\n",
      "2.1 Text-to-table Tasks\n",
      "Wu et al. (2022) pioneer the text-to-table task.\n",
      "Given the absence of a dedicated text-to-table\n",
      "dataset, Wu et al. (2022) repurpose existing tableto-text datasets, i.e., WikiBio (Lebret et al., 2016),\n",
      "E2E (Novikova et al., 2017), Rotowire (Wiseman et al., 2017) and WikiTableText (Bao et al.,\n",
      "2018), for text-to-table tasks by reversing their\n",
      "input-output pairs. They fine-tune BART (Lewis\n",
      "et al., 2019) to perform text-to-table in a sequenceto-sequence manner, and find that the fine-tuned\n",
      "BART outperforms the pipeline baselines using\n",
      "relation extraction and named entity extraction.\n",
      "STable (Pietruszka et al., 2022) employs two pretrained language models (PLMs) (T5 (Raffel et al.,\n",
      "2020) and TILT (Powalski et al., 2021)) for text-totable, and designs a permutation-based decoder to\n",
      "enhance the PLMs’ table generation ability. Subsequently, Li et al. (2023) find that the predefined row\n",
      "order in golden tables introduces bias into text-totable models. Consequently, they train table header\n",
      "and table body generators separately to produce\n",
      "final tables. While these studies achieve notable\n",
      "success, they primarily explored text-to-table performance before the LLM era. In addition, their\n",
      "evaluation datasets generally focus on a single domain, and adapt from the table-to-text datasets, resulting in hallucination issues. Thus, they are unsuitable for benchmarking LLMs in text-to-table.\n",
      "2.2 Large Language Models\n",
      "The advent of advanced LLMs,e.g., ChatGPT (OpenAI, 2022), GPT-4 (OpenAI, 2023), marks a pivotal\n",
      "moment that propels the field of NLP into a boom\n",
      "phase. Zhong et al. (2023) show that LLMs can\n",
      "achieve decent performance on benchmarks like\n",
      "GLUE (Wang et al., 2018), which spans eight representative NLP understanding tasks. Concurrently,\n",
      "several studies have scrutinized the performance\n",
      "of LLMs across various IE tasks. For example,\n",
      "Gao et al. (2023) test the capability of ChatGPT\n",
      "in event extraction. Similarly, González-Gallardo\n",
      "et al. (2023) employ ChatGPT on historical entity\n",
      "recognition. However, the results of these IE subtasks consistently show that LLMs underperform\n",
      "compared to state-of-the-art supervised approaches.\n",
      "The text-to-table task we focused on is more complex than the previous basic IE sub-tasks, however,\n",
      "it remains an unexplored area for evaluation on\n",
      "LLMs.\n",
      "3 CT-Eval\n",
      "In this section, we first discuss the data source for\n",
      "building CT-Eval (§ 3.1). Then, we give the details\n",
      "of how to control the hallucination in the preliminary collected data, including LLM hallucination\n",
      "judger (§ 3.2) and human cleaning processes (§ 3.3).\n",
      "Finally, we formulate the text-to-table task (§ 3.4)\n",
      "and provide the details of data statistics (§ 3.5).\n",
      "3.1 Data Source\n",
      "Following the success of WikiTableText (Bao et al.,\n",
      "2018), we also choose a multidisciplinary online\n",
      "encyclopedia as the data source to ensure data diversity. After carefully comparing existing Chinese\n",
      "online encyclopedias, we choose Baidu Baike2\n",
      ",\n",
      "which is one of the Chinese encyclopedias with\n",
      "the most entries in the world.\n",
      "We obtain the Baidu Baike data from the dumps\n",
      "provided by Xu et al. (2017). The data contains\n",
      "over 9M entity pages, each of which includes an\n",
      "infobox and the corresponding textual description,\n",
      "forming a text-to-table sample. Utilizing this data,\n",
      "we implement the following rule-based strategy\n",
      "for preliminary data cleaning: (1) Each page must\n",
      "contain at least one infobox; otherwise, the golden\n",
      "table is missing. (2) The number of tabular cells in\n",
      "the infobox should exceed three to ensure validity.\n",
      "(3) The length of the textual description should\n",
      "surpass 200 tokens. After that, 200K documenttable pairs are remaining for further processing.\n",
      "3.2 LLM Data Cleaning\n",
      "After the initial cleaning process, the hallucination issue persists due to Baidu Baike’s submission\n",
      "rules, wherein most of the page text and infoboxes\n",
      "are edited and maintained by individuals. Thus,\n",
      "the contents in the infoboxes may not always align\n",
      "precisely with the textual documents. For instance,\n",
      "some infoboxes may include additional knowledge\n",
      "unrelated to the text, potentially misleading the\n",
      "model from learning the text-to-table task.\n",
      "2\n",
      "https://baike.baidu.com/\n",
      "§ There is a document and a golden table that\n",
      "summarize the information contained in this\n",
      "document. Please help me identify whether the\n",
      "golden table contains additional information\n",
      "beyond the document. Give the reason first and\n",
      "then provide your judgment.\n",
      "Figure 1: Illustration of judgment prompt.\n",
      "To control the hallucination in the task samples,\n",
      "we decide to employ an LLM as a hallucination\n",
      "judger to filter out samples exhibiting hallucination.\n",
      "While using GPT-4 directly as the LLM hallucination judger is a straightforward approach, utilizing\n",
      "official APIs can be costly. Therefore, we first\n",
      "randomly select 5K samples from the data before\n",
      "cleaning. Then, we use GPT-4 to assess whether the\n",
      "golden tables contain additional information. The\n",
      "judgment prompt is illustrated in Figure 1, where\n",
      "the LLM is tasked with evaluating hallucination in\n",
      "a chain-of-thought manner. Next, we use the GPT4 judgment results to train an open-source LLM,\n",
      "employing the trained model to evaluate the remaining samples. Samples containing hallucinations are\n",
      "discarded. Given the capacity for understanding\n",
      "lengthy documents of existing Chinese LLMs (Bai\n",
      "et al., 2023b), we select ChatGLM3-6B-32k3\n",
      "as\n",
      "the open-source LLM hallucination judger. Finally,\n",
      "there are 88.6K samples after the data cleaning\n",
      "by the LLM hallucination judger. These samples\n",
      "totally cover 28 domains, e.g., physics and religion.\n",
      "3.3 Human Data Cleaning\n",
      "We split the LLM-cleaned samples into the training, validation, and test sets with 84.6K, 1K and\n",
      "1K samples, respectively. In the validation and test\n",
      "sets, we balance the number of samples in each\n",
      "domain to ensure a comprehensive evaluation. Furthermore, to mitigate the hallucination issue in the\n",
      "validation and test sets, we employ human annotators to manually cleanse their golden tables.\n",
      "In this phase, we employ five human annotators\n",
      "and one data expert, all of whom are native Chinese\n",
      "speakers with advanced educational qualifications.\n",
      "The data expert is a researcher with extensive experience in IE research. We first organize a tutorial\n",
      "for the five annotators to ensure alignment of annotation requirements. This includes clarifying the\n",
      "concept of the hallucination issue, emphasizing the\n",
      "additional information present in tables that cannot\n",
      "be directly extracted from the corresponding documents. Next, for each document-table pair in the\n",
      "3\n",
      "https://huggingface.co/THUDM/chatglm3-6b-32k\n",
      "CT-Eval\n",
      "Industry\n",
      "Informatics\n",
      "Physics\n",
      "Astronomy\n",
      "Traffic Engineering\n",
      "Biology\n",
      "Medical Science\n",
      "Culture\n",
      "Religion\n",
      "History\n",
      "Geography\n",
      "Education\n",
      "Business\n",
      "Finance\n",
      "Management\n",
      "Movie and Book\n",
      "Science Fiction\n",
      "Music\n",
      "Animation\n",
      "Dwelling\n",
      "Traveling\n",
      "Career\n",
      "Healthy\n",
      "Food\n",
      "Cosmetics\n",
      "Profile\n",
      "Security\n",
      "Military\n",
      "STEM\n",
      "24.0%\n",
      "Social Science\n",
      "60.9%\n",
      "Life\n",
      "Support\n",
      "11.6%\n",
      "O.\n",
      "3.5\n",
      "%\n",
      "Figure 2: Domain distribution in CT-EVAL\n",
      "validation and test sets, three annotators are asked\n",
      "to remove the hallucination information from tables. The data expert reviews 10% of the manually\n",
      "cleaned samples from each annotator. If the accuracy rate falls below 95%, the respective annotator\n",
      "will be required to redo the annotation. Finally,\n",
      "for each sample, if the three manually cleaned results are consistent, the results are saved as the final\n",
      "data. Otherwise, the results are decided by a group\n",
      "meeting among all annotators and the data expert.\n",
      "3.4 Task Overview\n",
      "Given a document D = {w1, w2,..., w|D|}, where\n",
      "wi\n",
      "is the i-th word in D, the text-to-table task aims\n",
      "to extract key information from D and outputs a table T = {c0,0, c0,1,..., c0,n, c1,0, c1,1,..., c1,n,...,\n",
      "ci,j,..., cm,n}, where c0,k(k ∈ {0, 1,..., n}) represents the column header and ck,0(k ∈ {0, 1,..., n})\n",
      "denotes as the row header. ci,j (i > 0, j > 0) represents the text of the cell in the i-th row and j-th\n",
      "column in the table with m rows and n columns.\n",
      "3.5 Data Statistics\n",
      "We compare CT-Eval with the previous datasets\n",
      "across several metrics including language, number of entries, average length, number of domains,\n",
      "hallucination rate, and average number of cells.\n",
      "As shown in Table 1, compared to previous\n",
      "datasets that mostly focus on one domain, CT-Eval\n",
      "covers 28 domains, offering a valuable resource\n",
      "to evaluate the text-to-table capabilities of LLMs\n",
      "across multiple domains. These 28 domains can\n",
      "be categorized into four branches: STEM, social\n",
      "science, life support and others. To provide a comprehensive understanding of domain coverage in\n",
      "CT-Eval, we present the distributions of each domain in Figure 2. Notably, the “Social Science”\n",
      "Original Item\n",
      "Translation\n",
      "材料 做法 厨师一点通\n",
      "炖牛肉时，可以放几个\n",
      "山楂进去，这样牛肉会\n",
      "烂得快些，而且有股山\n",
      "楂的清香。\n",
      "1.牛肉洗净放入清水锅\n",
      "中煮至七成熟，捞出切\n",
      "成方块\n",
      "2. …\n",
      "材料：\n",
      "牛肉，胡萝卜，\n",
      "白萝卜，大料，\n",
      "香叶\n",
      "东坡牛肉\n",
      "Original Item\n",
      "Ingredients Cooking method Cooking Tips\n",
      "When stewing beef, you\n",
      "can put a few hawthorn\n",
      "into it, so that the beef\n",
      "will rot faster and have a\n",
      "hawthorn fragrance.\n",
      "1. Wash the beef and\n",
      "put it into a pot of\n",
      "water to boil until\n",
      "it is seven mature,\n",
      "fish out and cut it\n",
      "into squares…\n",
      "2. …\n",
      "Ingredients:\n",
      "Beef, carrots, white\n",
      "radish, Chinese\n",
      "spices, allspice\n",
      "Dongpo\n",
      "Beef\n",
      "“标题”: “东坡牛肉”, “ ”章节“: [{”标题“: ”东坡牛肉的做法“},\n",
      "内容“: ”葱段、姜片、蒜头、辣椒、香叶、大料、盐、白糖、番茄酱、排骨\n",
      "酱、辣椒酱、高汤（用煮牛肉的汤即可）。\n",
      "做法：\n",
      "1、先来处理一下牛肉，\n",
      "将牛腩在冷水中多泡一会…“}\n",
      "Translation\n",
      "“Title”: “Dongpo Beef”, “ ”Chapter“: [{”Title “: ”Dongpo Beef Practice“}, :\n",
      "Ingredients：Diced green onion, ginger, garlic, chili pepper, coriander, dashi, salt,\n",
      "sugar, tomato sauce, rib sauce, chili sauce, broth. Cooking method:1, first, let's deal\n",
      "with the beef, soak the brisket in cold water for a while longer...\"}\n",
      "Figure 3: Text-to-table example from CT-EVAL.\n",
      "branch exhibits the highest proportion, encompassing domains such as culture and religion. Conversely, the “Other” branch, which includes topics\n",
      "related to cosmetics and profiles, represents the\n",
      "smallest portion. The “STEM” (science, technology, engineering, and mathematics) branch features\n",
      "data related to topics such as physical sciences and\n",
      "astronomy. The “Life Support” branch is highly\n",
      "relevant to everyday life, comprises 11.60% of the\n",
      "data, and primarily focuses on domains such as\n",
      "dwelling and health. An illustration of a data sample from CT-Eval is depicted in Figure 3. Each\n",
      "document-table pair, akin to the example shown,\n",
      "consists of a textual description and a golden table\n",
      "with varying numbers of columns and rows.\n",
      "To assess the quality of our dataset compared\n",
      "to previous ones, we randomly select 200 samples\n",
      "from the training, validation and test sets of CTEval, and previous E2E, Rotowire, WikiBio, and\n",
      "WikiTableText, respectively. Then, we compute the\n",
      "hallucination rate for each dataset through human\n",
      "annotation. Specifically, three annotators proficient\n",
      "in both English and Chinese judged whether the\n",
      "golden tables contained hallucination information\n",
      "following the guidance outlined in Section 3.3. The\n",
      "hallucination rate for each dataset is determined\n",
      "by the average proportion of hallucinated samples\n",
      "judged by all three annotators. We observe that\n",
      "the hallucination rate of the CT-Eval training set\n",
      "is 6.83%, similar to Rotowire (6.17%). Moreover,\n",
      "with the help of our human cleaning, the hallucination rates in our test and validation sets decrease to\n",
      "1.00% and 1.50%, respectively, significantly lower\n",
      "Dataset Language N. Entries Avg. Length N. Domains Hallu. R Avg. Cells\n",
      "E2E English 42,061 90.58 Restaurant 4.17% 4.46\n",
      "Rotowire English 3,398 1311.01 Sports 6.17% 40.49\n",
      "WikiBio English 582,659 416.71 Biography 8.67% 4.19\n",
      "WikiTableText English 10,000 59.76 Multiple Subclasses 18.83% 4.25\n",
      "CT-Eval(train) Chinese 84,603 911.46 28 Subclasses 6.83% 11.40\n",
      "CT-Eval(val.) Chinese 1,000 813.32 28 Subclasses 1.00% 10.78\n",
      "CT-Eval(test) Chinese 1,000 845.22 28 Subclasses 1.50% 10.86\n",
      "Table 1: Data Statistics of CT-Eval and previous text-to-table datasets. “N. Entries” denotes the number of entries.\n",
      "“Hallu. R” indicates hallucination rate. “Avg.Cells” indicates the average number of table cells\n",
      "than those in other datasets. Therefore, the reliability of our dataset can be verified. Regarding\n",
      "domains, the E2E, Rotowire and WikiBio datasets\n",
      "focus on single domains, whereas WikiTableText\n",
      "and CT-Eval encompass multiple domains to ensure data diversity. Concerning the length of the input documents, we note that previous E2E and WikiTableText primarily contain short documents with\n",
      "an average length of under 100 words, whereas documents in our dataset and Rotowire exceed words\n",
      "or characters, presenting more complex inputs for\n",
      "text-to-table models. In summary, CT-Eval stands\n",
      "as the sole dataset fulfilling criteria of data diversity,\n",
      "lengthy documents, and low hallucination.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is CT-Eval?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "CT-Eval is a dataset for the text-to-table task in natural language processing. It is a collection of document-table pairs in Chinese, designed to evaluate the performance of large language models (LLMs) in extracting structured information from unstructured text.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "1 Introduction\n",
      "Information extraction (IE) aims to identify and extract structured information from unstructured text.\n",
      "1Codes and data will be publicly available once accepted.\n",
      "Basic IE sub-tasks, such as, named entity recognition and entity linking, generally operate at the\n",
      "sentence level, which ignore models’ ability to understand the document-level meaning. In contrast,\n",
      "Text-to-Table, an emerging sub-task of IE, requires\n",
      "models to understand information within a given\n",
      "document and then generate structured tables. Despite great success in the domain Wu et al. (2022);\n",
      "Li et al. (2023), previous studies are oriented to\n",
      "English, limiting the research in other languages.\n",
      "Recently, large language models (LLMs) have\n",
      "demonstrated powerful performance across various NLP tasks (Zhao et al., 2023; Wang et al.,\n",
      "2023a,b,c). Some studies (González-Gallardo et al.,\n",
      "2023; Gao et al., 2023) have also utilized LLMs for\n",
      "several IE sub-tasks, and found that compared to\n",
      "supervised baselines, the performance of LLMs is\n",
      "sub-optimal. However, their performance of LLMs\n",
      "on Text-to-Table remains largely unexplored. Additionally, the rapid advancements in LLMs have facilitated their widespread adoption in multi-lingual\n",
      "settings, enabling language modeling abilities to be\n",
      "shared across different languages. Consequently, it\n",
      "is theoretically possible to feasible to employ LLMs\n",
      "for text-to-table in other languages, an area that\n",
      "has yet to be thoroughly investigated.\n",
      "Motivated by the aforementioned considerations,\n",
      "we decide to benchmark LLMs on Chinese text-totable. A heuristic approach involves translating existing English datasets into Chinese for subsequent\n",
      "performance evaluation. There are four datasets\n",
      "widely used in text-to-table: E2E (Novikova et al.,\n",
      "2017) is a restaurant domain dataset encompassing\n",
      "51.5K samples about restaurant information like\n",
      "names, addresses, rate scores, etc. Rotowire (Wiseman et al., 2017) is a sports domain dataset derived\n",
      "from NBA basketball games with 4.9K samples\n",
      "about NBA team information. WikiBio (Lebret\n",
      "et al., 2016), compiled from Wikipedia, containing\n",
      "72.6K documents and corresponding structured tables in the biography domain. WikiTableText (Bao\n",
      "arXiv:2405.12174v1 [cs.CL] 20 May 2024\n",
      "et al., 2018) is also derived from Wikipedia, and\n",
      "involves 13.3K multidisciplinary samples spanning\n",
      "fields like finance and politics. However, according to our preliminary analysis, these datasets also\n",
      "suffer from the issues of less diversity or high hallucination, making them unsuitable to benchmark\n",
      "LLMs. Specifically, (1) E2E, Rotowire, and WikiBio exhibit a lack of diversity as they focus on a\n",
      "single domain, violating the core principle of instruction tuning in LLMs, that is, diversity. (2)\n",
      "Although WikiTableText incorporates multiple domains for diversity, our analysis (§ 3.5) indicates\n",
      "that 18.83% of samples exhibit hallucination in\n",
      "the golden tables, that is, containing additional information beyond the provided documents. This\n",
      "arises from treating Wikipedia infoboxes as golden\n",
      "tables, created collaboratively by online users and\n",
      "potentially containing additional basic information.\n",
      "In this paper, we propose the Chinese Text-toTable Evaluation (CT-Eval) dataset, which is constructed through three steps to ensure data diversity\n",
      "and minimize hallucination. To ensure diversity,\n",
      "the first step involves collecting multidisciplinary\n",
      "document-table pairs. We choose the Baidu Baike\n",
      "as the data source, which is a popular Chinese multidisciplinary online encyclopedia. Each page in\n",
      "this source contains text and an infobox summarizing the corresponding key information. Second, to\n",
      "minimize data hallucination, we train an LLM, as a\n",
      "hallucination judger, to filter out task samples with\n",
      "hallucination in their golden tables (infoboxes). Finally, we obtain 88.6K samples with an average\n",
      "length of 911.46 Chinese characters. We split them\n",
      "into 86.6K, 1K and 1K for training, validation and\n",
      "testing. For validation and testing samples, human\n",
      "annotators further clean data hallucination in the\n",
      "golden tables to ensure evaluation reliability.\n",
      "Based on the proposed CT-Eval, we benchmark\n",
      "various mainstream LLMs in both zero-shot (for\n",
      "both open- and closed-source LLMs) and finetuning (only for open-source LLMs) scenarios. Our\n",
      "experiments reveal that (1) GPT-4 achieves the best\n",
      "zero-shot performance among all LLMs. However,\n",
      "its performance remains a discernible disparity\n",
      "compared to human judgment. (2) After fine-tuning\n",
      "on the training set of CT-Eval, all open-source\n",
      "LLMs demonstrate a significant performance improvement, outperforming zero-shot GPT-4 by a\n",
      "large margin, indicating the effectiveness of CTEval. In-depth analyses of LLM-generated tables\n",
      "reveal the persistence of hallucination issues in\n",
      "both zero-shot and fine-tuned LLMs, highlighting\n",
      "a challenge in using LLMs as text-to-table systems.\n",
      "Future work could not only evaluate LLMs’ performance on text-to-table via our CT-Eval benchmark\n",
      "dataset, but also leverage its training data to improve LLMs’ text-to-table ability via fine-tuning.\n",
      "2 Related Work\n",
      "2.1 Text-to-table Tasks\n",
      "Wu et al. (2022) pioneer the text-to-table task.\n",
      "Given the absence of a dedicated text-to-table\n",
      "dataset, Wu et al. (2022) repurpose existing tableto-text datasets, i.e., WikiBio (Lebret et al., 2016),\n",
      "E2E (Novikova et al., 2017), Rotowire (Wiseman et al., 2017) and WikiTableText (Bao et al.,\n",
      "2018), for text-to-table tasks by reversing their\n",
      "input-output pairs. They fine-tune BART (Lewis\n",
      "et al., 2019) to perform text-to-table in a sequenceto-sequence manner, and find that the fine-tuned\n",
      "BART outperforms the pipeline baselines using\n",
      "relation extraction and named entity extraction.\n",
      "STable (Pietruszka et al., 2022) employs two pretrained language models (PLMs) (T5 (Raffel et al.,\n",
      "2020) and TILT (Powalski et al., 2021)) for text-totable, and designs a permutation-based decoder to\n",
      "enhance the PLMs’ table generation ability. Subsequently, Li et al. (2023) find that the predefined row\n",
      "order in golden tables introduces bias into text-totable models. Consequently, they train table header\n",
      "and table body generators separately to produce\n",
      "final tables. While these studies achieve notable\n",
      "success, they primarily explored text-to-table performance before the LLM era. In addition, their\n",
      "evaluation datasets generally focus on a single domain, and adapt from the table-to-text datasets, resulting in hallucination issues. Thus, they are unsuitable for benchmarking LLMs in text-to-table.\n",
      "2.2 Large Language Models\n",
      "The advent of advanced LLMs,e.g., ChatGPT (OpenAI, 2022), GPT-4 (OpenAI, 2023), marks a pivotal\n",
      "moment that propels the field of NLP into a boom\n",
      "phase. Zhong et al. (2023) show that LLMs can\n",
      "achieve decent performance on benchmarks like\n",
      "GLUE (Wang et al., 2018), which spans eight representative NLP understanding tasks. Concurrently,\n",
      "several studies have scrutinized the performance\n",
      "of LLMs across various IE tasks. For example,\n",
      "Gao et al. (2023) test the capability of ChatGPT\n",
      "in event extraction. Similarly, González-Gallardo\n",
      "et al. (2023) employ ChatGPT on historical entity\n",
      "recognition. However, the results of these IE subtasks consistently show that LLMs underperform\n",
      "compared to state-of-the-art supervised approaches.\n",
      "The text-to-table task we focused on is more complex than the previous basic IE sub-tasks, however,\n",
      "it remains an unexplored area for evaluation on\n",
      "LLMs.\n",
      "3 CT-Eval\n",
      "In this section, we first discuss the data source for\n",
      "building CT-Eval (§ 3.1). Then, we give the details\n",
      "of how to control the hallucination in the preliminary collected data, including LLM hallucination\n",
      "judger (§ 3.2) and human cleaning processes (§ 3.3).\n",
      "Finally, we formulate the text-to-table task (§ 3.4)\n",
      "and provide the details of data statistics (§ 3.5).\n",
      "3.1 Data Source\n",
      "Following the success of WikiTableText (Bao et al.,\n",
      "2018), we also choose a multidisciplinary online\n",
      "encyclopedia as the data source to ensure data diversity. After carefully comparing existing Chinese\n",
      "online encyclopedias, we choose Baidu Baike2\n",
      ",\n",
      "which is one of the Chinese encyclopedias with\n",
      "the most entries in the world.\n",
      "We obtain the Baidu Baike data from the dumps\n",
      "provided by Xu et al. (2017). The data contains\n",
      "over 9M entity pages, each of which includes an\n",
      "infobox and the corresponding textual description,\n",
      "forming a text-to-table sample. Utilizing this data,\n",
      "we implement the following rule-based strategy\n",
      "for preliminary data cleaning: (1) Each page must\n",
      "contain at least one infobox; otherwise, the golden\n",
      "table is missing. (2) The number of tabular cells in\n",
      "the infobox should exceed three to ensure validity.\n",
      "(3) The length of the textual description should\n",
      "surpass 200 tokens. After that, 200K documenttable pairs are remaining for further processing.\n",
      "3.2 LLM Data Cleaning\n",
      "After the initial cleaning process, the hallucination issue persists due to Baidu Baike’s submission\n",
      "rules, wherein most of the page text and infoboxes\n",
      "are edited and maintained by individuals. Thus,\n",
      "the contents in the infoboxes may not always align\n",
      "precisely with the textual documents. For instance,\n",
      "some infoboxes may include additional knowledge\n",
      "unrelated to the text, potentially misleading the\n",
      "model from learning the text-to-table task.\n",
      "2\n",
      "https://baike.baidu.com/\n",
      "§ There is a document and a golden table that\n",
      "summarize the information contained in this\n",
      "document. Please help me identify whether the\n",
      "golden table contains additional information\n",
      "beyond the document. Give the reason first and\n",
      "then provide your judgment.\n",
      "Figure 1: Illustration of judgment prompt.\n",
      "To control the hallucination in the task samples,\n",
      "we decide to employ an LLM as a hallucination\n",
      "judger to filter out samples exhibiting hallucination.\n",
      "While using GPT-4 directly as the LLM hallucination judger is a straightforward approach, utilizing\n",
      "official APIs can be costly. Therefore, we first\n",
      "randomly select 5K samples from the data before\n",
      "cleaning. Then, we use GPT-4 to assess whether the\n",
      "golden tables contain additional information. The\n",
      "judgment prompt is illustrated in Figure 1, where\n",
      "the LLM is tasked with evaluating hallucination in\n",
      "a chain-of-thought manner. Next, we use the GPT4 judgment results to train an open-source LLM,\n",
      "employing the trained model to evaluate the remaining samples. Samples containing hallucinations are\n",
      "discarded. Given the capacity for understanding\n",
      "lengthy documents of existing Chinese LLMs (Bai\n",
      "et al., 2023b), we select ChatGLM3-6B-32k3\n",
      "as\n",
      "the open-source LLM hallucination judger. Finally,\n",
      "there are 88.6K samples after the data cleaning\n",
      "by the LLM hallucination judger. These samples\n",
      "totally cover 28 domains, e.g., physics and religion.\n",
      "3.3 Human Data Cleaning\n",
      "We split the LLM-cleaned samples into the training, validation, and test sets with 84.6K, 1K and\n",
      "1K samples, respectively. In the validation and test\n",
      "sets, we balance the number of samples in each\n",
      "domain to ensure a comprehensive evaluation. Furthermore, to mitigate the hallucination issue in the\n",
      "validation and test sets, we employ human annotators to manually cleanse their golden tables.\n",
      "In this phase, we employ five human annotators\n",
      "and one data expert, all of whom are native Chinese\n",
      "speakers with advanced educational qualifications.\n",
      "The data expert is a researcher with extensive experience in IE research. We first organize a tutorial\n",
      "for the five annotators to ensure alignment of annotation requirements. This includes clarifying the\n",
      "concept of the hallucination issue, emphasizing the\n",
      "additional information present in tables that cannot\n",
      "be directly extracted from the corresponding documents. Next, for each document-table pair in the\n",
      "3\n",
      "https://huggingface.co/THUDM/chatglm3-6b-32k\n",
      "CT-Eval\n",
      "Industry\n",
      "Informatics\n",
      "Physics\n",
      "Astronomy\n",
      "Traffic Engineering\n",
      "Biology\n",
      "Medical Science\n",
      "Culture\n",
      "Religion\n",
      "History\n",
      "Geography\n",
      "Education\n",
      "Business\n",
      "Finance\n",
      "Management\n",
      "Movie and Book\n",
      "Science Fiction\n",
      "Music\n",
      "Animation\n",
      "Dwelling\n",
      "Traveling\n",
      "Career\n",
      "Healthy\n",
      "Food\n",
      "Cosmetics\n",
      "Profile\n",
      "Security\n",
      "Military\n",
      "STEM\n",
      "24.0%\n",
      "Social Science\n",
      "60.9%\n",
      "Life\n",
      "Support\n",
      "11.6%\n",
      "O.\n",
      "3.5\n",
      "%\n",
      "Figure 2: Domain distribution in CT-EVAL\n",
      "validation and test sets, three annotators are asked\n",
      "to remove the hallucination information from tables. The data expert reviews 10% of the manually\n",
      "cleaned samples from each annotator. If the accuracy rate falls below 95%, the respective annotator\n",
      "will be required to redo the annotation. Finally,\n",
      "for each sample, if the three manually cleaned results are consistent, the results are saved as the final\n",
      "data. Otherwise, the results are decided by a group\n",
      "meeting among all annotators and the data expert.\n",
      "3.4 Task Overview\n",
      "Given a document D = {w1, w2,..., w|D|}, where\n",
      "wi\n",
      "is the i-th word in D, the text-to-table task aims\n",
      "to extract key information from D and outputs a table T = {c0,0, c0,1,..., c0,n, c1,0, c1,1,..., c1,n,...,\n",
      "ci,j,..., cm,n}, where c0,k(k ∈ {0, 1,..., n}) represents the column header and ck,0(k ∈ {0, 1,..., n})\n",
      "denotes as the row header. ci,j (i > 0, j > 0) represents the text of the cell in the i-th row and j-th\n",
      "column in the table with m rows and n columns.\n",
      "3.5 Data Statistics\n",
      "We compare CT-Eval with the previous datasets\n",
      "across several metrics including language, number of entries, average length, number of domains,\n",
      "hallucination rate, and average number of cells.\n",
      "As shown in Table 1, compared to previous\n",
      "datasets that mostly focus on one domain, CT-Eval\n",
      "covers 28 domains, offering a valuable resource\n",
      "to evaluate the text-to-table capabilities of LLMs\n",
      "across multiple domains. These 28 domains can\n",
      "be categorized into four branches: STEM, social\n",
      "science, life support and others. To provide a comprehensive understanding of domain coverage in\n",
      "CT-Eval, we present the distributions of each domain in Figure 2. Notably, the “Social Science”\n",
      "Original Item\n",
      "Translation\n",
      "材料 做法 厨师一点通\n",
      "炖牛肉时，可以放几个\n",
      "山楂进去，这样牛肉会\n",
      "烂得快些，而且有股山\n",
      "楂的清香。\n",
      "1.牛肉洗净放入清水锅\n",
      "中煮至七成熟，捞出切\n",
      "成方块\n",
      "2. …\n",
      "材料：\n",
      "牛肉，胡萝卜，\n",
      "白萝卜，大料，\n",
      "香叶\n",
      "东坡牛肉\n",
      "Original Item\n",
      "Ingredients Cooking method Cooking Tips\n",
      "When stewing beef, you\n",
      "can put a few hawthorn\n",
      "into it, so that the beef\n",
      "will rot faster and have a\n",
      "hawthorn fragrance.\n",
      "1. Wash the beef and\n",
      "put it into a pot of\n",
      "water to boil until\n",
      "it is seven mature,\n",
      "fish out and cut it\n",
      "into squares…\n",
      "2. …\n",
      "Ingredients:\n",
      "Beef, carrots, white\n",
      "radish, Chinese\n",
      "spices, allspice\n",
      "Dongpo\n",
      "Beef\n",
      "“标题”: “东坡牛肉”, “ ”章节“: [{”标题“: ”东坡牛肉的做法“},\n",
      "内容“: ”葱段、姜片、蒜头、辣椒、香叶、大料、盐、白糖、番茄酱、排骨\n",
      "酱、辣椒酱、高汤（用煮牛肉的汤即可）。\n",
      "做法：\n",
      "1、先来处理一下牛肉，\n",
      "将牛腩在冷水中多泡一会…“}\n",
      "Translation\n",
      "“Title”: “Dongpo Beef”, “ ”Chapter“: [{”Title “: ”Dongpo Beef Practice“}, :\n",
      "Ingredients：Diced green onion, ginger, garlic, chili pepper, coriander, dashi, salt,\n",
      "sugar, tomato sauce, rib sauce, chili sauce, broth. Cooking method:1, first, let's deal\n",
      "with the beef, soak the brisket in cold water for a while longer...\"}\n",
      "Figure 3: Text-to-table example from CT-EVAL.\n",
      "branch exhibits the highest proportion, encompassing domains such as culture and religion. Conversely, the “Other” branch, which includes topics\n",
      "related to cosmetics and profiles, represents the\n",
      "smallest portion. The “STEM” (science, technology, engineering, and mathematics) branch features\n",
      "data related to topics such as physical sciences and\n",
      "astronomy. The “Life Support” branch is highly\n",
      "relevant to everyday life, comprises 11.60% of the\n",
      "data, and primarily focuses on domains such as\n",
      "dwelling and health. An illustration of a data sample from CT-Eval is depicted in Figure 3. Each\n",
      "document-table pair, akin to the example shown,\n",
      "consists of a textual description and a golden table\n",
      "with varying numbers of columns and rows.\n",
      "To assess the quality of our dataset compared\n",
      "to previous ones, we randomly select 200 samples\n",
      "from the training, validation and test sets of CTEval, and previous E2E, Rotowire, WikiBio, and\n",
      "WikiTableText, respectively. Then, we compute the\n",
      "hallucination rate for each dataset through human\n",
      "annotation. Specifically, three annotators proficient\n",
      "in both English and Chinese judged whether the\n",
      "golden tables contained hallucination information\n",
      "following the guidance outlined in Section 3.3. The\n",
      "hallucination rate for each dataset is determined\n",
      "by the average proportion of hallucinated samples\n",
      "judged by all three annotators. We observe that\n",
      "the hallucination rate of the CT-Eval training set\n",
      "is 6.83%, similar to Rotowire (6.17%). Moreover,\n",
      "with the help of our human cleaning, the hallucination rates in our test and validation sets decrease to\n",
      "1.00% and 1.50%, respectively, significantly lower\n",
      "Dataset Language N. Entries Avg. Length N. Domains Hallu. R Avg. Cells\n",
      "E2E English 42,061 90.58 Restaurant 4.17% 4.46\n",
      "Rotowire English 3,398 1311.01 Sports 6.17% 40.49\n",
      "WikiBio English 582,659 416.71 Biography 8.67% 4.19\n",
      "WikiTableText English 10,000 59.76 Multiple Subclasses 18.83% 4.25\n",
      "CT-Eval(train) Chinese 84,603 911.46 28 Subclasses 6.83% 11.40\n",
      "CT-Eval(val.) Chinese 1,000 813.32 28 Subclasses 1.00% 10.78\n",
      "CT-Eval(test) Chinese 1,000 845.22 28 Subclasses 1.50% 10.86\n",
      "Table 1: Data Statistics of CT-Eval and previous text-to-table datasets. “N. Entries” denotes the number of entries.\n",
      "“Hallu. R” indicates hallucination rate. “Avg.Cells” indicates the average number of table cells\n",
      "than those in other datasets. Therefore, the reliability of our dataset can be verified. Regarding\n",
      "domains, the E2E, Rotowire and WikiBio datasets\n",
      "focus on single domains, whereas WikiTableText\n",
      "and CT-Eval encompass multiple domains to ensure data diversity. Concerning the length of the input documents, we note that previous E2E and WikiTableText primarily contain short documents with\n",
      "an average length of under 100 words, whereas documents in our dataset and Rotowire exceed words\n",
      "or characters, presenting more complex inputs for\n",
      "text-to-table models. In summary, CT-Eval stands\n",
      "as the sole dataset fulfilling criteria of data diversity,\n",
      "lengthy documents, and low hallucination.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is CT-Eval?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "CT-Eval is a Chinese text-to-table evaluation dataset created to benchmark the performance of large language models (LLMs) in generating structured tables from unstructured text. It is a comprehensive dataset designed to address the limitations of existing text-to-table datasets,\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "1 Introduction\n",
      "Information extraction (IE) aims to identify and extract structured information from unstructured text.\n",
      "1Codes and data will be publicly available once accepted.\n",
      "Basic IE sub-tasks, such as, named entity recognition and entity linking, generally operate at the\n",
      "sentence level, which ignore models’ ability to understand the document-level meaning. In contrast,\n",
      "Text-to-Table, an emerging sub-task of IE, requires\n",
      "models to understand information within a given\n",
      "document and then generate structured tables. Despite great success in the domain Wu et al. (2022);\n",
      "Li et al. (2023), previous studies are oriented to\n",
      "English, limiting the research in other languages.\n",
      "Recently, large language models (LLMs) have\n",
      "demonstrated powerful performance across various NLP tasks (Zhao et al., 2023; Wang et al.,\n",
      "2023a,b,c). Some studies (González-Gallardo et al.,\n",
      "2023; Gao et al., 2023) have also utilized LLMs for\n",
      "several IE sub-tasks, and found that compared to\n",
      "supervised baselines, the performance of LLMs is\n",
      "sub-optimal. However, their performance of LLMs\n",
      "on Text-to-Table remains largely unexplored. Additionally, the rapid advancements in LLMs have facilitated their widespread adoption in multi-lingual\n",
      "settings, enabling language modeling abilities to be\n",
      "shared across different languages. Consequently, it\n",
      "is theoretically possible to feasible to employ LLMs\n",
      "for text-to-table in other languages, an area that\n",
      "has yet to be thoroughly investigated.\n",
      "Motivated by the aforementioned considerations,\n",
      "we decide to benchmark LLMs on Chinese text-totable. A heuristic approach involves translating existing English datasets into Chinese for subsequent\n",
      "performance evaluation. There are four datasets\n",
      "widely used in text-to-table: E2E (Novikova et al.,\n",
      "2017) is a restaurant domain dataset encompassing\n",
      "51.5K samples about restaurant information like\n",
      "names, addresses, rate scores, etc. Rotowire (Wiseman et al., 2017) is a sports domain dataset derived\n",
      "from NBA basketball games with 4.9K samples\n",
      "about NBA team information. WikiBio (Lebret\n",
      "et al., 2016), compiled from Wikipedia, containing\n",
      "72.6K documents and corresponding structured tables in the biography domain. WikiTableText (Bao\n",
      "arXiv:2405.12174v1 [cs.CL] 20 May 2024\n",
      "et al., 2018) is also derived from Wikipedia, and\n",
      "involves 13.3K multidisciplinary samples spanning\n",
      "fields like finance and politics. However, according to our preliminary analysis, these datasets also\n",
      "suffer from the issues of less diversity or high hallucination, making them unsuitable to benchmark\n",
      "LLMs. Specifically, (1) E2E, Rotowire, and WikiBio exhibit a lack of diversity as they focus on a\n",
      "single domain, violating the core principle of instruction tuning in LLMs, that is, diversity. (2)\n",
      "Although WikiTableText incorporates multiple domains for diversity, our analysis (§ 3.5) indicates\n",
      "that 18.83% of samples exhibit hallucination in\n",
      "the golden tables, that is, containing additional information beyond the provided documents. This\n",
      "arises from treating Wikipedia infoboxes as golden\n",
      "tables, created collaboratively by online users and\n",
      "potentially containing additional basic information.\n",
      "In this paper, we propose the Chinese Text-toTable Evaluation (CT-Eval) dataset, which is constructed through three steps to ensure data diversity\n",
      "and minimize hallucination. To ensure diversity,\n",
      "the first step involves collecting multidisciplinary\n",
      "document-table pairs. We choose the Baidu Baike\n",
      "as the data source, which is a popular Chinese multidisciplinary online encyclopedia. Each page in\n",
      "this source contains text and an infobox summarizing the corresponding key information. Second, to\n",
      "minimize data hallucination, we train an LLM, as a\n",
      "hallucination judger, to filter out task samples with\n",
      "hallucination in their golden tables (infoboxes). Finally, we obtain 88.6K samples with an average\n",
      "length of 911.46 Chinese characters. We split them\n",
      "into 86.6K, 1K and 1K for training, validation and\n",
      "testing. For validation and testing samples, human\n",
      "annotators further clean data hallucination in the\n",
      "golden tables to ensure evaluation reliability.\n",
      "Based on the proposed CT-Eval, we benchmark\n",
      "various mainstream LLMs in both zero-shot (for\n",
      "both open- and closed-source LLMs) and finetuning (only for open-source LLMs) scenarios. Our\n",
      "experiments reveal that (1) GPT-4 achieves the best\n",
      "zero-shot performance among all LLMs. However,\n",
      "its performance remains a discernible disparity\n",
      "compared to human judgment. (2) After fine-tuning\n",
      "on the training set of CT-Eval, all open-source\n",
      "LLMs demonstrate a significant performance improvement, outperforming zero-shot GPT-4 by a\n",
      "large margin, indicating the effectiveness of CTEval. In-depth analyses of LLM-generated tables\n",
      "reveal the persistence of hallucination issues in\n",
      "both zero-shot and fine-tuned LLMs, highlighting\n",
      "a challenge in using LLMs as text-to-table systems.\n",
      "Future work could not only evaluate LLMs’ performance on text-to-table via our CT-Eval benchmark\n",
      "dataset, but also leverage its training data to improve LLMs’ text-to-table ability via fine-tuning.\n",
      "2 Related Work\n",
      "2.1 Text-to-table Tasks\n",
      "Wu et al. (2022) pioneer the text-to-table task.\n",
      "Given the absence of a dedicated text-to-table\n",
      "dataset, Wu et al. (2022) repurpose existing tableto-text datasets, i.e., WikiBio (Lebret et al., 2016),\n",
      "E2E (Novikova et al., 2017), Rotowire (Wiseman et al., 2017) and WikiTableText (Bao et al.,\n",
      "2018), for text-to-table tasks by reversing their\n",
      "input-output pairs. They fine-tune BART (Lewis\n",
      "et al., 2019) to perform text-to-table in a sequenceto-sequence manner, and find that the fine-tuned\n",
      "BART outperforms the pipeline baselines using\n",
      "relation extraction and named entity extraction.\n",
      "STable (Pietruszka et al., 2022) employs two pretrained language models (PLMs) (T5 (Raffel et al.,\n",
      "2020) and TILT (Powalski et al., 2021)) for text-totable, and designs a permutation-based decoder to\n",
      "enhance the PLMs’ table generation ability. Subsequently, Li et al. (2023) find that the predefined row\n",
      "order in golden tables introduces bias into text-totable models. Consequently, they train table header\n",
      "and table body generators separately to produce\n",
      "final tables. While these studies achieve notable\n",
      "success, they primarily explored text-to-table performance before the LLM era. In addition, their\n",
      "evaluation datasets generally focus on a single domain, and adapt from the table-to-text datasets, resulting in hallucination issues. Thus, they are unsuitable for benchmarking LLMs in text-to-table.\n",
      "2.2 Large Language Models\n",
      "The advent of advanced LLMs,e.g., ChatGPT (OpenAI, 2022), GPT-4 (OpenAI, 2023), marks a pivotal\n",
      "moment that propels the field of NLP into a boom\n",
      "phase. Zhong et al. (2023) show that LLMs can\n",
      "achieve decent performance on benchmarks like\n",
      "GLUE (Wang et al., 2018), which spans eight representative NLP understanding tasks. Concurrently,\n",
      "several studies have scrutinized the performance\n",
      "of LLMs across various IE tasks. For example,\n",
      "Gao et al. (2023) test the capability of ChatGPT\n",
      "in event extraction. Similarly, González-Gallardo\n",
      "et al. (2023) employ ChatGPT on historical entity\n",
      "recognition. However, the results of these IE subtasks consistently show that LLMs underperform\n",
      "compared to state-of-the-art supervised approaches.\n",
      "The text-to-table task we focused on is more complex than the previous basic IE sub-tasks, however,\n",
      "it remains an unexplored area for evaluation on\n",
      "LLMs.\n",
      "3 CT-Eval\n",
      "In this section, we first discuss the data source for\n",
      "building CT-Eval (§ 3.1). Then, we give the details\n",
      "of how to control the hallucination in the preliminary collected data, including LLM hallucination\n",
      "judger (§ 3.2) and human cleaning processes (§ 3.3).\n",
      "Finally, we formulate the text-to-table task (§ 3.4)\n",
      "and provide the details of data statistics (§ 3.5).\n",
      "3.1 Data Source\n",
      "Following the success of WikiTableText (Bao et al.,\n",
      "2018), we also choose a multidisciplinary online\n",
      "encyclopedia as the data source to ensure data diversity. After carefully comparing existing Chinese\n",
      "online encyclopedias, we choose Baidu Baike2\n",
      ",\n",
      "which is one of the Chinese encyclopedias with\n",
      "the most entries in the world.\n",
      "We obtain the Baidu Baike data from the dumps\n",
      "provided by Xu et al. (2017). The data contains\n",
      "over 9M entity pages, each of which includes an\n",
      "infobox and the corresponding textual description,\n",
      "forming a text-to-table sample. Utilizing this data,\n",
      "we implement the following rule-based strategy\n",
      "for preliminary data cleaning: (1) Each page must\n",
      "contain at least one infobox; otherwise, the golden\n",
      "table is missing. (2) The number of tabular cells in\n",
      "the infobox should exceed three to ensure validity.\n",
      "(3) The length of the textual description should\n",
      "surpass 200 tokens. After that, 200K documenttable pairs are remaining for further processing.\n",
      "3.2 LLM Data Cleaning\n",
      "After the initial cleaning process, the hallucination issue persists due to Baidu Baike’s submission\n",
      "rules, wherein most of the page text and infoboxes\n",
      "are edited and maintained by individuals. Thus,\n",
      "the contents in the infoboxes may not always align\n",
      "precisely with the textual documents. For instance,\n",
      "some infoboxes may include additional knowledge\n",
      "unrelated to the text, potentially misleading the\n",
      "model from learning the text-to-table task.\n",
      "2\n",
      "https://baike.baidu.com/\n",
      "§ There is a document and a golden table that\n",
      "summarize the information contained in this\n",
      "document. Please help me identify whether the\n",
      "golden table contains additional information\n",
      "beyond the document. Give the reason first and\n",
      "then provide your judgment.\n",
      "Figure 1: Illustration of judgment prompt.\n",
      "To control the hallucination in the task samples,\n",
      "we decide to employ an LLM as a hallucination\n",
      "judger to filter out samples exhibiting hallucination.\n",
      "While using GPT-4 directly as the LLM hallucination judger is a straightforward approach, utilizing\n",
      "official APIs can be costly. Therefore, we first\n",
      "randomly select 5K samples from the data before\n",
      "cleaning. Then, we use GPT-4 to assess whether the\n",
      "golden tables contain additional information. The\n",
      "judgment prompt is illustrated in Figure 1, where\n",
      "the LLM is tasked with evaluating hallucination in\n",
      "a chain-of-thought manner. Next, we use the GPT4 judgment results to train an open-source LLM,\n",
      "employing the trained model to evaluate the remaining samples. Samples containing hallucinations are\n",
      "discarded. Given the capacity for understanding\n",
      "lengthy documents of existing Chinese LLMs (Bai\n",
      "et al., 2023b), we select ChatGLM3-6B-32k3\n",
      "as\n",
      "the open-source LLM hallucination judger. Finally,\n",
      "there are 88.6K samples after the data cleaning\n",
      "by the LLM hallucination judger. These samples\n",
      "totally cover 28 domains, e.g., physics and religion.\n",
      "3.3 Human Data Cleaning\n",
      "We split the LLM-cleaned samples into the training, validation, and test sets with 84.6K, 1K and\n",
      "1K samples, respectively. In the validation and test\n",
      "sets, we balance the number of samples in each\n",
      "domain to ensure a comprehensive evaluation. Furthermore, to mitigate the hallucination issue in the\n",
      "validation and test sets, we employ human annotators to manually cleanse their golden tables.\n",
      "In this phase, we employ five human annotators\n",
      "and one data expert, all of whom are native Chinese\n",
      "speakers with advanced educational qualifications.\n",
      "The data expert is a researcher with extensive experience in IE research. We first organize a tutorial\n",
      "for the five annotators to ensure alignment of annotation requirements. This includes clarifying the\n",
      "concept of the hallucination issue, emphasizing the\n",
      "additional information present in tables that cannot\n",
      "be directly extracted from the corresponding documents. Next, for each document-table pair in the\n",
      "3\n",
      "https://huggingface.co/THUDM/chatglm3-6b-32k\n",
      "CT-Eval\n",
      "Industry\n",
      "Informatics\n",
      "Physics\n",
      "Astronomy\n",
      "Traffic Engineering\n",
      "Biology\n",
      "Medical Science\n",
      "Culture\n",
      "Religion\n",
      "History\n",
      "Geography\n",
      "Education\n",
      "Business\n",
      "Finance\n",
      "Management\n",
      "Movie and Book\n",
      "Science Fiction\n",
      "Music\n",
      "Animation\n",
      "Dwelling\n",
      "Traveling\n",
      "Career\n",
      "Healthy\n",
      "Food\n",
      "Cosmetics\n",
      "Profile\n",
      "Security\n",
      "Military\n",
      "STEM\n",
      "24.0%\n",
      "Social Science\n",
      "60.9%\n",
      "Life\n",
      "Support\n",
      "11.6%\n",
      "O.\n",
      "3.5\n",
      "%\n",
      "Figure 2: Domain distribution in CT-EVAL\n",
      "validation and test sets, three annotators are asked\n",
      "to remove the hallucination information from tables. The data expert reviews 10% of the manually\n",
      "cleaned samples from each annotator. If the accuracy rate falls below 95%, the respective annotator\n",
      "will be required to redo the annotation. Finally,\n",
      "for each sample, if the three manually cleaned results are consistent, the results are saved as the final\n",
      "data. Otherwise, the results are decided by a group\n",
      "meeting among all annotators and the data expert.\n",
      "3.4 Task Overview\n",
      "Given a document D = {w1, w2,..., w|D|}, where\n",
      "wi\n",
      "is the i-th word in D, the text-to-table task aims\n",
      "to extract key information from D and outputs a table T = {c0,0, c0,1,..., c0,n, c1,0, c1,1,..., c1,n,...,\n",
      "ci,j,..., cm,n}, where c0,k(k ∈ {0, 1,..., n}) represents the column header and ck,0(k ∈ {0, 1,..., n})\n",
      "denotes as the row header. ci,j (i > 0, j > 0) represents the text of the cell in the i-th row and j-th\n",
      "column in the table with m rows and n columns.\n",
      "3.5 Data Statistics\n",
      "We compare CT-Eval with the previous datasets\n",
      "across several metrics including language, number of entries, average length, number of domains,\n",
      "hallucination rate, and average number of cells.\n",
      "As shown in Table 1, compared to previous\n",
      "datasets that mostly focus on one domain, CT-Eval\n",
      "covers 28 domains, offering a valuable resource\n",
      "to evaluate the text-to-table capabilities of LLMs\n",
      "across multiple domains. These 28 domains can\n",
      "be categorized into four branches: STEM, social\n",
      "science, life support and others. To provide a comprehensive understanding of domain coverage in\n",
      "CT-Eval, we present the distributions of each domain in Figure 2. Notably, the “Social Science”\n",
      "Original Item\n",
      "Translation\n",
      "材料 做法 厨师一点通\n",
      "炖牛肉时，可以放几个\n",
      "山楂进去，这样牛肉会\n",
      "烂得快些，而且有股山\n",
      "楂的清香。\n",
      "1.牛肉洗净放入清水锅\n",
      "中煮至七成熟，捞出切\n",
      "成方块\n",
      "2. …\n",
      "材料：\n",
      "牛肉，胡萝卜，\n",
      "白萝卜，大料，\n",
      "香叶\n",
      "东坡牛肉\n",
      "Original Item\n",
      "Ingredients Cooking method Cooking Tips\n",
      "When stewing beef, you\n",
      "can put a few hawthorn\n",
      "into it, so that the beef\n",
      "will rot faster and have a\n",
      "hawthorn fragrance.\n",
      "1. Wash the beef and\n",
      "put it into a pot of\n",
      "water to boil until\n",
      "it is seven mature,\n",
      "fish out and cut it\n",
      "into squares…\n",
      "2. …\n",
      "Ingredients:\n",
      "Beef, carrots, white\n",
      "radish, Chinese\n",
      "spices, allspice\n",
      "Dongpo\n",
      "Beef\n",
      "“标题”: “东坡牛肉”, “ ”章节“: [{”标题“: ”东坡牛肉的做法“},\n",
      "内容“: ”葱段、姜片、蒜头、辣椒、香叶、大料、盐、白糖、番茄酱、排骨\n",
      "酱、辣椒酱、高汤（用煮牛肉的汤即可）。\n",
      "做法：\n",
      "1、先来处理一下牛肉，\n",
      "将牛腩在冷水中多泡一会…“}\n",
      "Translation\n",
      "“Title”: “Dongpo Beef”, “ ”Chapter“: [{”Title “: ”Dongpo Beef Practice“}, :\n",
      "Ingredients：Diced green onion, ginger, garlic, chili pepper, coriander, dashi, salt,\n",
      "sugar, tomato sauce, rib sauce, chili sauce, broth. Cooking method:1, first, let's deal\n",
      "with the beef, soak the brisket in cold water for a while longer...\"}\n",
      "Figure 3: Text-to-table example from CT-EVAL.\n",
      "branch exhibits the highest proportion, encompassing domains such as culture and religion. Conversely, the “Other” branch, which includes topics\n",
      "related to cosmetics and profiles, represents the\n",
      "smallest portion. The “STEM” (science, technology, engineering, and mathematics) branch features\n",
      "data related to topics such as physical sciences and\n",
      "astronomy. The “Life Support” branch is highly\n",
      "relevant to everyday life, comprises 11.60% of the\n",
      "data, and primarily focuses on domains such as\n",
      "dwelling and health. An illustration of a data sample from CT-Eval is depicted in Figure 3. Each\n",
      "document-table pair, akin to the example shown,\n",
      "consists of a textual description and a golden table\n",
      "with varying numbers of columns and rows.\n",
      "To assess the quality of our dataset compared\n",
      "to previous ones, we randomly select 200 samples\n",
      "from the training, validation and test sets of CTEval, and previous E2E, Rotowire, WikiBio, and\n",
      "WikiTableText, respectively. Then, we compute the\n",
      "hallucination rate for each dataset through human\n",
      "annotation. Specifically, three annotators proficient\n",
      "in both English and Chinese judged whether the\n",
      "golden tables contained hallucination information\n",
      "following the guidance outlined in Section 3.3. The\n",
      "hallucination rate for each dataset is determined\n",
      "by the average proportion of hallucinated samples\n",
      "judged by all three annotators. We observe that\n",
      "the hallucination rate of the CT-Eval training set\n",
      "is 6.83%, similar to Rotowire (6.17%). Moreover,\n",
      "with the help of our human cleaning, the hallucination rates in our test and validation sets decrease to\n",
      "1.00% and 1.50%, respectively, significantly lower\n",
      "Dataset Language N. Entries Avg. Length N. Domains Hallu. R Avg. Cells\n",
      "E2E English 42,061 90.58 Restaurant 4.17% 4.46\n",
      "Rotowire English 3,398 1311.01 Sports 6.17% 40.49\n",
      "WikiBio English 582,659 416.71 Biography 8.67% 4.19\n",
      "WikiTableText English 10,000 59.76 Multiple Subclasses 18.83% 4.25\n",
      "CT-Eval(train) Chinese 84,603 911.46 28 Subclasses 6.83% 11.40\n",
      "CT-Eval(val.) Chinese 1,000 813.32 28 Subclasses 1.00% 10.78\n",
      "CT-Eval(test) Chinese 1,000 845.22 28 Subclasses 1.50% 10.86\n",
      "Table 1: Data Statistics of CT-Eval and previous text-to-table datasets. “N. Entries” denotes the number of entries.\n",
      "“Hallu. R” indicates hallucination rate. “Avg.Cells” indicates the average number of table cells\n",
      "than those in other datasets. Therefore, the reliability of our dataset can be verified. Regarding\n",
      "domains, the E2E, Rotowire and WikiBio datasets\n",
      "focus on single domains, whereas WikiTableText\n",
      "and CT-Eval encompass multiple domains to ensure data diversity. Concerning the length of the input documents, we note that previous E2E and WikiTableText primarily contain short documents with\n",
      "an average length of under 100 words, whereas documents in our dataset and Rotowire exceed words\n",
      "or characters, presenting more complex inputs for\n",
      "text-to-table models. In summary, CT-Eval stands\n",
      "as the sole dataset fulfilling criteria of data diversity,\n",
      "lengthy documents, and low hallucination.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is CT-Eval?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "CT-Eval is a Chinese text-to-table evaluation dataset created to benchmark the performance of Large Language Models (LLMs) in generating structured tables from unstructured text. It's a new dataset that aims to fill the gap in the existing text-to-table\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "1 Introduction\n",
      "Information extraction (IE) aims to identify and extract structured information from unstructured text.\n",
      "1Codes and data will be publicly available once accepted.\n",
      "Basic IE sub-tasks, such as, named entity recognition and entity linking, generally operate at the\n",
      "sentence level, which ignore models’ ability to understand the document-level meaning. In contrast,\n",
      "Text-to-Table, an emerging sub-task of IE, requires\n",
      "models to understand information within a given\n",
      "document and then generate structured tables. Despite great success in the domain Wu et al. (2022);\n",
      "Li et al. (2023), previous studies are oriented to\n",
      "English, limiting the research in other languages.\n",
      "Recently, large language models (LLMs) have\n",
      "demonstrated powerful performance across various NLP tasks (Zhao et al., 2023; Wang et al.,\n",
      "2023a,b,c). Some studies (González-Gallardo et al.,\n",
      "2023; Gao et al., 2023) have also utilized LLMs for\n",
      "several IE sub-tasks, and found that compared to\n",
      "supervised baselines, the performance of LLMs is\n",
      "sub-optimal. However, their performance of LLMs\n",
      "on Text-to-Table remains largely unexplored. Additionally, the rapid advancements in LLMs have facilitated their widespread adoption in multi-lingual\n",
      "settings, enabling language modeling abilities to be\n",
      "shared across different languages. Consequently, it\n",
      "is theoretically possible to feasible to employ LLMs\n",
      "for text-to-table in other languages, an area that\n",
      "has yet to be thoroughly investigated.\n",
      "Motivated by the aforementioned considerations,\n",
      "we decide to benchmark LLMs on Chinese text-totable. A heuristic approach involves translating existing English datasets into Chinese for subsequent\n",
      "performance evaluation. There are four datasets\n",
      "widely used in text-to-table: E2E (Novikova et al.,\n",
      "2017) is a restaurant domain dataset encompassing\n",
      "51.5K samples about restaurant information like\n",
      "names, addresses, rate scores, etc. Rotowire (Wiseman et al., 2017) is a sports domain dataset derived\n",
      "from NBA basketball games with 4.9K samples\n",
      "about NBA team information. WikiBio (Lebret\n",
      "et al., 2016), compiled from Wikipedia, containing\n",
      "72.6K documents and corresponding structured tables in the biography domain. WikiTableText (Bao\n",
      "arXiv:2405.12174v1 [cs.CL] 20 May 2024\n",
      "et al., 2018) is also derived from Wikipedia, and\n",
      "involves 13.3K multidisciplinary samples spanning\n",
      "fields like finance and politics. However, according to our preliminary analysis, these datasets also\n",
      "suffer from the issues of less diversity or high hallucination, making them unsuitable to benchmark\n",
      "LLMs. Specifically, (1) E2E, Rotowire, and WikiBio exhibit a lack of diversity as they focus on a\n",
      "single domain, violating the core principle of instruction tuning in LLMs, that is, diversity. (2)\n",
      "Although WikiTableText incorporates multiple domains for diversity, our analysis (§ 3.5) indicates\n",
      "that 18.83% of samples exhibit hallucination in\n",
      "the golden tables, that is, containing additional information beyond the provided documents. This\n",
      "arises from treating Wikipedia infoboxes as golden\n",
      "tables, created collaboratively by online users and\n",
      "potentially containing additional basic information.\n",
      "In this paper, we propose the Chinese Text-toTable Evaluation (CT-Eval) dataset, which is constructed through three steps to ensure data diversity\n",
      "and minimize hallucination. To ensure diversity,\n",
      "the first step involves collecting multidisciplinary\n",
      "document-table pairs. We choose the Baidu Baike\n",
      "as the data source, which is a popular Chinese multidisciplinary online encyclopedia. Each page in\n",
      "this source contains text and an infobox summarizing the corresponding key information. Second, to\n",
      "minimize data hallucination, we train an LLM, as a\n",
      "hallucination judger, to filter out task samples with\n",
      "hallucination in their golden tables (infoboxes). Finally, we obtain 88.6K samples with an average\n",
      "length of 911.46 Chinese characters. We split them\n",
      "into 86.6K, 1K and 1K for training, validation and\n",
      "testing. For validation and testing samples, human\n",
      "annotators further clean data hallucination in the\n",
      "golden tables to ensure evaluation reliability.\n",
      "Based on the proposed CT-Eval, we benchmark\n",
      "various mainstream LLMs in both zero-shot (for\n",
      "both open- and closed-source LLMs) and finetuning (only for open-source LLMs) scenarios. Our\n",
      "experiments reveal that (1) GPT-4 achieves the best\n",
      "zero-shot performance among all LLMs. However,\n",
      "its performance remains a discernible disparity\n",
      "compared to human judgment. (2) After fine-tuning\n",
      "on the training set of CT-Eval, all open-source\n",
      "LLMs demonstrate a significant performance improvement, outperforming zero-shot GPT-4 by a\n",
      "large margin, indicating the effectiveness of CTEval. In-depth analyses of LLM-generated tables\n",
      "reveal the persistence of hallucination issues in\n",
      "both zero-shot and fine-tuned LLMs, highlighting\n",
      "a challenge in using LLMs as text-to-table systems.\n",
      "Future work could not only evaluate LLMs’ performance on text-to-table via our CT-Eval benchmark\n",
      "dataset, but also leverage its training data to improve LLMs’ text-to-table ability via fine-tuning.\n",
      "2 Related Work\n",
      "2.1 Text-to-table Tasks\n",
      "Wu et al. (2022) pioneer the text-to-table task.\n",
      "Given the absence of a dedicated text-to-table\n",
      "dataset, Wu et al. (2022) repurpose existing tableto-text datasets, i.e., WikiBio (Lebret et al., 2016),\n",
      "E2E (Novikova et al., 2017), Rotowire (Wiseman et al., 2017) and WikiTableText (Bao et al.,\n",
      "2018), for text-to-table tasks by reversing their\n",
      "input-output pairs. They fine-tune BART (Lewis\n",
      "et al., 2019) to perform text-to-table in a sequenceto-sequence manner, and find that the fine-tuned\n",
      "BART outperforms the pipeline baselines using\n",
      "relation extraction and named entity extraction.\n",
      "STable (Pietruszka et al., 2022) employs two pretrained language models (PLMs) (T5 (Raffel et al.,\n",
      "2020) and TILT (Powalski et al., 2021)) for text-totable, and designs a permutation-based decoder to\n",
      "enhance the PLMs’ table generation ability. Subsequently, Li et al. (2023) find that the predefined row\n",
      "order in golden tables introduces bias into text-totable models. Consequently, they train table header\n",
      "and table body generators separately to produce\n",
      "final tables. While these studies achieve notable\n",
      "success, they primarily explored text-to-table performance before the LLM era. In addition, their\n",
      "evaluation datasets generally focus on a single domain, and adapt from the table-to-text datasets, resulting in hallucination issues. Thus, they are unsuitable for benchmarking LLMs in text-to-table.\n",
      "2.2 Large Language Models\n",
      "The advent of advanced LLMs,e.g., ChatGPT (OpenAI, 2022), GPT-4 (OpenAI, 2023), marks a pivotal\n",
      "moment that propels the field of NLP into a boom\n",
      "phase. Zhong et al. (2023) show that LLMs can\n",
      "achieve decent performance on benchmarks like\n",
      "GLUE (Wang et al., 2018), which spans eight representative NLP understanding tasks. Concurrently,\n",
      "several studies have scrutinized the performance\n",
      "of LLMs across various IE tasks. For example,\n",
      "Gao et al. (2023) test the capability of ChatGPT\n",
      "in event extraction. Similarly, González-Gallardo\n",
      "et al. (2023) employ ChatGPT on historical entity\n",
      "recognition. However, the results of these IE subtasks consistently show that LLMs underperform\n",
      "compared to state-of-the-art supervised approaches.\n",
      "The text-to-table task we focused on is more complex than the previous basic IE sub-tasks, however,\n",
      "it remains an unexplored area for evaluation on\n",
      "LLMs.\n",
      "3 CT-Eval\n",
      "In this section, we first discuss the data source for\n",
      "building CT-Eval (§ 3.1). Then, we give the details\n",
      "of how to control the hallucination in the preliminary collected data, including LLM hallucination\n",
      "judger (§ 3.2) and human cleaning processes (§ 3.3).\n",
      "Finally, we formulate the text-to-table task (§ 3.4)\n",
      "and provide the details of data statistics (§ 3.5).\n",
      "3.1 Data Source\n",
      "Following the success of WikiTableText (Bao et al.,\n",
      "2018), we also choose a multidisciplinary online\n",
      "encyclopedia as the data source to ensure data diversity. After carefully comparing existing Chinese\n",
      "online encyclopedias, we choose Baidu Baike2\n",
      ",\n",
      "which is one of the Chinese encyclopedias with\n",
      "the most entries in the world.\n",
      "We obtain the Baidu Baike data from the dumps\n",
      "provided by Xu et al. (2017). The data contains\n",
      "over 9M entity pages, each of which includes an\n",
      "infobox and the corresponding textual description,\n",
      "forming a text-to-table sample. Utilizing this data,\n",
      "we implement the following rule-based strategy\n",
      "for preliminary data cleaning: (1) Each page must\n",
      "contain at least one infobox; otherwise, the golden\n",
      "table is missing. (2) The number of tabular cells in\n",
      "the infobox should exceed three to ensure validity.\n",
      "(3) The length of the textual description should\n",
      "surpass 200 tokens. After that, 200K documenttable pairs are remaining for further processing.\n",
      "3.2 LLM Data Cleaning\n",
      "After the initial cleaning process, the hallucination issue persists due to Baidu Baike’s submission\n",
      "rules, wherein most of the page text and infoboxes\n",
      "are edited and maintained by individuals. Thus,\n",
      "the contents in the infoboxes may not always align\n",
      "precisely with the textual documents. For instance,\n",
      "some infoboxes may include additional knowledge\n",
      "unrelated to the text, potentially misleading the\n",
      "model from learning the text-to-table task.\n",
      "2\n",
      "https://baike.baidu.com/\n",
      "§ There is a document and a golden table that\n",
      "summarize the information contained in this\n",
      "document. Please help me identify whether the\n",
      "golden table contains additional information\n",
      "beyond the document. Give the reason first and\n",
      "then provide your judgment.\n",
      "Figure 1: Illustration of judgment prompt.\n",
      "To control the hallucination in the task samples,\n",
      "we decide to employ an LLM as a hallucination\n",
      "judger to filter out samples exhibiting hallucination.\n",
      "While using GPT-4 directly as the LLM hallucination judger is a straightforward approach, utilizing\n",
      "official APIs can be costly. Therefore, we first\n",
      "randomly select 5K samples from the data before\n",
      "cleaning. Then, we use GPT-4 to assess whether the\n",
      "golden tables contain additional information. The\n",
      "judgment prompt is illustrated in Figure 1, where\n",
      "the LLM is tasked with evaluating hallucination in\n",
      "a chain-of-thought manner. Next, we use the GPT4 judgment results to train an open-source LLM,\n",
      "employing the trained model to evaluate the remaining samples. Samples containing hallucinations are\n",
      "discarded. Given the capacity for understanding\n",
      "lengthy documents of existing Chinese LLMs (Bai\n",
      "et al., 2023b), we select ChatGLM3-6B-32k3\n",
      "as\n",
      "the open-source LLM hallucination judger. Finally,\n",
      "there are 88.6K samples after the data cleaning\n",
      "by the LLM hallucination judger. These samples\n",
      "totally cover 28 domains, e.g., physics and religion.\n",
      "3.3 Human Data Cleaning\n",
      "We split the LLM-cleaned samples into the training, validation, and test sets with 84.6K, 1K and\n",
      "1K samples, respectively. In the validation and test\n",
      "sets, we balance the number of samples in each\n",
      "domain to ensure a comprehensive evaluation. Furthermore, to mitigate the hallucination issue in the\n",
      "validation and test sets, we employ human annotators to manually cleanse their golden tables.\n",
      "In this phase, we employ five human annotators\n",
      "and one data expert, all of whom are native Chinese\n",
      "speakers with advanced educational qualifications.\n",
      "The data expert is a researcher with extensive experience in IE research. We first organize a tutorial\n",
      "for the five annotators to ensure alignment of annotation requirements. This includes clarifying the\n",
      "concept of the hallucination issue, emphasizing the\n",
      "additional information present in tables that cannot\n",
      "be directly extracted from the corresponding documents. Next, for each document-table pair in the\n",
      "3\n",
      "https://huggingface.co/THUDM/chatglm3-6b-32k\n",
      "CT-Eval\n",
      "Industry\n",
      "Informatics\n",
      "Physics\n",
      "Astronomy\n",
      "Traffic Engineering\n",
      "Biology\n",
      "Medical Science\n",
      "Culture\n",
      "Religion\n",
      "History\n",
      "Geography\n",
      "Education\n",
      "Business\n",
      "Finance\n",
      "Management\n",
      "Movie and Book\n",
      "Science Fiction\n",
      "Music\n",
      "Animation\n",
      "Dwelling\n",
      "Traveling\n",
      "Career\n",
      "Healthy\n",
      "Food\n",
      "Cosmetics\n",
      "Profile\n",
      "Security\n",
      "Military\n",
      "STEM\n",
      "24.0%\n",
      "Social Science\n",
      "60.9%\n",
      "Life\n",
      "Support\n",
      "11.6%\n",
      "O.\n",
      "3.5\n",
      "%\n",
      "Figure 2: Domain distribution in CT-EVAL\n",
      "validation and test sets, three annotators are asked\n",
      "to remove the hallucination information from tables. The data expert reviews 10% of the manually\n",
      "cleaned samples from each annotator. If the accuracy rate falls below 95%, the respective annotator\n",
      "will be required to redo the annotation. Finally,\n",
      "for each sample, if the three manually cleaned results are consistent, the results are saved as the final\n",
      "data. Otherwise, the results are decided by a group\n",
      "meeting among all annotators and the data expert.\n",
      "3.4 Task Overview\n",
      "Given a document D = {w1, w2,..., w|D|}, where\n",
      "wi\n",
      "is the i-th word in D, the text-to-table task aims\n",
      "to extract key information from D and outputs a table T = {c0,0, c0,1,..., c0,n, c1,0, c1,1,..., c1,n,...,\n",
      "ci,j,..., cm,n}, where c0,k(k ∈ {0, 1,..., n}) represents the column header and ck,0(k ∈ {0, 1,..., n})\n",
      "denotes as the row header. ci,j (i > 0, j > 0) represents the text of the cell in the i-th row and j-th\n",
      "column in the table with m rows and n columns.\n",
      "3.5 Data Statistics\n",
      "We compare CT-Eval with the previous datasets\n",
      "across several metrics including language, number of entries, average length, number of domains,\n",
      "hallucination rate, and average number of cells.\n",
      "As shown in Table 1, compared to previous\n",
      "datasets that mostly focus on one domain, CT-Eval\n",
      "covers 28 domains, offering a valuable resource\n",
      "to evaluate the text-to-table capabilities of LLMs\n",
      "across multiple domains. These 28 domains can\n",
      "be categorized into four branches: STEM, social\n",
      "science, life support and others. To provide a comprehensive understanding of domain coverage in\n",
      "CT-Eval, we present the distributions of each domain in Figure 2. Notably, the “Social Science”\n",
      "Original Item\n",
      "Translation\n",
      "材料 做法 厨师一点通\n",
      "炖牛肉时，可以放几个\n",
      "山楂进去，这样牛肉会\n",
      "烂得快些，而且有股山\n",
      "楂的清香。\n",
      "1.牛肉洗净放入清水锅\n",
      "中煮至七成熟，捞出切\n",
      "成方块\n",
      "2. …\n",
      "材料：\n",
      "牛肉，胡萝卜，\n",
      "白萝卜，大料，\n",
      "香叶\n",
      "东坡牛肉\n",
      "Original Item\n",
      "Ingredients Cooking method Cooking Tips\n",
      "When stewing beef, you\n",
      "can put a few hawthorn\n",
      "into it, so that the beef\n",
      "will rot faster and have a\n",
      "hawthorn fragrance.\n",
      "1. Wash the beef and\n",
      "put it into a pot of\n",
      "water to boil until\n",
      "it is seven mature,\n",
      "fish out and cut it\n",
      "into squares…\n",
      "2. …\n",
      "Ingredients:\n",
      "Beef, carrots, white\n",
      "radish, Chinese\n",
      "spices, allspice\n",
      "Dongpo\n",
      "Beef\n",
      "“标题”: “东坡牛肉”, “ ”章节“: [{”标题“: ”东坡牛肉的做法“},\n",
      "内容“: ”葱段、姜片、蒜头、辣椒、香叶、大料、盐、白糖、番茄酱、排骨\n",
      "酱、辣椒酱、高汤（用煮牛肉的汤即可）。\n",
      "做法：\n",
      "1、先来处理一下牛肉，\n",
      "将牛腩在冷水中多泡一会…“}\n",
      "Translation\n",
      "“Title”: “Dongpo Beef”, “ ”Chapter“: [{”Title “: ”Dongpo Beef Practice“}, :\n",
      "Ingredients：Diced green onion, ginger, garlic, chili pepper, coriander, dashi, salt,\n",
      "sugar, tomato sauce, rib sauce, chili sauce, broth. Cooking method:1, first, let's deal\n",
      "with the beef, soak the brisket in cold water for a while longer...\"}\n",
      "Figure 3: Text-to-table example from CT-EVAL.\n",
      "branch exhibits the highest proportion, encompassing domains such as culture and religion. Conversely, the “Other” branch, which includes topics\n",
      "related to cosmetics and profiles, represents the\n",
      "smallest portion. The “STEM” (science, technology, engineering, and mathematics) branch features\n",
      "data related to topics such as physical sciences and\n",
      "astronomy. The “Life Support” branch is highly\n",
      "relevant to everyday life, comprises 11.60% of the\n",
      "data, and primarily focuses on domains such as\n",
      "dwelling and health. An illustration of a data sample from CT-Eval is depicted in Figure 3. Each\n",
      "document-table pair, akin to the example shown,\n",
      "consists of a textual description and a golden table\n",
      "with varying numbers of columns and rows.\n",
      "To assess the quality of our dataset compared\n",
      "to previous ones, we randomly select 200 samples\n",
      "from the training, validation and test sets of CTEval, and previous E2E, Rotowire, WikiBio, and\n",
      "WikiTableText, respectively. Then, we compute the\n",
      "hallucination rate for each dataset through human\n",
      "annotation. Specifically, three annotators proficient\n",
      "in both English and Chinese judged whether the\n",
      "golden tables contained hallucination information\n",
      "following the guidance outlined in Section 3.3. The\n",
      "hallucination rate for each dataset is determined\n",
      "by the average proportion of hallucinated samples\n",
      "judged by all three annotators. We observe that\n",
      "the hallucination rate of the CT-Eval training set\n",
      "is 6.83%, similar to Rotowire (6.17%). Moreover,\n",
      "with the help of our human cleaning, the hallucination rates in our test and validation sets decrease to\n",
      "1.00% and 1.50%, respectively, significantly lower\n",
      "Dataset Language N. Entries Avg. Length N. Domains Hallu. R Avg. Cells\n",
      "E2E English 42,061 90.58 Restaurant 4.17% 4.46\n",
      "Rotowire English 3,398 1311.01 Sports 6.17% 40.49\n",
      "WikiBio English 582,659 416.71 Biography 8.67% 4.19\n",
      "WikiTableText English 10,000 59.76 Multiple Subclasses 18.83% 4.25\n",
      "CT-Eval(train) Chinese 84,603 911.46 28 Subclasses 6.83% 11.40\n",
      "CT-Eval(val.) Chinese 1,000 813.32 28 Subclasses 1.00% 10.78\n",
      "CT-Eval(test) Chinese 1,000 845.22 28 Subclasses 1.50% 10.86\n",
      "Table 1: Data Statistics of CT-Eval and previous text-to-table datasets. “N. Entries” denotes the number of entries.\n",
      "“Hallu. R” indicates hallucination rate. “Avg.Cells” indicates the average number of table cells\n",
      "than those in other datasets. Therefore, the reliability of our dataset can be verified. Regarding\n",
      "domains, the E2E, Rotowire and WikiBio datasets\n",
      "focus on single domains, whereas WikiTableText\n",
      "and CT-Eval encompass multiple domains to ensure data diversity. Concerning the length of the input documents, we note that previous E2E and WikiTableText primarily contain short documents with\n",
      "an average length of under 100 words, whereas documents in our dataset and Rotowire exceed words\n",
      "or characters, presenting more complex inputs for\n",
      "text-to-table models. In summary, CT-Eval stands\n",
      "as the sole dataset fulfilling criteria of data diversity,\n",
      "lengthy documents, and low hallucination.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is CT-Eval?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "CT-Eval is a text-to-table evaluation dataset specifically designed for Chinese text-to-table tasks. It is a newly proposed dataset constructed by the authors to benchmark the performance of large language models (LLMs) in text-to-table tasks in the Chinese language\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "1 Introduction\n",
      "Information extraction (IE) aims to identify and extract structured information from unstructured text.\n",
      "1Codes and data will be publicly available once accepted.\n",
      "Basic IE sub-tasks, such as, named entity recognition and entity linking, generally operate at the\n",
      "sentence level, which ignore models’ ability to understand the document-level meaning. In contrast,\n",
      "Text-to-Table, an emerging sub-task of IE, requires\n",
      "models to understand information within a given\n",
      "document and then generate structured tables. Despite great success in the domain Wu et al. (2022);\n",
      "Li et al. (2023), previous studies are oriented to\n",
      "English, limiting the research in other languages.\n",
      "Recently, large language models (LLMs) have\n",
      "demonstrated powerful performance across various NLP tasks (Zhao et al., 2023; Wang et al.,\n",
      "2023a,b,c). Some studies (González-Gallardo et al.,\n",
      "2023; Gao et al., 2023) have also utilized LLMs for\n",
      "several IE sub-tasks, and found that compared to\n",
      "supervised baselines, the performance of LLMs is\n",
      "sub-optimal. However, their performance of LLMs\n",
      "on Text-to-Table remains largely unexplored. Additionally, the rapid advancements in LLMs have facilitated their widespread adoption in multi-lingual\n",
      "settings, enabling language modeling abilities to be\n",
      "shared across different languages. Consequently, it\n",
      "is theoretically possible to feasible to employ LLMs\n",
      "for text-to-table in other languages, an area that\n",
      "has yet to be thoroughly investigated.\n",
      "Motivated by the aforementioned considerations,\n",
      "we decide to benchmark LLMs on Chinese text-totable. A heuristic approach involves translating existing English datasets into Chinese for subsequent\n",
      "performance evaluation. There are four datasets\n",
      "widely used in text-to-table: E2E (Novikova et al.,\n",
      "2017) is a restaurant domain dataset encompassing\n",
      "51.5K samples about restaurant information like\n",
      "names, addresses, rate scores, etc. Rotowire (Wiseman et al., 2017) is a sports domain dataset derived\n",
      "from NBA basketball games with 4.9K samples\n",
      "about NBA team information. WikiBio (Lebret\n",
      "et al., 2016), compiled from Wikipedia, containing\n",
      "72.6K documents and corresponding structured tables in the biography domain. WikiTableText (Bao\n",
      "arXiv:2405.12174v1 [cs.CL] 20 May 2024\n",
      "et al., 2018) is also derived from Wikipedia, and\n",
      "involves 13.3K multidisciplinary samples spanning\n",
      "fields like finance and politics. However, according to our preliminary analysis, these datasets also\n",
      "suffer from the issues of less diversity or high hallucination, making them unsuitable to benchmark\n",
      "LLMs. Specifically, (1) E2E, Rotowire, and WikiBio exhibit a lack of diversity as they focus on a\n",
      "single domain, violating the core principle of instruction tuning in LLMs, that is, diversity. (2)\n",
      "Although WikiTableText incorporates multiple domains for diversity, our analysis (§ 3.5) indicates\n",
      "that 18.83% of samples exhibit hallucination in\n",
      "the golden tables, that is, containing additional information beyond the provided documents. This\n",
      "arises from treating Wikipedia infoboxes as golden\n",
      "tables, created collaboratively by online users and\n",
      "potentially containing additional basic information.\n",
      "In this paper, we propose the Chinese Text-toTable Evaluation (CT-Eval) dataset, which is constructed through three steps to ensure data diversity\n",
      "and minimize hallucination. To ensure diversity,\n",
      "the first step involves collecting multidisciplinary\n",
      "document-table pairs. We choose the Baidu Baike\n",
      "as the data source, which is a popular Chinese multidisciplinary online encyclopedia. Each page in\n",
      "this source contains text and an infobox summarizing the corresponding key information. Second, to\n",
      "minimize data hallucination, we train an LLM, as a\n",
      "hallucination judger, to filter out task samples with\n",
      "hallucination in their golden tables (infoboxes). Finally, we obtain 88.6K samples with an average\n",
      "length of 911.46 Chinese characters. We split them\n",
      "into 86.6K, 1K and 1K for training, validation and\n",
      "testing. For validation and testing samples, human\n",
      "annotators further clean data hallucination in the\n",
      "golden tables to ensure evaluation reliability.\n",
      "Based on the proposed CT-Eval, we benchmark\n",
      "various mainstream LLMs in both zero-shot (for\n",
      "both open- and closed-source LLMs) and finetuning (only for open-source LLMs) scenarios. Our\n",
      "experiments reveal that (1) GPT-4 achieves the best\n",
      "zero-shot performance among all LLMs. However,\n",
      "its performance remains a discernible disparity\n",
      "compared to human judgment. (2) After fine-tuning\n",
      "on the training set of CT-Eval, all open-source\n",
      "LLMs demonstrate a significant performance improvement, outperforming zero-shot GPT-4 by a\n",
      "large margin, indicating the effectiveness of CTEval. In-depth analyses of LLM-generated tables\n",
      "reveal the persistence of hallucination issues in\n",
      "both zero-shot and fine-tuned LLMs, highlighting\n",
      "a challenge in using LLMs as text-to-table systems.\n",
      "Future work could not only evaluate LLMs’ performance on text-to-table via our CT-Eval benchmark\n",
      "dataset, but also leverage its training data to improve LLMs’ text-to-table ability via fine-tuning.\n",
      "2 Related Work\n",
      "2.1 Text-to-table Tasks\n",
      "Wu et al. (2022) pioneer the text-to-table task.\n",
      "Given the absence of a dedicated text-to-table\n",
      "dataset, Wu et al. (2022) repurpose existing tableto-text datasets, i.e., WikiBio (Lebret et al., 2016),\n",
      "E2E (Novikova et al., 2017), Rotowire (Wiseman et al., 2017) and WikiTableText (Bao et al.,\n",
      "2018), for text-to-table tasks by reversing their\n",
      "input-output pairs. They fine-tune BART (Lewis\n",
      "et al., 2019) to perform text-to-table in a sequenceto-sequence manner, and find that the fine-tuned\n",
      "BART outperforms the pipeline baselines using\n",
      "relation extraction and named entity extraction.\n",
      "STable (Pietruszka et al., 2022) employs two pretrained language models (PLMs) (T5 (Raffel et al.,\n",
      "2020) and TILT (Powalski et al., 2021)) for text-totable, and designs a permutation-based decoder to\n",
      "enhance the PLMs’ table generation ability. Subsequently, Li et al. (2023) find that the predefined row\n",
      "order in golden tables introduces bias into text-totable models. Consequently, they train table header\n",
      "and table body generators separately to produce\n",
      "final tables. While these studies achieve notable\n",
      "success, they primarily explored text-to-table performance before the LLM era. In addition, their\n",
      "evaluation datasets generally focus on a single domain, and adapt from the table-to-text datasets, resulting in hallucination issues. Thus, they are unsuitable for benchmarking LLMs in text-to-table.\n",
      "2.2 Large Language Models\n",
      "The advent of advanced LLMs,e.g., ChatGPT (OpenAI, 2022), GPT-4 (OpenAI, 2023), marks a pivotal\n",
      "moment that propels the field of NLP into a boom\n",
      "phase. Zhong et al. (2023) show that LLMs can\n",
      "achieve decent performance on benchmarks like\n",
      "GLUE (Wang et al., 2018), which spans eight representative NLP understanding tasks. Concurrently,\n",
      "several studies have scrutinized the performance\n",
      "of LLMs across various IE tasks. For example,\n",
      "Gao et al. (2023) test the capability of ChatGPT\n",
      "in event extraction. Similarly, González-Gallardo\n",
      "et al. (2023) employ ChatGPT on historical entity\n",
      "recognition. However, the results of these IE subtasks consistently show that LLMs underperform\n",
      "compared to state-of-the-art supervised approaches.\n",
      "The text-to-table task we focused on is more complex than the previous basic IE sub-tasks, however,\n",
      "it remains an unexplored area for evaluation on\n",
      "LLMs.\n",
      "3 CT-Eval\n",
      "In this section, we first discuss the data source for\n",
      "building CT-Eval (§ 3.1). Then, we give the details\n",
      "of how to control the hallucination in the preliminary collected data, including LLM hallucination\n",
      "judger (§ 3.2) and human cleaning processes (§ 3.3).\n",
      "Finally, we formulate the text-to-table task (§ 3.4)\n",
      "and provide the details of data statistics (§ 3.5).\n",
      "3.1 Data Source\n",
      "Following the success of WikiTableText (Bao et al.,\n",
      "2018), we also choose a multidisciplinary online\n",
      "encyclopedia as the data source to ensure data diversity. After carefully comparing existing Chinese\n",
      "online encyclopedias, we choose Baidu Baike2\n",
      ",\n",
      "which is one of the Chinese encyclopedias with\n",
      "the most entries in the world.\n",
      "We obtain the Baidu Baike data from the dumps\n",
      "provided by Xu et al. (2017). The data contains\n",
      "over 9M entity pages, each of which includes an\n",
      "infobox and the corresponding textual description,\n",
      "forming a text-to-table sample. Utilizing this data,\n",
      "we implement the following rule-based strategy\n",
      "for preliminary data cleaning: (1) Each page must\n",
      "contain at least one infobox; otherwise, the golden\n",
      "table is missing. (2) The number of tabular cells in\n",
      "the infobox should exceed three to ensure validity.\n",
      "(3) The length of the textual description should\n",
      "surpass 200 tokens. After that, 200K documenttable pairs are remaining for further processing.\n",
      "3.2 LLM Data Cleaning\n",
      "After the initial cleaning process, the hallucination issue persists due to Baidu Baike’s submission\n",
      "rules, wherein most of the page text and infoboxes\n",
      "are edited and maintained by individuals. Thus,\n",
      "the contents in the infoboxes may not always align\n",
      "precisely with the textual documents. For instance,\n",
      "some infoboxes may include additional knowledge\n",
      "unrelated to the text, potentially misleading the\n",
      "model from learning the text-to-table task.\n",
      "2\n",
      "https://baike.baidu.com/\n",
      "§ There is a document and a golden table that\n",
      "summarize the information contained in this\n",
      "document. Please help me identify whether the\n",
      "golden table contains additional information\n",
      "beyond the document. Give the reason first and\n",
      "then provide your judgment.\n",
      "Figure 1: Illustration of judgment prompt.\n",
      "To control the hallucination in the task samples,\n",
      "we decide to employ an LLM as a hallucination\n",
      "judger to filter out samples exhibiting hallucination.\n",
      "While using GPT-4 directly as the LLM hallucination judger is a straightforward approach, utilizing\n",
      "official APIs can be costly. Therefore, we first\n",
      "randomly select 5K samples from the data before\n",
      "cleaning. Then, we use GPT-4 to assess whether the\n",
      "golden tables contain additional information. The\n",
      "judgment prompt is illustrated in Figure 1, where\n",
      "the LLM is tasked with evaluating hallucination in\n",
      "a chain-of-thought manner. Next, we use the GPT4 judgment results to train an open-source LLM,\n",
      "employing the trained model to evaluate the remaining samples. Samples containing hallucinations are\n",
      "discarded. Given the capacity for understanding\n",
      "lengthy documents of existing Chinese LLMs (Bai\n",
      "et al., 2023b), we select ChatGLM3-6B-32k3\n",
      "as\n",
      "the open-source LLM hallucination judger. Finally,\n",
      "there are 88.6K samples after the data cleaning\n",
      "by the LLM hallucination judger. These samples\n",
      "totally cover 28 domains, e.g., physics and religion.\n",
      "3.3 Human Data Cleaning\n",
      "We split the LLM-cleaned samples into the training, validation, and test sets with 84.6K, 1K and\n",
      "1K samples, respectively. In the validation and test\n",
      "sets, we balance the number of samples in each\n",
      "domain to ensure a comprehensive evaluation. Furthermore, to mitigate the hallucination issue in the\n",
      "validation and test sets, we employ human annotators to manually cleanse their golden tables.\n",
      "In this phase, we employ five human annotators\n",
      "and one data expert, all of whom are native Chinese\n",
      "speakers with advanced educational qualifications.\n",
      "The data expert is a researcher with extensive experience in IE research. We first organize a tutorial\n",
      "for the five annotators to ensure alignment of annotation requirements. This includes clarifying the\n",
      "concept of the hallucination issue, emphasizing the\n",
      "additional information present in tables that cannot\n",
      "be directly extracted from the corresponding documents. Next, for each document-table pair in the\n",
      "3\n",
      "https://huggingface.co/THUDM/chatglm3-6b-32k\n",
      "CT-Eval\n",
      "Industry\n",
      "Informatics\n",
      "Physics\n",
      "Astronomy\n",
      "Traffic Engineering\n",
      "Biology\n",
      "Medical Science\n",
      "Culture\n",
      "Religion\n",
      "History\n",
      "Geography\n",
      "Education\n",
      "Business\n",
      "Finance\n",
      "Management\n",
      "Movie and Book\n",
      "Science Fiction\n",
      "Music\n",
      "Animation\n",
      "Dwelling\n",
      "Traveling\n",
      "Career\n",
      "Healthy\n",
      "Food\n",
      "Cosmetics\n",
      "Profile\n",
      "Security\n",
      "Military\n",
      "STEM\n",
      "24.0%\n",
      "Social Science\n",
      "60.9%\n",
      "Life\n",
      "Support\n",
      "11.6%\n",
      "O.\n",
      "3.5\n",
      "%\n",
      "Figure 2: Domain distribution in CT-EVAL\n",
      "validation and test sets, three annotators are asked\n",
      "to remove the hallucination information from tables. The data expert reviews 10% of the manually\n",
      "cleaned samples from each annotator. If the accuracy rate falls below 95%, the respective annotator\n",
      "will be required to redo the annotation. Finally,\n",
      "for each sample, if the three manually cleaned results are consistent, the results are saved as the final\n",
      "data. Otherwise, the results are decided by a group\n",
      "meeting among all annotators and the data expert.\n",
      "3.4 Task Overview\n",
      "Given a document D = {w1, w2,..., w|D|}, where\n",
      "wi\n",
      "is the i-th word in D, the text-to-table task aims\n",
      "to extract key information from D and outputs a table T = {c0,0, c0,1,..., c0,n, c1,0, c1,1,..., c1,n,...,\n",
      "ci,j,..., cm,n}, where c0,k(k ∈ {0, 1,..., n}) represents the column header and ck,0(k ∈ {0, 1,..., n})\n",
      "denotes as the row header. ci,j (i > 0, j > 0) represents the text of the cell in the i-th row and j-th\n",
      "column in the table with m rows and n columns.\n",
      "3.5 Data Statistics\n",
      "We compare CT-Eval with the previous datasets\n",
      "across several metrics including language, number of entries, average length, number of domains,\n",
      "hallucination rate, and average number of cells.\n",
      "As shown in Table 1, compared to previous\n",
      "datasets that mostly focus on one domain, CT-Eval\n",
      "covers 28 domains, offering a valuable resource\n",
      "to evaluate the text-to-table capabilities of LLMs\n",
      "across multiple domains. These 28 domains can\n",
      "be categorized into four branches: STEM, social\n",
      "science, life support and others. To provide a comprehensive understanding of domain coverage in\n",
      "CT-Eval, we present the distributions of each domain in Figure 2. Notably, the “Social Science”\n",
      "Original Item\n",
      "Translation\n",
      "材料 做法 厨师一点通\n",
      "炖牛肉时，可以放几个\n",
      "山楂进去，这样牛肉会\n",
      "烂得快些，而且有股山\n",
      "楂的清香。\n",
      "1.牛肉洗净放入清水锅\n",
      "中煮至七成熟，捞出切\n",
      "成方块\n",
      "2. …\n",
      "材料：\n",
      "牛肉，胡萝卜，\n",
      "白萝卜，大料，\n",
      "香叶\n",
      "东坡牛肉\n",
      "Original Item\n",
      "Ingredients Cooking method Cooking Tips\n",
      "When stewing beef, you\n",
      "can put a few hawthorn\n",
      "into it, so that the beef\n",
      "will rot faster and have a\n",
      "hawthorn fragrance.\n",
      "1. Wash the beef and\n",
      "put it into a pot of\n",
      "water to boil until\n",
      "it is seven mature,\n",
      "fish out and cut it\n",
      "into squares…\n",
      "2. …\n",
      "Ingredients:\n",
      "Beef, carrots, white\n",
      "radish, Chinese\n",
      "spices, allspice\n",
      "Dongpo\n",
      "Beef\n",
      "“标题”: “东坡牛肉”, “ ”章节“: [{”标题“: ”东坡牛肉的做法“},\n",
      "内容“: ”葱段、姜片、蒜头、辣椒、香叶、大料、盐、白糖、番茄酱、排骨\n",
      "酱、辣椒酱、高汤（用煮牛肉的汤即可）。\n",
      "做法：\n",
      "1、先来处理一下牛肉，\n",
      "将牛腩在冷水中多泡一会…“}\n",
      "Translation\n",
      "“Title”: “Dongpo Beef”, “ ”Chapter“: [{”Title “: ”Dongpo Beef Practice“}, :\n",
      "Ingredients：Diced green onion, ginger, garlic, chili pepper, coriander, dashi, salt,\n",
      "sugar, tomato sauce, rib sauce, chili sauce, broth. Cooking method:1, first, let's deal\n",
      "with the beef, soak the brisket in cold water for a while longer...\"}\n",
      "Figure 3: Text-to-table example from CT-EVAL.\n",
      "branch exhibits the highest proportion, encompassing domains such as culture and religion. Conversely, the “Other” branch, which includes topics\n",
      "related to cosmetics and profiles, represents the\n",
      "smallest portion. The “STEM” (science, technology, engineering, and mathematics) branch features\n",
      "data related to topics such as physical sciences and\n",
      "astronomy. The “Life Support” branch is highly\n",
      "relevant to everyday life, comprises 11.60% of the\n",
      "data, and primarily focuses on domains such as\n",
      "dwelling and health. An illustration of a data sample from CT-Eval is depicted in Figure 3. Each\n",
      "document-table pair, akin to the example shown,\n",
      "consists of a textual description and a golden table\n",
      "with varying numbers of columns and rows.\n",
      "To assess the quality of our dataset compared\n",
      "to previous ones, we randomly select 200 samples\n",
      "from the training, validation and test sets of CTEval, and previous E2E, Rotowire, WikiBio, and\n",
      "WikiTableText, respectively. Then, we compute the\n",
      "hallucination rate for each dataset through human\n",
      "annotation. Specifically, three annotators proficient\n",
      "in both English and Chinese judged whether the\n",
      "golden tables contained hallucination information\n",
      "following the guidance outlined in Section 3.3. The\n",
      "hallucination rate for each dataset is determined\n",
      "by the average proportion of hallucinated samples\n",
      "judged by all three annotators. We observe that\n",
      "the hallucination rate of the CT-Eval training set\n",
      "is 6.83%, similar to Rotowire (6.17%). Moreover,\n",
      "with the help of our human cleaning, the hallucination rates in our test and validation sets decrease to\n",
      "1.00% and 1.50%, respectively, significantly lower\n",
      "Dataset Language N. Entries Avg. Length N. Domains Hallu. R Avg. Cells\n",
      "E2E English 42,061 90.58 Restaurant 4.17% 4.46\n",
      "Rotowire English 3,398 1311.01 Sports 6.17% 40.49\n",
      "WikiBio English 582,659 416.71 Biography 8.67% 4.19\n",
      "WikiTableText English 10,000 59.76 Multiple Subclasses 18.83% 4.25\n",
      "CT-Eval(train) Chinese 84,603 911.46 28 Subclasses 6.83% 11.40\n",
      "CT-Eval(val.) Chinese 1,000 813.32 28 Subclasses 1.00% 10.78\n",
      "CT-Eval(test) Chinese 1,000 845.22 28 Subclasses 1.50% 10.86\n",
      "Table 1: Data Statistics of CT-Eval and previous text-to-table datasets. “N. Entries” denotes the number of entries.\n",
      "“Hallu. R” indicates hallucination rate. “Avg.Cells” indicates the average number of table cells\n",
      "than those in other datasets. Therefore, the reliability of our dataset can be verified. Regarding\n",
      "domains, the E2E, Rotowire and WikiBio datasets\n",
      "focus on single domains, whereas WikiTableText\n",
      "and CT-Eval encompass multiple domains to ensure data diversity. Concerning the length of the input documents, we note that previous E2E and WikiTableText primarily contain short documents with\n",
      "an average length of under 100 words, whereas documents in our dataset and Rotowire exceed words\n",
      "or characters, presenting more complex inputs for\n",
      "text-to-table models. In summary, CT-Eval stands\n",
      "as the sole dataset fulfilling criteria of data diversity,\n",
      "lengthy documents, and low hallucination.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is CT-Eval?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "CT-Eval is a benchmark dataset for evaluating the performance of large language models (LLMs) on the text-to-table task in Chinese. The dataset is specifically designed to ensure data diversity, low hallucination rate, and lengthy documents, which makes it\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "1 Introduction\n",
      "Information extraction (IE) aims to identify and extract structured information from unstructured text.\n",
      "1Codes and data will be publicly available once accepted.\n",
      "Basic IE sub-tasks, such as, named entity recognition and entity linking, generally operate at the\n",
      "sentence level, which ignore models’ ability to understand the document-level meaning. In contrast,\n",
      "Text-to-Table, an emerging sub-task of IE, requires\n",
      "models to understand information within a given\n",
      "document and then generate structured tables. Despite great success in the domain Wu et al. (2022);\n",
      "Li et al. (2023), previous studies are oriented to\n",
      "English, limiting the research in other languages.\n",
      "Recently, large language models (LLMs) have\n",
      "demonstrated powerful performance across various NLP tasks (Zhao et al., 2023; Wang et al.,\n",
      "2023a,b,c). Some studies (González-Gallardo et al.,\n",
      "2023; Gao et al., 2023) have also utilized LLMs for\n",
      "several IE sub-tasks, and found that compared to\n",
      "supervised baselines, the performance of LLMs is\n",
      "sub-optimal. However, their performance of LLMs\n",
      "on Text-to-Table remains largely unexplored. Additionally, the rapid advancements in LLMs have facilitated their widespread adoption in multi-lingual\n",
      "settings, enabling language modeling abilities to be\n",
      "shared across different languages. Consequently, it\n",
      "is theoretically possible to feasible to employ LLMs\n",
      "for text-to-table in other languages, an area that\n",
      "has yet to be thoroughly investigated.\n",
      "Motivated by the aforementioned considerations,\n",
      "we decide to benchmark LLMs on Chinese text-totable. A heuristic approach involves translating existing English datasets into Chinese for subsequent\n",
      "performance evaluation. There are four datasets\n",
      "widely used in text-to-table: E2E (Novikova et al.,\n",
      "2017) is a restaurant domain dataset encompassing\n",
      "51.5K samples about restaurant information like\n",
      "names, addresses, rate scores, etc. Rotowire (Wiseman et al., 2017) is a sports domain dataset derived\n",
      "from NBA basketball games with 4.9K samples\n",
      "about NBA team information. WikiBio (Lebret\n",
      "et al., 2016), compiled from Wikipedia, containing\n",
      "72.6K documents and corresponding structured tables in the biography domain. WikiTableText (Bao\n",
      "arXiv:2405.12174v1 [cs.CL] 20 May 2024\n",
      "et al., 2018) is also derived from Wikipedia, and\n",
      "involves 13.3K multidisciplinary samples spanning\n",
      "fields like finance and politics. However, according to our preliminary analysis, these datasets also\n",
      "suffer from the issues of less diversity or high hallucination, making them unsuitable to benchmark\n",
      "LLMs. Specifically, (1) E2E, Rotowire, and WikiBio exhibit a lack of diversity as they focus on a\n",
      "single domain, violating the core principle of instruction tuning in LLMs, that is, diversity. (2)\n",
      "Although WikiTableText incorporates multiple domains for diversity, our analysis (§ 3.5) indicates\n",
      "that 18.83% of samples exhibit hallucination in\n",
      "the golden tables, that is, containing additional information beyond the provided documents. This\n",
      "arises from treating Wikipedia infoboxes as golden\n",
      "tables, created collaboratively by online users and\n",
      "potentially containing additional basic information.\n",
      "In this paper, we propose the Chinese Text-toTable Evaluation (CT-Eval) dataset, which is constructed through three steps to ensure data diversity\n",
      "and minimize hallucination. To ensure diversity,\n",
      "the first step involves collecting multidisciplinary\n",
      "document-table pairs. We choose the Baidu Baike\n",
      "as the data source, which is a popular Chinese multidisciplinary online encyclopedia. Each page in\n",
      "this source contains text and an infobox summarizing the corresponding key information. Second, to\n",
      "minimize data hallucination, we train an LLM, as a\n",
      "hallucination judger, to filter out task samples with\n",
      "hallucination in their golden tables (infoboxes). Finally, we obtain 88.6K samples with an average\n",
      "length of 911.46 Chinese characters. We split them\n",
      "into 86.6K, 1K and 1K for training, validation and\n",
      "testing. For validation and testing samples, human\n",
      "annotators further clean data hallucination in the\n",
      "golden tables to ensure evaluation reliability.\n",
      "Based on the proposed CT-Eval, we benchmark\n",
      "various mainstream LLMs in both zero-shot (for\n",
      "both open- and closed-source LLMs) and finetuning (only for open-source LLMs) scenarios. Our\n",
      "experiments reveal that (1) GPT-4 achieves the best\n",
      "zero-shot performance among all LLMs. However,\n",
      "its performance remains a discernible disparity\n",
      "compared to human judgment. (2) After fine-tuning\n",
      "on the training set of CT-Eval, all open-source\n",
      "LLMs demonstrate a significant performance improvement, outperforming zero-shot GPT-4 by a\n",
      "large margin, indicating the effectiveness of CTEval. In-depth analyses of LLM-generated tables\n",
      "reveal the persistence of hallucination issues in\n",
      "both zero-shot and fine-tuned LLMs, highlighting\n",
      "a challenge in using LLMs as text-to-table systems.\n",
      "Future work could not only evaluate LLMs’ performance on text-to-table via our CT-Eval benchmark\n",
      "dataset, but also leverage its training data to improve LLMs’ text-to-table ability via fine-tuning.\n",
      "2 Related Work\n",
      "2.1 Text-to-table Tasks\n",
      "Wu et al. (2022) pioneer the text-to-table task.\n",
      "Given the absence of a dedicated text-to-table\n",
      "dataset, Wu et al. (2022) repurpose existing tableto-text datasets, i.e., WikiBio (Lebret et al., 2016),\n",
      "E2E (Novikova et al., 2017), Rotowire (Wiseman et al., 2017) and WikiTableText (Bao et al.,\n",
      "2018), for text-to-table tasks by reversing their\n",
      "input-output pairs. They fine-tune BART (Lewis\n",
      "et al., 2019) to perform text-to-table in a sequenceto-sequence manner, and find that the fine-tuned\n",
      "BART outperforms the pipeline baselines using\n",
      "relation extraction and named entity extraction.\n",
      "STable (Pietruszka et al., 2022) employs two pretrained language models (PLMs) (T5 (Raffel et al.,\n",
      "2020) and TILT (Powalski et al., 2021)) for text-totable, and designs a permutation-based decoder to\n",
      "enhance the PLMs’ table generation ability. Subsequently, Li et al. (2023) find that the predefined row\n",
      "order in golden tables introduces bias into text-totable models. Consequently, they train table header\n",
      "and table body generators separately to produce\n",
      "final tables. While these studies achieve notable\n",
      "success, they primarily explored text-to-table performance before the LLM era. In addition, their\n",
      "evaluation datasets generally focus on a single domain, and adapt from the table-to-text datasets, resulting in hallucination issues. Thus, they are unsuitable for benchmarking LLMs in text-to-table.\n",
      "2.2 Large Language Models\n",
      "The advent of advanced LLMs,e.g., ChatGPT (OpenAI, 2022), GPT-4 (OpenAI, 2023), marks a pivotal\n",
      "moment that propels the field of NLP into a boom\n",
      "phase. Zhong et al. (2023) show that LLMs can\n",
      "achieve decent performance on benchmarks like\n",
      "GLUE (Wang et al., 2018), which spans eight representative NLP understanding tasks. Concurrently,\n",
      "several studies have scrutinized the performance\n",
      "of LLMs across various IE tasks. For example,\n",
      "Gao et al. (2023) test the capability of ChatGPT\n",
      "in event extraction. Similarly, González-Gallardo\n",
      "et al. (2023) employ ChatGPT on historical entity\n",
      "recognition. However, the results of these IE subtasks consistently show that LLMs underperform\n",
      "compared to state-of-the-art supervised approaches.\n",
      "The text-to-table task we focused on is more complex than the previous basic IE sub-tasks, however,\n",
      "it remains an unexplored area for evaluation on\n",
      "LLMs.\n",
      "3 CT-Eval\n",
      "In this section, we first discuss the data source for\n",
      "building CT-Eval (§ 3.1). Then, we give the details\n",
      "of how to control the hallucination in the preliminary collected data, including LLM hallucination\n",
      "judger (§ 3.2) and human cleaning processes (§ 3.3).\n",
      "Finally, we formulate the text-to-table task (§ 3.4)\n",
      "and provide the details of data statistics (§ 3.5).\n",
      "3.1 Data Source\n",
      "Following the success of WikiTableText (Bao et al.,\n",
      "2018), we also choose a multidisciplinary online\n",
      "encyclopedia as the data source to ensure data diversity. After carefully comparing existing Chinese\n",
      "online encyclopedias, we choose Baidu Baike2\n",
      ",\n",
      "which is one of the Chinese encyclopedias with\n",
      "the most entries in the world.\n",
      "We obtain the Baidu Baike data from the dumps\n",
      "provided by Xu et al. (2017). The data contains\n",
      "over 9M entity pages, each of which includes an\n",
      "infobox and the corresponding textual description,\n",
      "forming a text-to-table sample. Utilizing this data,\n",
      "we implement the following rule-based strategy\n",
      "for preliminary data cleaning: (1) Each page must\n",
      "contain at least one infobox; otherwise, the golden\n",
      "table is missing. (2) The number of tabular cells in\n",
      "the infobox should exceed three to ensure validity.\n",
      "(3) The length of the textual description should\n",
      "surpass 200 tokens. After that, 200K documenttable pairs are remaining for further processing.\n",
      "3.2 LLM Data Cleaning\n",
      "After the initial cleaning process, the hallucination issue persists due to Baidu Baike’s submission\n",
      "rules, wherein most of the page text and infoboxes\n",
      "are edited and maintained by individuals. Thus,\n",
      "the contents in the infoboxes may not always align\n",
      "precisely with the textual documents. For instance,\n",
      "some infoboxes may include additional knowledge\n",
      "unrelated to the text, potentially misleading the\n",
      "model from learning the text-to-table task.\n",
      "2\n",
      "https://baike.baidu.com/\n",
      "§ There is a document and a golden table that\n",
      "summarize the information contained in this\n",
      "document. Please help me identify whether the\n",
      "golden table contains additional information\n",
      "beyond the document. Give the reason first and\n",
      "then provide your judgment.\n",
      "Figure 1: Illustration of judgment prompt.\n",
      "To control the hallucination in the task samples,\n",
      "we decide to employ an LLM as a hallucination\n",
      "judger to filter out samples exhibiting hallucination.\n",
      "While using GPT-4 directly as the LLM hallucination judger is a straightforward approach, utilizing\n",
      "official APIs can be costly. Therefore, we first\n",
      "randomly select 5K samples from the data before\n",
      "cleaning. Then, we use GPT-4 to assess whether the\n",
      "golden tables contain additional information. The\n",
      "judgment prompt is illustrated in Figure 1, where\n",
      "the LLM is tasked with evaluating hallucination in\n",
      "a chain-of-thought manner. Next, we use the GPT4 judgment results to train an open-source LLM,\n",
      "employing the trained model to evaluate the remaining samples. Samples containing hallucinations are\n",
      "discarded. Given the capacity for understanding\n",
      "lengthy documents of existing Chinese LLMs (Bai\n",
      "et al., 2023b), we select ChatGLM3-6B-32k3\n",
      "as\n",
      "the open-source LLM hallucination judger. Finally,\n",
      "there are 88.6K samples after the data cleaning\n",
      "by the LLM hallucination judger. These samples\n",
      "totally cover 28 domains, e.g., physics and religion.\n",
      "3.3 Human Data Cleaning\n",
      "We split the LLM-cleaned samples into the training, validation, and test sets with 84.6K, 1K and\n",
      "1K samples, respectively. In the validation and test\n",
      "sets, we balance the number of samples in each\n",
      "domain to ensure a comprehensive evaluation. Furthermore, to mitigate the hallucination issue in the\n",
      "validation and test sets, we employ human annotators to manually cleanse their golden tables.\n",
      "In this phase, we employ five human annotators\n",
      "and one data expert, all of whom are native Chinese\n",
      "speakers with advanced educational qualifications.\n",
      "The data expert is a researcher with extensive experience in IE research. We first organize a tutorial\n",
      "for the five annotators to ensure alignment of annotation requirements. This includes clarifying the\n",
      "concept of the hallucination issue, emphasizing the\n",
      "additional information present in tables that cannot\n",
      "be directly extracted from the corresponding documents. Next, for each document-table pair in the\n",
      "3\n",
      "https://huggingface.co/THUDM/chatglm3-6b-32k\n",
      "CT-Eval\n",
      "Industry\n",
      "Informatics\n",
      "Physics\n",
      "Astronomy\n",
      "Traffic Engineering\n",
      "Biology\n",
      "Medical Science\n",
      "Culture\n",
      "Religion\n",
      "History\n",
      "Geography\n",
      "Education\n",
      "Business\n",
      "Finance\n",
      "Management\n",
      "Movie and Book\n",
      "Science Fiction\n",
      "Music\n",
      "Animation\n",
      "Dwelling\n",
      "Traveling\n",
      "Career\n",
      "Healthy\n",
      "Food\n",
      "Cosmetics\n",
      "Profile\n",
      "Security\n",
      "Military\n",
      "STEM\n",
      "24.0%\n",
      "Social Science\n",
      "60.9%\n",
      "Life\n",
      "Support\n",
      "11.6%\n",
      "O.\n",
      "3.5\n",
      "%\n",
      "Figure 2: Domain distribution in CT-EVAL\n",
      "validation and test sets, three annotators are asked\n",
      "to remove the hallucination information from tables. The data expert reviews 10% of the manually\n",
      "cleaned samples from each annotator. If the accuracy rate falls below 95%, the respective annotator\n",
      "will be required to redo the annotation. Finally,\n",
      "for each sample, if the three manually cleaned results are consistent, the results are saved as the final\n",
      "data. Otherwise, the results are decided by a group\n",
      "meeting among all annotators and the data expert.\n",
      "3.4 Task Overview\n",
      "Given a document D = {w1, w2,..., w|D|}, where\n",
      "wi\n",
      "is the i-th word in D, the text-to-table task aims\n",
      "to extract key information from D and outputs a table T = {c0,0, c0,1,..., c0,n, c1,0, c1,1,..., c1,n,...,\n",
      "ci,j,..., cm,n}, where c0,k(k ∈ {0, 1,..., n}) represents the column header and ck,0(k ∈ {0, 1,..., n})\n",
      "denotes as the row header. ci,j (i > 0, j > 0) represents the text of the cell in the i-th row and j-th\n",
      "column in the table with m rows and n columns.\n",
      "3.5 Data Statistics\n",
      "We compare CT-Eval with the previous datasets\n",
      "across several metrics including language, number of entries, average length, number of domains,\n",
      "hallucination rate, and average number of cells.\n",
      "As shown in Table 1, compared to previous\n",
      "datasets that mostly focus on one domain, CT-Eval\n",
      "covers 28 domains, offering a valuable resource\n",
      "to evaluate the text-to-table capabilities of LLMs\n",
      "across multiple domains. These 28 domains can\n",
      "be categorized into four branches: STEM, social\n",
      "science, life support and others. To provide a comprehensive understanding of domain coverage in\n",
      "CT-Eval, we present the distributions of each domain in Figure 2. Notably, the “Social Science”\n",
      "Original Item\n",
      "Translation\n",
      "材料 做法 厨师一点通\n",
      "炖牛肉时，可以放几个\n",
      "山楂进去，这样牛肉会\n",
      "烂得快些，而且有股山\n",
      "楂的清香。\n",
      "1.牛肉洗净放入清水锅\n",
      "中煮至七成熟，捞出切\n",
      "成方块\n",
      "2. …\n",
      "材料：\n",
      "牛肉，胡萝卜，\n",
      "白萝卜，大料，\n",
      "香叶\n",
      "东坡牛肉\n",
      "Original Item\n",
      "Ingredients Cooking method Cooking Tips\n",
      "When stewing beef, you\n",
      "can put a few hawthorn\n",
      "into it, so that the beef\n",
      "will rot faster and have a\n",
      "hawthorn fragrance.\n",
      "1. Wash the beef and\n",
      "put it into a pot of\n",
      "water to boil until\n",
      "it is seven mature,\n",
      "fish out and cut it\n",
      "into squares…\n",
      "2. …\n",
      "Ingredients:\n",
      "Beef, carrots, white\n",
      "radish, Chinese\n",
      "spices, allspice\n",
      "Dongpo\n",
      "Beef\n",
      "“标题”: “东坡牛肉”, “ ”章节“: [{”标题“: ”东坡牛肉的做法“},\n",
      "内容“: ”葱段、姜片、蒜头、辣椒、香叶、大料、盐、白糖、番茄酱、排骨\n",
      "酱、辣椒酱、高汤（用煮牛肉的汤即可）。\n",
      "做法：\n",
      "1、先来处理一下牛肉，\n",
      "将牛腩在冷水中多泡一会…“}\n",
      "Translation\n",
      "“Title”: “Dongpo Beef”, “ ”Chapter“: [{”Title “: ”Dongpo Beef Practice“}, :\n",
      "Ingredients：Diced green onion, ginger, garlic, chili pepper, coriander, dashi, salt,\n",
      "sugar, tomato sauce, rib sauce, chili sauce, broth. Cooking method:1, first, let's deal\n",
      "with the beef, soak the brisket in cold water for a while longer...\"}\n",
      "Figure 3: Text-to-table example from CT-EVAL.\n",
      "branch exhibits the highest proportion, encompassing domains such as culture and religion. Conversely, the “Other” branch, which includes topics\n",
      "related to cosmetics and profiles, represents the\n",
      "smallest portion. The “STEM” (science, technology, engineering, and mathematics) branch features\n",
      "data related to topics such as physical sciences and\n",
      "astronomy. The “Life Support” branch is highly\n",
      "relevant to everyday life, comprises 11.60% of the\n",
      "data, and primarily focuses on domains such as\n",
      "dwelling and health. An illustration of a data sample from CT-Eval is depicted in Figure 3. Each\n",
      "document-table pair, akin to the example shown,\n",
      "consists of a textual description and a golden table\n",
      "with varying numbers of columns and rows.\n",
      "To assess the quality of our dataset compared\n",
      "to previous ones, we randomly select 200 samples\n",
      "from the training, validation and test sets of CTEval, and previous E2E, Rotowire, WikiBio, and\n",
      "WikiTableText, respectively. Then, we compute the\n",
      "hallucination rate for each dataset through human\n",
      "annotation. Specifically, three annotators proficient\n",
      "in both English and Chinese judged whether the\n",
      "golden tables contained hallucination information\n",
      "following the guidance outlined in Section 3.3. The\n",
      "hallucination rate for each dataset is determined\n",
      "by the average proportion of hallucinated samples\n",
      "judged by all three annotators. We observe that\n",
      "the hallucination rate of the CT-Eval training set\n",
      "is 6.83%, similar to Rotowire (6.17%). Moreover,\n",
      "with the help of our human cleaning, the hallucination rates in our test and validation sets decrease to\n",
      "1.00% and 1.50%, respectively, significantly lower\n",
      "Dataset Language N. Entries Avg. Length N. Domains Hallu. R Avg. Cells\n",
      "E2E English 42,061 90.58 Restaurant 4.17% 4.46\n",
      "Rotowire English 3,398 1311.01 Sports 6.17% 40.49\n",
      "WikiBio English 582,659 416.71 Biography 8.67% 4.19\n",
      "WikiTableText English 10,000 59.76 Multiple Subclasses 18.83% 4.25\n",
      "CT-Eval(train) Chinese 84,603 911.46 28 Subclasses 6.83% 11.40\n",
      "CT-Eval(val.) Chinese 1,000 813.32 28 Subclasses 1.00% 10.78\n",
      "CT-Eval(test) Chinese 1,000 845.22 28 Subclasses 1.50% 10.86\n",
      "Table 1: Data Statistics of CT-Eval and previous text-to-table datasets. “N. Entries” denotes the number of entries.\n",
      "“Hallu. R” indicates hallucination rate. “Avg.Cells” indicates the average number of table cells\n",
      "than those in other datasets. Therefore, the reliability of our dataset can be verified. Regarding\n",
      "domains, the E2E, Rotowire and WikiBio datasets\n",
      "focus on single domains, whereas WikiTableText\n",
      "and CT-Eval encompass multiple domains to ensure data diversity. Concerning the length of the input documents, we note that previous E2E and WikiTableText primarily contain short documents with\n",
      "an average length of under 100 words, whereas documents in our dataset and Rotowire exceed words\n",
      "or characters, presenting more complex inputs for\n",
      "text-to-table models. In summary, CT-Eval stands\n",
      "as the sole dataset fulfilling criteria of data diversity,\n",
      "lengthy documents, and low hallucination.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is CT-Eval?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "CT-Eval is a dataset designed for evaluating the performance of large language models (LLMs) in the text-to-table task. CT-Eval is a Chinese text-to-table dataset that aims to benchmark the ability of LLMs to extract structured information\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "1 Introduction\n",
      "Information extraction (IE) aims to identify and extract structured information from unstructured text.\n",
      "1Codes and data will be publicly available once accepted.\n",
      "Basic IE sub-tasks, such as, named entity recognition and entity linking, generally operate at the\n",
      "sentence level, which ignore models’ ability to understand the document-level meaning. In contrast,\n",
      "Text-to-Table, an emerging sub-task of IE, requires\n",
      "models to understand information within a given\n",
      "document and then generate structured tables. Despite great success in the domain Wu et al. (2022);\n",
      "Li et al. (2023), previous studies are oriented to\n",
      "English, limiting the research in other languages.\n",
      "Recently, large language models (LLMs) have\n",
      "demonstrated powerful performance across various NLP tasks (Zhao et al., 2023; Wang et al.,\n",
      "2023a,b,c). Some studies (González-Gallardo et al.,\n",
      "2023; Gao et al., 2023) have also utilized LLMs for\n",
      "several IE sub-tasks, and found that compared to\n",
      "supervised baselines, the performance of LLMs is\n",
      "sub-optimal. However, their performance of LLMs\n",
      "on Text-to-Table remains largely unexplored. Additionally, the rapid advancements in LLMs have facilitated their widespread adoption in multi-lingual\n",
      "settings, enabling language modeling abilities to be\n",
      "shared across different languages. Consequently, it\n",
      "is theoretically possible to feasible to employ LLMs\n",
      "for text-to-table in other languages, an area that\n",
      "has yet to be thoroughly investigated.\n",
      "Motivated by the aforementioned considerations,\n",
      "we decide to benchmark LLMs on Chinese text-totable. A heuristic approach involves translating existing English datasets into Chinese for subsequent\n",
      "performance evaluation. There are four datasets\n",
      "widely used in text-to-table: E2E (Novikova et al.,\n",
      "2017) is a restaurant domain dataset encompassing\n",
      "51.5K samples about restaurant information like\n",
      "names, addresses, rate scores, etc. Rotowire (Wiseman et al., 2017) is a sports domain dataset derived\n",
      "from NBA basketball games with 4.9K samples\n",
      "about NBA team information. WikiBio (Lebret\n",
      "et al., 2016), compiled from Wikipedia, containing\n",
      "72.6K documents and corresponding structured tables in the biography domain. WikiTableText (Bao\n",
      "arXiv:2405.12174v1 [cs.CL] 20 May 2024\n",
      "et al., 2018) is also derived from Wikipedia, and\n",
      "involves 13.3K multidisciplinary samples spanning\n",
      "fields like finance and politics. However, according to our preliminary analysis, these datasets also\n",
      "suffer from the issues of less diversity or high hallucination, making them unsuitable to benchmark\n",
      "LLMs. Specifically, (1) E2E, Rotowire, and WikiBio exhibit a lack of diversity as they focus on a\n",
      "single domain, violating the core principle of instruction tuning in LLMs, that is, diversity. (2)\n",
      "Although WikiTableText incorporates multiple domains for diversity, our analysis (§ 3.5) indicates\n",
      "that 18.83% of samples exhibit hallucination in\n",
      "the golden tables, that is, containing additional information beyond the provided documents. This\n",
      "arises from treating Wikipedia infoboxes as golden\n",
      "tables, created collaboratively by online users and\n",
      "potentially containing additional basic information.\n",
      "In this paper, we propose the Chinese Text-toTable Evaluation (CT-Eval) dataset, which is constructed through three steps to ensure data diversity\n",
      "and minimize hallucination. To ensure diversity,\n",
      "the first step involves collecting multidisciplinary\n",
      "document-table pairs. We choose the Baidu Baike\n",
      "as the data source, which is a popular Chinese multidisciplinary online encyclopedia. Each page in\n",
      "this source contains text and an infobox summarizing the corresponding key information. Second, to\n",
      "minimize data hallucination, we train an LLM, as a\n",
      "hallucination judger, to filter out task samples with\n",
      "hallucination in their golden tables (infoboxes). Finally, we obtain 88.6K samples with an average\n",
      "length of 911.46 Chinese characters. We split them\n",
      "into 86.6K, 1K and 1K for training, validation and\n",
      "testing. For validation and testing samples, human\n",
      "annotators further clean data hallucination in the\n",
      "golden tables to ensure evaluation reliability.\n",
      "Based on the proposed CT-Eval, we benchmark\n",
      "various mainstream LLMs in both zero-shot (for\n",
      "both open- and closed-source LLMs) and finetuning (only for open-source LLMs) scenarios. Our\n",
      "experiments reveal that (1) GPT-4 achieves the best\n",
      "zero-shot performance among all LLMs. However,\n",
      "its performance remains a discernible disparity\n",
      "compared to human judgment. (2) After fine-tuning\n",
      "on the training set of CT-Eval, all open-source\n",
      "LLMs demonstrate a significant performance improvement, outperforming zero-shot GPT-4 by a\n",
      "large margin, indicating the effectiveness of CTEval. In-depth analyses of LLM-generated tables\n",
      "reveal the persistence of hallucination issues in\n",
      "both zero-shot and fine-tuned LLMs, highlighting\n",
      "a challenge in using LLMs as text-to-table systems.\n",
      "Future work could not only evaluate LLMs’ performance on text-to-table via our CT-Eval benchmark\n",
      "dataset, but also leverage its training data to improve LLMs’ text-to-table ability via fine-tuning.\n",
      "2 Related Work\n",
      "2.1 Text-to-table Tasks\n",
      "Wu et al. (2022) pioneer the text-to-table task.\n",
      "Given the absence of a dedicated text-to-table\n",
      "dataset, Wu et al. (2022) repurpose existing tableto-text datasets, i.e., WikiBio (Lebret et al., 2016),\n",
      "E2E (Novikova et al., 2017), Rotowire (Wiseman et al., 2017) and WikiTableText (Bao et al.,\n",
      "2018), for text-to-table tasks by reversing their\n",
      "input-output pairs. They fine-tune BART (Lewis\n",
      "et al., 2019) to perform text-to-table in a sequenceto-sequence manner, and find that the fine-tuned\n",
      "BART outperforms the pipeline baselines using\n",
      "relation extraction and named entity extraction.\n",
      "STable (Pietruszka et al., 2022) employs two pretrained language models (PLMs) (T5 (Raffel et al.,\n",
      "2020) and TILT (Powalski et al., 2021)) for text-totable, and designs a permutation-based decoder to\n",
      "enhance the PLMs’ table generation ability. Subsequently, Li et al. (2023) find that the predefined row\n",
      "order in golden tables introduces bias into text-totable models. Consequently, they train table header\n",
      "and table body generators separately to produce\n",
      "final tables. While these studies achieve notable\n",
      "success, they primarily explored text-to-table performance before the LLM era. In addition, their\n",
      "evaluation datasets generally focus on a single domain, and adapt from the table-to-text datasets, resulting in hallucination issues. Thus, they are unsuitable for benchmarking LLMs in text-to-table.\n",
      "2.2 Large Language Models\n",
      "The advent of advanced LLMs,e.g., ChatGPT (OpenAI, 2022), GPT-4 (OpenAI, 2023), marks a pivotal\n",
      "moment that propels the field of NLP into a boom\n",
      "phase. Zhong et al. (2023) show that LLMs can\n",
      "achieve decent performance on benchmarks like\n",
      "GLUE (Wang et al., 2018), which spans eight representative NLP understanding tasks. Concurrently,\n",
      "several studies have scrutinized the performance\n",
      "of LLMs across various IE tasks. For example,\n",
      "Gao et al. (2023) test the capability of ChatGPT\n",
      "in event extraction. Similarly, González-Gallardo\n",
      "et al. (2023) employ ChatGPT on historical entity\n",
      "recognition. However, the results of these IE subtasks consistently show that LLMs underperform\n",
      "compared to state-of-the-art supervised approaches.\n",
      "The text-to-table task we focused on is more complex than the previous basic IE sub-tasks, however,\n",
      "it remains an unexplored area for evaluation on\n",
      "LLMs.\n",
      "3 CT-Eval\n",
      "In this section, we first discuss the data source for\n",
      "building CT-Eval (§ 3.1). Then, we give the details\n",
      "of how to control the hallucination in the preliminary collected data, including LLM hallucination\n",
      "judger (§ 3.2) and human cleaning processes (§ 3.3).\n",
      "Finally, we formulate the text-to-table task (§ 3.4)\n",
      "and provide the details of data statistics (§ 3.5).\n",
      "3.1 Data Source\n",
      "Following the success of WikiTableText (Bao et al.,\n",
      "2018), we also choose a multidisciplinary online\n",
      "encyclopedia as the data source to ensure data diversity. After carefully comparing existing Chinese\n",
      "online encyclopedias, we choose Baidu Baike2\n",
      ",\n",
      "which is one of the Chinese encyclopedias with\n",
      "the most entries in the world.\n",
      "We obtain the Baidu Baike data from the dumps\n",
      "provided by Xu et al. (2017). The data contains\n",
      "over 9M entity pages, each of which includes an\n",
      "infobox and the corresponding textual description,\n",
      "forming a text-to-table sample. Utilizing this data,\n",
      "we implement the following rule-based strategy\n",
      "for preliminary data cleaning: (1) Each page must\n",
      "contain at least one infobox; otherwise, the golden\n",
      "table is missing. (2) The number of tabular cells in\n",
      "the infobox should exceed three to ensure validity.\n",
      "(3) The length of the textual description should\n",
      "surpass 200 tokens. After that, 200K documenttable pairs are remaining for further processing.\n",
      "3.2 LLM Data Cleaning\n",
      "After the initial cleaning process, the hallucination issue persists due to Baidu Baike’s submission\n",
      "rules, wherein most of the page text and infoboxes\n",
      "are edited and maintained by individuals. Thus,\n",
      "the contents in the infoboxes may not always align\n",
      "precisely with the textual documents. For instance,\n",
      "some infoboxes may include additional knowledge\n",
      "unrelated to the text, potentially misleading the\n",
      "model from learning the text-to-table task.\n",
      "2\n",
      "https://baike.baidu.com/\n",
      "§ There is a document and a golden table that\n",
      "summarize the information contained in this\n",
      "document. Please help me identify whether the\n",
      "golden table contains additional information\n",
      "beyond the document. Give the reason first and\n",
      "then provide your judgment.\n",
      "Figure 1: Illustration of judgment prompt.\n",
      "To control the hallucination in the task samples,\n",
      "we decide to employ an LLM as a hallucination\n",
      "judger to filter out samples exhibiting hallucination.\n",
      "While using GPT-4 directly as the LLM hallucination judger is a straightforward approach, utilizing\n",
      "official APIs can be costly. Therefore, we first\n",
      "randomly select 5K samples from the data before\n",
      "cleaning. Then, we use GPT-4 to assess whether the\n",
      "golden tables contain additional information. The\n",
      "judgment prompt is illustrated in Figure 1, where\n",
      "the LLM is tasked with evaluating hallucination in\n",
      "a chain-of-thought manner. Next, we use the GPT4 judgment results to train an open-source LLM,\n",
      "employing the trained model to evaluate the remaining samples. Samples containing hallucinations are\n",
      "discarded. Given the capacity for understanding\n",
      "lengthy documents of existing Chinese LLMs (Bai\n",
      "et al., 2023b), we select ChatGLM3-6B-32k3\n",
      "as\n",
      "the open-source LLM hallucination judger. Finally,\n",
      "there are 88.6K samples after the data cleaning\n",
      "by the LLM hallucination judger. These samples\n",
      "totally cover 28 domains, e.g., physics and religion.\n",
      "3.3 Human Data Cleaning\n",
      "We split the LLM-cleaned samples into the training, validation, and test sets with 84.6K, 1K and\n",
      "1K samples, respectively. In the validation and test\n",
      "sets, we balance the number of samples in each\n",
      "domain to ensure a comprehensive evaluation. Furthermore, to mitigate the hallucination issue in the\n",
      "validation and test sets, we employ human annotators to manually cleanse their golden tables.\n",
      "In this phase, we employ five human annotators\n",
      "and one data expert, all of whom are native Chinese\n",
      "speakers with advanced educational qualifications.\n",
      "The data expert is a researcher with extensive experience in IE research. We first organize a tutorial\n",
      "for the five annotators to ensure alignment of annotation requirements. This includes clarifying the\n",
      "concept of the hallucination issue, emphasizing the\n",
      "additional information present in tables that cannot\n",
      "be directly extracted from the corresponding documents. Next, for each document-table pair in the\n",
      "3\n",
      "https://huggingface.co/THUDM/chatglm3-6b-32k\n",
      "CT-Eval\n",
      "Industry\n",
      "Informatics\n",
      "Physics\n",
      "Astronomy\n",
      "Traffic Engineering\n",
      "Biology\n",
      "Medical Science\n",
      "Culture\n",
      "Religion\n",
      "History\n",
      "Geography\n",
      "Education\n",
      "Business\n",
      "Finance\n",
      "Management\n",
      "Movie and Book\n",
      "Science Fiction\n",
      "Music\n",
      "Animation\n",
      "Dwelling\n",
      "Traveling\n",
      "Career\n",
      "Healthy\n",
      "Food\n",
      "Cosmetics\n",
      "Profile\n",
      "Security\n",
      "Military\n",
      "STEM\n",
      "24.0%\n",
      "Social Science\n",
      "60.9%\n",
      "Life\n",
      "Support\n",
      "11.6%\n",
      "O.\n",
      "3.5\n",
      "%\n",
      "Figure 2: Domain distribution in CT-EVAL\n",
      "validation and test sets, three annotators are asked\n",
      "to remove the hallucination information from tables. The data expert reviews 10% of the manually\n",
      "cleaned samples from each annotator. If the accuracy rate falls below 95%, the respective annotator\n",
      "will be required to redo the annotation. Finally,\n",
      "for each sample, if the three manually cleaned results are consistent, the results are saved as the final\n",
      "data. Otherwise, the results are decided by a group\n",
      "meeting among all annotators and the data expert.\n",
      "3.4 Task Overview\n",
      "Given a document D = {w1, w2,..., w|D|}, where\n",
      "wi\n",
      "is the i-th word in D, the text-to-table task aims\n",
      "to extract key information from D and outputs a table T = {c0,0, c0,1,..., c0,n, c1,0, c1,1,..., c1,n,...,\n",
      "ci,j,..., cm,n}, where c0,k(k ∈ {0, 1,..., n}) represents the column header and ck,0(k ∈ {0, 1,..., n})\n",
      "denotes as the row header. ci,j (i > 0, j > 0) represents the text of the cell in the i-th row and j-th\n",
      "column in the table with m rows and n columns.\n",
      "3.5 Data Statistics\n",
      "We compare CT-Eval with the previous datasets\n",
      "across several metrics including language, number of entries, average length, number of domains,\n",
      "hallucination rate, and average number of cells.\n",
      "As shown in Table 1, compared to previous\n",
      "datasets that mostly focus on one domain, CT-Eval\n",
      "covers 28 domains, offering a valuable resource\n",
      "to evaluate the text-to-table capabilities of LLMs\n",
      "across multiple domains. These 28 domains can\n",
      "be categorized into four branches: STEM, social\n",
      "science, life support and others. To provide a comprehensive understanding of domain coverage in\n",
      "CT-Eval, we present the distributions of each domain in Figure 2. Notably, the “Social Science”\n",
      "Original Item\n",
      "Translation\n",
      "材料 做法 厨师一点通\n",
      "炖牛肉时，可以放几个\n",
      "山楂进去，这样牛肉会\n",
      "烂得快些，而且有股山\n",
      "楂的清香。\n",
      "1.牛肉洗净放入清水锅\n",
      "中煮至七成熟，捞出切\n",
      "成方块\n",
      "2. …\n",
      "材料：\n",
      "牛肉，胡萝卜，\n",
      "白萝卜，大料，\n",
      "香叶\n",
      "东坡牛肉\n",
      "Original Item\n",
      "Ingredients Cooking method Cooking Tips\n",
      "When stewing beef, you\n",
      "can put a few hawthorn\n",
      "into it, so that the beef\n",
      "will rot faster and have a\n",
      "hawthorn fragrance.\n",
      "1. Wash the beef and\n",
      "put it into a pot of\n",
      "water to boil until\n",
      "it is seven mature,\n",
      "fish out and cut it\n",
      "into squares…\n",
      "2. …\n",
      "Ingredients:\n",
      "Beef, carrots, white\n",
      "radish, Chinese\n",
      "spices, allspice\n",
      "Dongpo\n",
      "Beef\n",
      "“标题”: “东坡牛肉”, “ ”章节“: [{”标题“: ”东坡牛肉的做法“},\n",
      "内容“: ”葱段、姜片、蒜头、辣椒、香叶、大料、盐、白糖、番茄酱、排骨\n",
      "酱、辣椒酱、高汤（用煮牛肉的汤即可）。\n",
      "做法：\n",
      "1、先来处理一下牛肉，\n",
      "将牛腩在冷水中多泡一会…“}\n",
      "Translation\n",
      "“Title”: “Dongpo Beef”, “ ”Chapter“: [{”Title “: ”Dongpo Beef Practice“}, :\n",
      "Ingredients：Diced green onion, ginger, garlic, chili pepper, coriander, dashi, salt,\n",
      "sugar, tomato sauce, rib sauce, chili sauce, broth. Cooking method:1, first, let's deal\n",
      "with the beef, soak the brisket in cold water for a while longer...\"}\n",
      "Figure 3: Text-to-table example from CT-EVAL.\n",
      "branch exhibits the highest proportion, encompassing domains such as culture and religion. Conversely, the “Other” branch, which includes topics\n",
      "related to cosmetics and profiles, represents the\n",
      "smallest portion. The “STEM” (science, technology, engineering, and mathematics) branch features\n",
      "data related to topics such as physical sciences and\n",
      "astronomy. The “Life Support” branch is highly\n",
      "relevant to everyday life, comprises 11.60% of the\n",
      "data, and primarily focuses on domains such as\n",
      "dwelling and health. An illustration of a data sample from CT-Eval is depicted in Figure 3. Each\n",
      "document-table pair, akin to the example shown,\n",
      "consists of a textual description and a golden table\n",
      "with varying numbers of columns and rows.\n",
      "To assess the quality of our dataset compared\n",
      "to previous ones, we randomly select 200 samples\n",
      "from the training, validation and test sets of CTEval, and previous E2E, Rotowire, WikiBio, and\n",
      "WikiTableText, respectively. Then, we compute the\n",
      "hallucination rate for each dataset through human\n",
      "annotation. Specifically, three annotators proficient\n",
      "in both English and Chinese judged whether the\n",
      "golden tables contained hallucination information\n",
      "following the guidance outlined in Section 3.3. The\n",
      "hallucination rate for each dataset is determined\n",
      "by the average proportion of hallucinated samples\n",
      "judged by all three annotators. We observe that\n",
      "the hallucination rate of the CT-Eval training set\n",
      "is 6.83%, similar to Rotowire (6.17%). Moreover,\n",
      "with the help of our human cleaning, the hallucination rates in our test and validation sets decrease to\n",
      "1.00% and 1.50%, respectively, significantly lower\n",
      "Dataset Language N. Entries Avg. Length N. Domains Hallu. R Avg. Cells\n",
      "E2E English 42,061 90.58 Restaurant 4.17% 4.46\n",
      "Rotowire English 3,398 1311.01 Sports 6.17% 40.49\n",
      "WikiBio English 582,659 416.71 Biography 8.67% 4.19\n",
      "WikiTableText English 10,000 59.76 Multiple Subclasses 18.83% 4.25\n",
      "CT-Eval(train) Chinese 84,603 911.46 28 Subclasses 6.83% 11.40\n",
      "CT-Eval(val.) Chinese 1,000 813.32 28 Subclasses 1.00% 10.78\n",
      "CT-Eval(test) Chinese 1,000 845.22 28 Subclasses 1.50% 10.86\n",
      "Table 1: Data Statistics of CT-Eval and previous text-to-table datasets. “N. Entries” denotes the number of entries.\n",
      "“Hallu. R” indicates hallucination rate. “Avg.Cells” indicates the average number of table cells\n",
      "than those in other datasets. Therefore, the reliability of our dataset can be verified. Regarding\n",
      "domains, the E2E, Rotowire and WikiBio datasets\n",
      "focus on single domains, whereas WikiTableText\n",
      "and CT-Eval encompass multiple domains to ensure data diversity. Concerning the length of the input documents, we note that previous E2E and WikiTableText primarily contain short documents with\n",
      "an average length of under 100 words, whereas documents in our dataset and Rotowire exceed words\n",
      "or characters, presenting more complex inputs for\n",
      "text-to-table models. In summary, CT-Eval stands\n",
      "as the sole dataset fulfilling criteria of data diversity,\n",
      "lengthy documents, and low hallucination.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is CT-Eval?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "CT-Eval is a benchmark dataset specifically designed for evaluating the performance of Large Language Models (LLMs) in the text-to-table task. The acronym \"CT-Eval\" stands for \"Chinese Text-to-Table Evaluation\".\n",
      "\n",
      "CT-Eval is a\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "1 Introduction\n",
      "Information extraction (IE) aims to identify and extract structured information from unstructured text.\n",
      "1Codes and data will be publicly available once accepted.\n",
      "Basic IE sub-tasks, such as, named entity recognition and entity linking, generally operate at the\n",
      "sentence level, which ignore models’ ability to understand the document-level meaning. In contrast,\n",
      "Text-to-Table, an emerging sub-task of IE, requires\n",
      "models to understand information within a given\n",
      "document and then generate structured tables. Despite great success in the domain Wu et al. (2022);\n",
      "Li et al. (2023), previous studies are oriented to\n",
      "English, limiting the research in other languages.\n",
      "Recently, large language models (LLMs) have\n",
      "demonstrated powerful performance across various NLP tasks (Zhao et al., 2023; Wang et al.,\n",
      "2023a,b,c). Some studies (González-Gallardo et al.,\n",
      "2023; Gao et al., 2023) have also utilized LLMs for\n",
      "several IE sub-tasks, and found that compared to\n",
      "supervised baselines, the performance of LLMs is\n",
      "sub-optimal. However, their performance of LLMs\n",
      "on Text-to-Table remains largely unexplored. Additionally, the rapid advancements in LLMs have facilitated their widespread adoption in multi-lingual\n",
      "settings, enabling language modeling abilities to be\n",
      "shared across different languages. Consequently, it\n",
      "is theoretically possible to feasible to employ LLMs\n",
      "for text-to-table in other languages, an area that\n",
      "has yet to be thoroughly investigated.\n",
      "Motivated by the aforementioned considerations,\n",
      "we decide to benchmark LLMs on Chinese text-totable. A heuristic approach involves translating existing English datasets into Chinese for subsequent\n",
      "performance evaluation. There are four datasets\n",
      "widely used in text-to-table: E2E (Novikova et al.,\n",
      "2017) is a restaurant domain dataset encompassing\n",
      "51.5K samples about restaurant information like\n",
      "names, addresses, rate scores, etc. Rotowire (Wiseman et al., 2017) is a sports domain dataset derived\n",
      "from NBA basketball games with 4.9K samples\n",
      "about NBA team information. WikiBio (Lebret\n",
      "et al., 2016), compiled from Wikipedia, containing\n",
      "72.6K documents and corresponding structured tables in the biography domain. WikiTableText (Bao\n",
      "arXiv:2405.12174v1 [cs.CL] 20 May 2024\n",
      "et al., 2018) is also derived from Wikipedia, and\n",
      "involves 13.3K multidisciplinary samples spanning\n",
      "fields like finance and politics. However, according to our preliminary analysis, these datasets also\n",
      "suffer from the issues of less diversity or high hallucination, making them unsuitable to benchmark\n",
      "LLMs. Specifically, (1) E2E, Rotowire, and WikiBio exhibit a lack of diversity as they focus on a\n",
      "single domain, violating the core principle of instruction tuning in LLMs, that is, diversity. (2)\n",
      "Although WikiTableText incorporates multiple domains for diversity, our analysis (§ 3.5) indicates\n",
      "that 18.83% of samples exhibit hallucination in\n",
      "the golden tables, that is, containing additional information beyond the provided documents. This\n",
      "arises from treating Wikipedia infoboxes as golden\n",
      "tables, created collaboratively by online users and\n",
      "potentially containing additional basic information.\n",
      "In this paper, we propose the Chinese Text-toTable Evaluation (CT-Eval) dataset, which is constructed through three steps to ensure data diversity\n",
      "and minimize hallucination. To ensure diversity,\n",
      "the first step involves collecting multidisciplinary\n",
      "document-table pairs. We choose the Baidu Baike\n",
      "as the data source, which is a popular Chinese multidisciplinary online encyclopedia. Each page in\n",
      "this source contains text and an infobox summarizing the corresponding key information. Second, to\n",
      "minimize data hallucination, we train an LLM, as a\n",
      "hallucination judger, to filter out task samples with\n",
      "hallucination in their golden tables (infoboxes). Finally, we obtain 88.6K samples with an average\n",
      "length of 911.46 Chinese characters. We split them\n",
      "into 86.6K, 1K and 1K for training, validation and\n",
      "testing. For validation and testing samples, human\n",
      "annotators further clean data hallucination in the\n",
      "golden tables to ensure evaluation reliability.\n",
      "Based on the proposed CT-Eval, we benchmark\n",
      "various mainstream LLMs in both zero-shot (for\n",
      "both open- and closed-source LLMs) and finetuning (only for open-source LLMs) scenarios. Our\n",
      "experiments reveal that (1) GPT-4 achieves the best\n",
      "zero-shot performance among all LLMs. However,\n",
      "its performance remains a discernible disparity\n",
      "compared to human judgment. (2) After fine-tuning\n",
      "on the training set of CT-Eval, all open-source\n",
      "LLMs demonstrate a significant performance improvement, outperforming zero-shot GPT-4 by a\n",
      "large margin, indicating the effectiveness of CTEval. In-depth analyses of LLM-generated tables\n",
      "reveal the persistence of hallucination issues in\n",
      "both zero-shot and fine-tuned LLMs, highlighting\n",
      "a challenge in using LLMs as text-to-table systems.\n",
      "Future work could not only evaluate LLMs’ performance on text-to-table via our CT-Eval benchmark\n",
      "dataset, but also leverage its training data to improve LLMs’ text-to-table ability via fine-tuning.\n",
      "2 Related Work\n",
      "2.1 Text-to-table Tasks\n",
      "Wu et al. (2022) pioneer the text-to-table task.\n",
      "Given the absence of a dedicated text-to-table\n",
      "dataset, Wu et al. (2022) repurpose existing tableto-text datasets, i.e., WikiBio (Lebret et al., 2016),\n",
      "E2E (Novikova et al., 2017), Rotowire (Wiseman et al., 2017) and WikiTableText (Bao et al.,\n",
      "2018), for text-to-table tasks by reversing their\n",
      "input-output pairs. They fine-tune BART (Lewis\n",
      "et al., 2019) to perform text-to-table in a sequenceto-sequence manner, and find that the fine-tuned\n",
      "BART outperforms the pipeline baselines using\n",
      "relation extraction and named entity extraction.\n",
      "STable (Pietruszka et al., 2022) employs two pretrained language models (PLMs) (T5 (Raffel et al.,\n",
      "2020) and TILT (Powalski et al., 2021)) for text-totable, and designs a permutation-based decoder to\n",
      "enhance the PLMs’ table generation ability. Subsequently, Li et al. (2023) find that the predefined row\n",
      "order in golden tables introduces bias into text-totable models. Consequently, they train table header\n",
      "and table body generators separately to produce\n",
      "final tables. While these studies achieve notable\n",
      "success, they primarily explored text-to-table performance before the LLM era. In addition, their\n",
      "evaluation datasets generally focus on a single domain, and adapt from the table-to-text datasets, resulting in hallucination issues. Thus, they are unsuitable for benchmarking LLMs in text-to-table.\n",
      "2.2 Large Language Models\n",
      "The advent of advanced LLMs,e.g., ChatGPT (OpenAI, 2022), GPT-4 (OpenAI, 2023), marks a pivotal\n",
      "moment that propels the field of NLP into a boom\n",
      "phase. Zhong et al. (2023) show that LLMs can\n",
      "achieve decent performance on benchmarks like\n",
      "GLUE (Wang et al., 2018), which spans eight representative NLP understanding tasks. Concurrently,\n",
      "several studies have scrutinized the performance\n",
      "of LLMs across various IE tasks. For example,\n",
      "Gao et al. (2023) test the capability of ChatGPT\n",
      "in event extraction. Similarly, González-Gallardo\n",
      "et al. (2023) employ ChatGPT on historical entity\n",
      "recognition. However, the results of these IE subtasks consistently show that LLMs underperform\n",
      "compared to state-of-the-art supervised approaches.\n",
      "The text-to-table task we focused on is more complex than the previous basic IE sub-tasks, however,\n",
      "it remains an unexplored area for evaluation on\n",
      "LLMs.\n",
      "3 CT-Eval\n",
      "In this section, we first discuss the data source for\n",
      "building CT-Eval (§ 3.1). Then, we give the details\n",
      "of how to control the hallucination in the preliminary collected data, including LLM hallucination\n",
      "judger (§ 3.2) and human cleaning processes (§ 3.3).\n",
      "Finally, we formulate the text-to-table task (§ 3.4)\n",
      "and provide the details of data statistics (§ 3.5).\n",
      "3.1 Data Source\n",
      "Following the success of WikiTableText (Bao et al.,\n",
      "2018), we also choose a multidisciplinary online\n",
      "encyclopedia as the data source to ensure data diversity. After carefully comparing existing Chinese\n",
      "online encyclopedias, we choose Baidu Baike2\n",
      ",\n",
      "which is one of the Chinese encyclopedias with\n",
      "the most entries in the world.\n",
      "We obtain the Baidu Baike data from the dumps\n",
      "provided by Xu et al. (2017). The data contains\n",
      "over 9M entity pages, each of which includes an\n",
      "infobox and the corresponding textual description,\n",
      "forming a text-to-table sample. Utilizing this data,\n",
      "we implement the following rule-based strategy\n",
      "for preliminary data cleaning: (1) Each page must\n",
      "contain at least one infobox; otherwise, the golden\n",
      "table is missing. (2) The number of tabular cells in\n",
      "the infobox should exceed three to ensure validity.\n",
      "(3) The length of the textual description should\n",
      "surpass 200 tokens. After that, 200K documenttable pairs are remaining for further processing.\n",
      "3.2 LLM Data Cleaning\n",
      "After the initial cleaning process, the hallucination issue persists due to Baidu Baike’s submission\n",
      "rules, wherein most of the page text and infoboxes\n",
      "are edited and maintained by individuals. Thus,\n",
      "the contents in the infoboxes may not always align\n",
      "precisely with the textual documents. For instance,\n",
      "some infoboxes may include additional knowledge\n",
      "unrelated to the text, potentially misleading the\n",
      "model from learning the text-to-table task.\n",
      "2\n",
      "https://baike.baidu.com/\n",
      "§ There is a document and a golden table that\n",
      "summarize the information contained in this\n",
      "document. Please help me identify whether the\n",
      "golden table contains additional information\n",
      "beyond the document. Give the reason first and\n",
      "then provide your judgment.\n",
      "Figure 1: Illustration of judgment prompt.\n",
      "To control the hallucination in the task samples,\n",
      "we decide to employ an LLM as a hallucination\n",
      "judger to filter out samples exhibiting hallucination.\n",
      "While using GPT-4 directly as the LLM hallucination judger is a straightforward approach, utilizing\n",
      "official APIs can be costly. Therefore, we first\n",
      "randomly select 5K samples from the data before\n",
      "cleaning. Then, we use GPT-4 to assess whether the\n",
      "golden tables contain additional information. The\n",
      "judgment prompt is illustrated in Figure 1, where\n",
      "the LLM is tasked with evaluating hallucination in\n",
      "a chain-of-thought manner. Next, we use the GPT4 judgment results to train an open-source LLM,\n",
      "employing the trained model to evaluate the remaining samples. Samples containing hallucinations are\n",
      "discarded. Given the capacity for understanding\n",
      "lengthy documents of existing Chinese LLMs (Bai\n",
      "et al., 2023b), we select ChatGLM3-6B-32k3\n",
      "as\n",
      "the open-source LLM hallucination judger. Finally,\n",
      "there are 88.6K samples after the data cleaning\n",
      "by the LLM hallucination judger. These samples\n",
      "totally cover 28 domains, e.g., physics and religion.\n",
      "3.3 Human Data Cleaning\n",
      "We split the LLM-cleaned samples into the training, validation, and test sets with 84.6K, 1K and\n",
      "1K samples, respectively. In the validation and test\n",
      "sets, we balance the number of samples in each\n",
      "domain to ensure a comprehensive evaluation. Furthermore, to mitigate the hallucination issue in the\n",
      "validation and test sets, we employ human annotators to manually cleanse their golden tables.\n",
      "In this phase, we employ five human annotators\n",
      "and one data expert, all of whom are native Chinese\n",
      "speakers with advanced educational qualifications.\n",
      "The data expert is a researcher with extensive experience in IE research. We first organize a tutorial\n",
      "for the five annotators to ensure alignment of annotation requirements. This includes clarifying the\n",
      "concept of the hallucination issue, emphasizing the\n",
      "additional information present in tables that cannot\n",
      "be directly extracted from the corresponding documents. Next, for each document-table pair in the\n",
      "3\n",
      "https://huggingface.co/THUDM/chatglm3-6b-32k\n",
      "CT-Eval\n",
      "Industry\n",
      "Informatics\n",
      "Physics\n",
      "Astronomy\n",
      "Traffic Engineering\n",
      "Biology\n",
      "Medical Science\n",
      "Culture\n",
      "Religion\n",
      "History\n",
      "Geography\n",
      "Education\n",
      "Business\n",
      "Finance\n",
      "Management\n",
      "Movie and Book\n",
      "Science Fiction\n",
      "Music\n",
      "Animation\n",
      "Dwelling\n",
      "Traveling\n",
      "Career\n",
      "Healthy\n",
      "Food\n",
      "Cosmetics\n",
      "Profile\n",
      "Security\n",
      "Military\n",
      "STEM\n",
      "24.0%\n",
      "Social Science\n",
      "60.9%\n",
      "Life\n",
      "Support\n",
      "11.6%\n",
      "O.\n",
      "3.5\n",
      "%\n",
      "Figure 2: Domain distribution in CT-EVAL\n",
      "validation and test sets, three annotators are asked\n",
      "to remove the hallucination information from tables. The data expert reviews 10% of the manually\n",
      "cleaned samples from each annotator. If the accuracy rate falls below 95%, the respective annotator\n",
      "will be required to redo the annotation. Finally,\n",
      "for each sample, if the three manually cleaned results are consistent, the results are saved as the final\n",
      "data. Otherwise, the results are decided by a group\n",
      "meeting among all annotators and the data expert.\n",
      "3.4 Task Overview\n",
      "Given a document D = {w1, w2,..., w|D|}, where\n",
      "wi\n",
      "is the i-th word in D, the text-to-table task aims\n",
      "to extract key information from D and outputs a table T = {c0,0, c0,1,..., c0,n, c1,0, c1,1,..., c1,n,...,\n",
      "ci,j,..., cm,n}, where c0,k(k ∈ {0, 1,..., n}) represents the column header and ck,0(k ∈ {0, 1,..., n})\n",
      "denotes as the row header. ci,j (i > 0, j > 0) represents the text of the cell in the i-th row and j-th\n",
      "column in the table with m rows and n columns.\n",
      "3.5 Data Statistics\n",
      "We compare CT-Eval with the previous datasets\n",
      "across several metrics including language, number of entries, average length, number of domains,\n",
      "hallucination rate, and average number of cells.\n",
      "As shown in Table 1, compared to previous\n",
      "datasets that mostly focus on one domain, CT-Eval\n",
      "covers 28 domains, offering a valuable resource\n",
      "to evaluate the text-to-table capabilities of LLMs\n",
      "across multiple domains. These 28 domains can\n",
      "be categorized into four branches: STEM, social\n",
      "science, life support and others. To provide a comprehensive understanding of domain coverage in\n",
      "CT-Eval, we present the distributions of each domain in Figure 2. Notably, the “Social Science”\n",
      "Original Item\n",
      "Translation\n",
      "材料 做法 厨师一点通\n",
      "炖牛肉时，可以放几个\n",
      "山楂进去，这样牛肉会\n",
      "烂得快些，而且有股山\n",
      "楂的清香。\n",
      "1.牛肉洗净放入清水锅\n",
      "中煮至七成熟，捞出切\n",
      "成方块\n",
      "2. …\n",
      "材料：\n",
      "牛肉，胡萝卜，\n",
      "白萝卜，大料，\n",
      "香叶\n",
      "东坡牛肉\n",
      "Original Item\n",
      "Ingredients Cooking method Cooking Tips\n",
      "When stewing beef, you\n",
      "can put a few hawthorn\n",
      "into it, so that the beef\n",
      "will rot faster and have a\n",
      "hawthorn fragrance.\n",
      "1. Wash the beef and\n",
      "put it into a pot of\n",
      "water to boil until\n",
      "it is seven mature,\n",
      "fish out and cut it\n",
      "into squares…\n",
      "2. …\n",
      "Ingredients:\n",
      "Beef, carrots, white\n",
      "radish, Chinese\n",
      "spices, allspice\n",
      "Dongpo\n",
      "Beef\n",
      "“标题”: “东坡牛肉”, “ ”章节“: [{”标题“: ”东坡牛肉的做法“},\n",
      "内容“: ”葱段、姜片、蒜头、辣椒、香叶、大料、盐、白糖、番茄酱、排骨\n",
      "酱、辣椒酱、高汤（用煮牛肉的汤即可）。\n",
      "做法：\n",
      "1、先来处理一下牛肉，\n",
      "将牛腩在冷水中多泡一会…“}\n",
      "Translation\n",
      "“Title”: “Dongpo Beef”, “ ”Chapter“: [{”Title “: ”Dongpo Beef Practice“}, :\n",
      "Ingredients：Diced green onion, ginger, garlic, chili pepper, coriander, dashi, salt,\n",
      "sugar, tomato sauce, rib sauce, chili sauce, broth. Cooking method:1, first, let's deal\n",
      "with the beef, soak the brisket in cold water for a while longer...\"}\n",
      "Figure 3: Text-to-table example from CT-EVAL.\n",
      "branch exhibits the highest proportion, encompassing domains such as culture and religion. Conversely, the “Other” branch, which includes topics\n",
      "related to cosmetics and profiles, represents the\n",
      "smallest portion. The “STEM” (science, technology, engineering, and mathematics) branch features\n",
      "data related to topics such as physical sciences and\n",
      "astronomy. The “Life Support” branch is highly\n",
      "relevant to everyday life, comprises 11.60% of the\n",
      "data, and primarily focuses on domains such as\n",
      "dwelling and health. An illustration of a data sample from CT-Eval is depicted in Figure 3. Each\n",
      "document-table pair, akin to the example shown,\n",
      "consists of a textual description and a golden table\n",
      "with varying numbers of columns and rows.\n",
      "To assess the quality of our dataset compared\n",
      "to previous ones, we randomly select 200 samples\n",
      "from the training, validation and test sets of CTEval, and previous E2E, Rotowire, WikiBio, and\n",
      "WikiTableText, respectively. Then, we compute the\n",
      "hallucination rate for each dataset through human\n",
      "annotation. Specifically, three annotators proficient\n",
      "in both English and Chinese judged whether the\n",
      "golden tables contained hallucination information\n",
      "following the guidance outlined in Section 3.3. The\n",
      "hallucination rate for each dataset is determined\n",
      "by the average proportion of hallucinated samples\n",
      "judged by all three annotators. We observe that\n",
      "the hallucination rate of the CT-Eval training set\n",
      "is 6.83%, similar to Rotowire (6.17%). Moreover,\n",
      "with the help of our human cleaning, the hallucination rates in our test and validation sets decrease to\n",
      "1.00% and 1.50%, respectively, significantly lower\n",
      "Dataset Language N. Entries Avg. Length N. Domains Hallu. R Avg. Cells\n",
      "E2E English 42,061 90.58 Restaurant 4.17% 4.46\n",
      "Rotowire English 3,398 1311.01 Sports 6.17% 40.49\n",
      "WikiBio English 582,659 416.71 Biography 8.67% 4.19\n",
      "WikiTableText English 10,000 59.76 Multiple Subclasses 18.83% 4.25\n",
      "CT-Eval(train) Chinese 84,603 911.46 28 Subclasses 6.83% 11.40\n",
      "CT-Eval(val.) Chinese 1,000 813.32 28 Subclasses 1.00% 10.78\n",
      "CT-Eval(test) Chinese 1,000 845.22 28 Subclasses 1.50% 10.86\n",
      "Table 1: Data Statistics of CT-Eval and previous text-to-table datasets. “N. Entries” denotes the number of entries.\n",
      "“Hallu. R” indicates hallucination rate. “Avg.Cells” indicates the average number of table cells\n",
      "than those in other datasets. Therefore, the reliability of our dataset can be verified. Regarding\n",
      "domains, the E2E, Rotowire and WikiBio datasets\n",
      "focus on single domains, whereas WikiTableText\n",
      "and CT-Eval encompass multiple domains to ensure data diversity. Concerning the length of the input documents, we note that previous E2E and WikiTableText primarily contain short documents with\n",
      "an average length of under 100 words, whereas documents in our dataset and Rotowire exceed words\n",
      "or characters, presenting more complex inputs for\n",
      "text-to-table models. In summary, CT-Eval stands\n",
      "as the sole dataset fulfilling criteria of data diversity,\n",
      "lengthy documents, and low hallucination.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is CT-Eval?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "CT-Eval is a dataset for evaluating the performance of text-to-table models in Chinese, specifically designed to address the limitations of existing text-to-table datasets. It is a benchmark dataset for evaluating the ability of large language models (LLMs) to generate\n",
      "\n",
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "1 Introduction\n",
      "Information extraction (IE) aims to identify and extract structured information from unstructured text.\n",
      "1Codes and data will be publicly available once accepted.\n",
      "Basic IE sub-tasks, such as, named entity recognition and entity linking, generally operate at the\n",
      "sentence level, which ignore models’ ability to understand the document-level meaning. In contrast,\n",
      "Text-to-Table, an emerging sub-task of IE, requires\n",
      "models to understand information within a given\n",
      "document and then generate structured tables. Despite great success in the domain Wu et al. (2022);\n",
      "Li et al. (2023), previous studies are oriented to\n",
      "English, limiting the research in other languages.\n",
      "Recently, large language models (LLMs) have\n",
      "demonstrated powerful performance across various NLP tasks (Zhao et al., 2023; Wang et al.,\n",
      "2023a,b,c). Some studies (González-Gallardo et al.,\n",
      "2023; Gao et al., 2023) have also utilized LLMs for\n",
      "several IE sub-tasks, and found that compared to\n",
      "supervised baselines, the performance of LLMs is\n",
      "sub-optimal. However, their performance of LLMs\n",
      "on Text-to-Table remains largely unexplored. Additionally, the rapid advancements in LLMs have facilitated their widespread adoption in multi-lingual\n",
      "settings, enabling language modeling abilities to be\n",
      "shared across different languages. Consequently, it\n",
      "is theoretically possible to feasible to employ LLMs\n",
      "for text-to-table in other languages, an area that\n",
      "has yet to be thoroughly investigated.\n",
      "Motivated by the aforementioned considerations,\n",
      "we decide to benchmark LLMs on Chinese text-totable. A heuristic approach involves translating existing English datasets into Chinese for subsequent\n",
      "performance evaluation. There are four datasets\n",
      "widely used in text-to-table: E2E (Novikova et al.,\n",
      "2017) is a restaurant domain dataset encompassing\n",
      "51.5K samples about restaurant information like\n",
      "names, addresses, rate scores, etc. Rotowire (Wiseman et al., 2017) is a sports domain dataset derived\n",
      "from NBA basketball games with 4.9K samples\n",
      "about NBA team information. WikiBio (Lebret\n",
      "et al., 2016), compiled from Wikipedia, containing\n",
      "72.6K documents and corresponding structured tables in the biography domain. WikiTableText (Bao\n",
      "arXiv:2405.12174v1 [cs.CL] 20 May 2024\n",
      "et al., 2018) is also derived from Wikipedia, and\n",
      "involves 13.3K multidisciplinary samples spanning\n",
      "fields like finance and politics. However, according to our preliminary analysis, these datasets also\n",
      "suffer from the issues of less diversity or high hallucination, making them unsuitable to benchmark\n",
      "LLMs. Specifically, (1) E2E, Rotowire, and WikiBio exhibit a lack of diversity as they focus on a\n",
      "single domain, violating the core principle of instruction tuning in LLMs, that is, diversity. (2)\n",
      "Although WikiTableText incorporates multiple domains for diversity, our analysis (§ 3.5) indicates\n",
      "that 18.83% of samples exhibit hallucination in\n",
      "the golden tables, that is, containing additional information beyond the provided documents. This\n",
      "arises from treating Wikipedia infoboxes as golden\n",
      "tables, created collaboratively by online users and\n",
      "potentially containing additional basic information.\n",
      "In this paper, we propose the Chinese Text-toTable Evaluation (CT-Eval) dataset, which is constructed through three steps to ensure data diversity\n",
      "and minimize hallucination. To ensure diversity,\n",
      "the first step involves collecting multidisciplinary\n",
      "document-table pairs. We choose the Baidu Baike\n",
      "as the data source, which is a popular Chinese multidisciplinary online encyclopedia. Each page in\n",
      "this source contains text and an infobox summarizing the corresponding key information. Second, to\n",
      "minimize data hallucination, we train an LLM, as a\n",
      "hallucination judger, to filter out task samples with\n",
      "hallucination in their golden tables (infoboxes). Finally, we obtain 88.6K samples with an average\n",
      "length of 911.46 Chinese characters. We split them\n",
      "into 86.6K, 1K and 1K for training, validation and\n",
      "testing. For validation and testing samples, human\n",
      "annotators further clean data hallucination in the\n",
      "golden tables to ensure evaluation reliability.\n",
      "Based on the proposed CT-Eval, we benchmark\n",
      "various mainstream LLMs in both zero-shot (for\n",
      "both open- and closed-source LLMs) and finetuning (only for open-source LLMs) scenarios. Our\n",
      "experiments reveal that (1) GPT-4 achieves the best\n",
      "zero-shot performance among all LLMs. However,\n",
      "its performance remains a discernible disparity\n",
      "compared to human judgment. (2) After fine-tuning\n",
      "on the training set of CT-Eval, all open-source\n",
      "LLMs demonstrate a significant performance improvement, outperforming zero-shot GPT-4 by a\n",
      "large margin, indicating the effectiveness of CTEval. In-depth analyses of LLM-generated tables\n",
      "reveal the persistence of hallucination issues in\n",
      "both zero-shot and fine-tuned LLMs, highlighting\n",
      "a challenge in using LLMs as text-to-table systems.\n",
      "Future work could not only evaluate LLMs’ performance on text-to-table via our CT-Eval benchmark\n",
      "dataset, but also leverage its training data to improve LLMs’ text-to-table ability via fine-tuning.\n",
      "2 Related Work\n",
      "2.1 Text-to-table Tasks\n",
      "Wu et al. (2022) pioneer the text-to-table task.\n",
      "Given the absence of a dedicated text-to-table\n",
      "dataset, Wu et al. (2022) repurpose existing tableto-text datasets, i.e., WikiBio (Lebret et al., 2016),\n",
      "E2E (Novikova et al., 2017), Rotowire (Wiseman et al., 2017) and WikiTableText (Bao et al.,\n",
      "2018), for text-to-table tasks by reversing their\n",
      "input-output pairs. They fine-tune BART (Lewis\n",
      "et al., 2019) to perform text-to-table in a sequenceto-sequence manner, and find that the fine-tuned\n",
      "BART outperforms the pipeline baselines using\n",
      "relation extraction and named entity extraction.\n",
      "STable (Pietruszka et al., 2022) employs two pretrained language models (PLMs) (T5 (Raffel et al.,\n",
      "2020) and TILT (Powalski et al., 2021)) for text-totable, and designs a permutation-based decoder to\n",
      "enhance the PLMs’ table generation ability. Subsequently, Li et al. (2023) find that the predefined row\n",
      "order in golden tables introduces bias into text-totable models. Consequently, they train table header\n",
      "and table body generators separately to produce\n",
      "final tables. While these studies achieve notable\n",
      "success, they primarily explored text-to-table performance before the LLM era. In addition, their\n",
      "evaluation datasets generally focus on a single domain, and adapt from the table-to-text datasets, resulting in hallucination issues. Thus, they are unsuitable for benchmarking LLMs in text-to-table.\n",
      "2.2 Large Language Models\n",
      "The advent of advanced LLMs,e.g., ChatGPT (OpenAI, 2022), GPT-4 (OpenAI, 2023), marks a pivotal\n",
      "moment that propels the field of NLP into a boom\n",
      "phase. Zhong et al. (2023) show that LLMs can\n",
      "achieve decent performance on benchmarks like\n",
      "GLUE (Wang et al., 2018), which spans eight representative NLP understanding tasks. Concurrently,\n",
      "several studies have scrutinized the performance\n",
      "of LLMs across various IE tasks. For example,\n",
      "Gao et al. (2023) test the capability of ChatGPT\n",
      "in event extraction. Similarly, González-Gallardo\n",
      "et al. (2023) employ ChatGPT on historical entity\n",
      "recognition. However, the results of these IE subtasks consistently show that LLMs underperform\n",
      "compared to state-of-the-art supervised approaches.\n",
      "The text-to-table task we focused on is more complex than the previous basic IE sub-tasks, however,\n",
      "it remains an unexplored area for evaluation on\n",
      "LLMs.\n",
      "3 CT-Eval\n",
      "In this section, we first discuss the data source for\n",
      "building CT-Eval (§ 3.1). Then, we give the details\n",
      "of how to control the hallucination in the preliminary collected data, including LLM hallucination\n",
      "judger (§ 3.2) and human cleaning processes (§ 3.3).\n",
      "Finally, we formulate the text-to-table task (§ 3.4)\n",
      "and provide the details of data statistics (§ 3.5).\n",
      "3.1 Data Source\n",
      "Following the success of WikiTableText (Bao et al.,\n",
      "2018), we also choose a multidisciplinary online\n",
      "encyclopedia as the data source to ensure data diversity. After carefully comparing existing Chinese\n",
      "online encyclopedias, we choose Baidu Baike2\n",
      ",\n",
      "which is one of the Chinese encyclopedias with\n",
      "the most entries in the world.\n",
      "We obtain the Baidu Baike data from the dumps\n",
      "provided by Xu et al. (2017). The data contains\n",
      "over 9M entity pages, each of which includes an\n",
      "infobox and the corresponding textual description,\n",
      "forming a text-to-table sample. Utilizing this data,\n",
      "we implement the following rule-based strategy\n",
      "for preliminary data cleaning: (1) Each page must\n",
      "contain at least one infobox; otherwise, the golden\n",
      "table is missing. (2) The number of tabular cells in\n",
      "the infobox should exceed three to ensure validity.\n",
      "(3) The length of the textual description should\n",
      "surpass 200 tokens. After that, 200K documenttable pairs are remaining for further processing.\n",
      "3.2 LLM Data Cleaning\n",
      "After the initial cleaning process, the hallucination issue persists due to Baidu Baike’s submission\n",
      "rules, wherein most of the page text and infoboxes\n",
      "are edited and maintained by individuals. Thus,\n",
      "the contents in the infoboxes may not always align\n",
      "precisely with the textual documents. For instance,\n",
      "some infoboxes may include additional knowledge\n",
      "unrelated to the text, potentially misleading the\n",
      "model from learning the text-to-table task.\n",
      "2\n",
      "https://baike.baidu.com/\n",
      "§ There is a document and a golden table that\n",
      "summarize the information contained in this\n",
      "document. Please help me identify whether the\n",
      "golden table contains additional information\n",
      "beyond the document. Give the reason first and\n",
      "then provide your judgment.\n",
      "Figure 1: Illustration of judgment prompt.\n",
      "To control the hallucination in the task samples,\n",
      "we decide to employ an LLM as a hallucination\n",
      "judger to filter out samples exhibiting hallucination.\n",
      "While using GPT-4 directly as the LLM hallucination judger is a straightforward approach, utilizing\n",
      "official APIs can be costly. Therefore, we first\n",
      "randomly select 5K samples from the data before\n",
      "cleaning. Then, we use GPT-4 to assess whether the\n",
      "golden tables contain additional information. The\n",
      "judgment prompt is illustrated in Figure 1, where\n",
      "the LLM is tasked with evaluating hallucination in\n",
      "a chain-of-thought manner. Next, we use the GPT4 judgment results to train an open-source LLM,\n",
      "employing the trained model to evaluate the remaining samples. Samples containing hallucinations are\n",
      "discarded. Given the capacity for understanding\n",
      "lengthy documents of existing Chinese LLMs (Bai\n",
      "et al., 2023b), we select ChatGLM3-6B-32k3\n",
      "as\n",
      "the open-source LLM hallucination judger. Finally,\n",
      "there are 88.6K samples after the data cleaning\n",
      "by the LLM hallucination judger. These samples\n",
      "totally cover 28 domains, e.g., physics and religion.\n",
      "3.3 Human Data Cleaning\n",
      "We split the LLM-cleaned samples into the training, validation, and test sets with 84.6K, 1K and\n",
      "1K samples, respectively. In the validation and test\n",
      "sets, we balance the number of samples in each\n",
      "domain to ensure a comprehensive evaluation. Furthermore, to mitigate the hallucination issue in the\n",
      "validation and test sets, we employ human annotators to manually cleanse their golden tables.\n",
      "In this phase, we employ five human annotators\n",
      "and one data expert, all of whom are native Chinese\n",
      "speakers with advanced educational qualifications.\n",
      "The data expert is a researcher with extensive experience in IE research. We first organize a tutorial\n",
      "for the five annotators to ensure alignment of annotation requirements. This includes clarifying the\n",
      "concept of the hallucination issue, emphasizing the\n",
      "additional information present in tables that cannot\n",
      "be directly extracted from the corresponding documents. Next, for each document-table pair in the\n",
      "3\n",
      "https://huggingface.co/THUDM/chatglm3-6b-32k\n",
      "CT-Eval\n",
      "Industry\n",
      "Informatics\n",
      "Physics\n",
      "Astronomy\n",
      "Traffic Engineering\n",
      "Biology\n",
      "Medical Science\n",
      "Culture\n",
      "Religion\n",
      "History\n",
      "Geography\n",
      "Education\n",
      "Business\n",
      "Finance\n",
      "Management\n",
      "Movie and Book\n",
      "Science Fiction\n",
      "Music\n",
      "Animation\n",
      "Dwelling\n",
      "Traveling\n",
      "Career\n",
      "Healthy\n",
      "Food\n",
      "Cosmetics\n",
      "Profile\n",
      "Security\n",
      "Military\n",
      "STEM\n",
      "24.0%\n",
      "Social Science\n",
      "60.9%\n",
      "Life\n",
      "Support\n",
      "11.6%\n",
      "O.\n",
      "3.5\n",
      "%\n",
      "Figure 2: Domain distribution in CT-EVAL\n",
      "validation and test sets, three annotators are asked\n",
      "to remove the hallucination information from tables. The data expert reviews 10% of the manually\n",
      "cleaned samples from each annotator. If the accuracy rate falls below 95%, the respective annotator\n",
      "will be required to redo the annotation. Finally,\n",
      "for each sample, if the three manually cleaned results are consistent, the results are saved as the final\n",
      "data. Otherwise, the results are decided by a group\n",
      "meeting among all annotators and the data expert.\n",
      "3.4 Task Overview\n",
      "Given a document D = {w1, w2,..., w|D|}, where\n",
      "wi\n",
      "is the i-th word in D, the text-to-table task aims\n",
      "to extract key information from D and outputs a table T = {c0,0, c0,1,..., c0,n, c1,0, c1,1,..., c1,n,...,\n",
      "ci,j,..., cm,n}, where c0,k(k ∈ {0, 1,..., n}) represents the column header and ck,0(k ∈ {0, 1,..., n})\n",
      "denotes as the row header. ci,j (i > 0, j > 0) represents the text of the cell in the i-th row and j-th\n",
      "column in the table with m rows and n columns.\n",
      "3.5 Data Statistics\n",
      "We compare CT-Eval with the previous datasets\n",
      "across several metrics including language, number of entries, average length, number of domains,\n",
      "hallucination rate, and average number of cells.\n",
      "As shown in Table 1, compared to previous\n",
      "datasets that mostly focus on one domain, CT-Eval\n",
      "covers 28 domains, offering a valuable resource\n",
      "to evaluate the text-to-table capabilities of LLMs\n",
      "across multiple domains. These 28 domains can\n",
      "be categorized into four branches: STEM, social\n",
      "science, life support and others. To provide a comprehensive understanding of domain coverage in\n",
      "CT-Eval, we present the distributions of each domain in Figure 2. Notably, the “Social Science”\n",
      "Original Item\n",
      "Translation\n",
      "材料 做法 厨师一点通\n",
      "炖牛肉时，可以放几个\n",
      "山楂进去，这样牛肉会\n",
      "烂得快些，而且有股山\n",
      "楂的清香。\n",
      "1.牛肉洗净放入清水锅\n",
      "中煮至七成熟，捞出切\n",
      "成方块\n",
      "2. …\n",
      "材料：\n",
      "牛肉，胡萝卜，\n",
      "白萝卜，大料，\n",
      "香叶\n",
      "东坡牛肉\n",
      "Original Item\n",
      "Ingredients Cooking method Cooking Tips\n",
      "When stewing beef, you\n",
      "can put a few hawthorn\n",
      "into it, so that the beef\n",
      "will rot faster and have a\n",
      "hawthorn fragrance.\n",
      "1. Wash the beef and\n",
      "put it into a pot of\n",
      "water to boil until\n",
      "it is seven mature,\n",
      "fish out and cut it\n",
      "into squares…\n",
      "2. …\n",
      "Ingredients:\n",
      "Beef, carrots, white\n",
      "radish, Chinese\n",
      "spices, allspice\n",
      "Dongpo\n",
      "Beef\n",
      "“标题”: “东坡牛肉”, “ ”章节“: [{”标题“: ”东坡牛肉的做法“},\n",
      "内容“: ”葱段、姜片、蒜头、辣椒、香叶、大料、盐、白糖、番茄酱、排骨\n",
      "酱、辣椒酱、高汤（用煮牛肉的汤即可）。\n",
      "做法：\n",
      "1、先来处理一下牛肉，\n",
      "将牛腩在冷水中多泡一会…“}\n",
      "Translation\n",
      "“Title”: “Dongpo Beef”, “ ”Chapter“: [{”Title “: ”Dongpo Beef Practice“}, :\n",
      "Ingredients：Diced green onion, ginger, garlic, chili pepper, coriander, dashi, salt,\n",
      "sugar, tomato sauce, rib sauce, chili sauce, broth. Cooking method:1, first, let's deal\n",
      "with the beef, soak the brisket in cold water for a while longer...\"}\n",
      "Figure 3: Text-to-table example from CT-EVAL.\n",
      "branch exhibits the highest proportion, encompassing domains such as culture and religion. Conversely, the “Other” branch, which includes topics\n",
      "related to cosmetics and profiles, represents the\n",
      "smallest portion. The “STEM” (science, technology, engineering, and mathematics) branch features\n",
      "data related to topics such as physical sciences and\n",
      "astronomy. The “Life Support” branch is highly\n",
      "relevant to everyday life, comprises 11.60% of the\n",
      "data, and primarily focuses on domains such as\n",
      "dwelling and health. An illustration of a data sample from CT-Eval is depicted in Figure 3. Each\n",
      "document-table pair, akin to the example shown,\n",
      "consists of a textual description and a golden table\n",
      "with varying numbers of columns and rows.\n",
      "To assess the quality of our dataset compared\n",
      "to previous ones, we randomly select 200 samples\n",
      "from the training, validation and test sets of CTEval, and previous E2E, Rotowire, WikiBio, and\n",
      "WikiTableText, respectively. Then, we compute the\n",
      "hallucination rate for each dataset through human\n",
      "annotation. Specifically, three annotators proficient\n",
      "in both English and Chinese judged whether the\n",
      "golden tables contained hallucination information\n",
      "following the guidance outlined in Section 3.3. The\n",
      "hallucination rate for each dataset is determined\n",
      "by the average proportion of hallucinated samples\n",
      "judged by all three annotators. We observe that\n",
      "the hallucination rate of the CT-Eval training set\n",
      "is 6.83%, similar to Rotowire (6.17%). Moreover,\n",
      "with the help of our human cleaning, the hallucination rates in our test and validation sets decrease to\n",
      "1.00% and 1.50%, respectively, significantly lower\n",
      "Dataset Language N. Entries Avg. Length N. Domains Hallu. R Avg. Cells\n",
      "E2E English 42,061 90.58 Restaurant 4.17% 4.46\n",
      "Rotowire English 3,398 1311.01 Sports 6.17% 40.49\n",
      "WikiBio English 582,659 416.71 Biography 8.67% 4.19\n",
      "WikiTableText English 10,000 59.76 Multiple Subclasses 18.83% 4.25\n",
      "CT-Eval(train) Chinese 84,603 911.46 28 Subclasses 6.83% 11.40\n",
      "CT-Eval(val.) Chinese 1,000 813.32 28 Subclasses 1.00% 10.78\n",
      "CT-Eval(test) Chinese 1,000 845.22 28 Subclasses 1.50% 10.86\n",
      "Table 1: Data Statistics of CT-Eval and previous text-to-table datasets. “N. Entries” denotes the number of entries.\n",
      "“Hallu. R” indicates hallucination rate. “Avg.Cells” indicates the average number of table cells\n",
      "than those in other datasets. Therefore, the reliability of our dataset can be verified. Regarding\n",
      "domains, the E2E, Rotowire and WikiBio datasets\n",
      "focus on single domains, whereas WikiTableText\n",
      "and CT-Eval encompass multiple domains to ensure data diversity. Concerning the length of the input documents, we note that previous E2E and WikiTableText primarily contain short documents with\n",
      "an average length of under 100 words, whereas documents in our dataset and Rotowire exceed words\n",
      "or characters, presenting more complex inputs for\n",
      "text-to-table models. In summary, CT-Eval stands\n",
      "as the sole dataset fulfilling criteria of data diversity,\n",
      "lengthy documents, and low hallucination.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is CT-Eval?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "CT-Eval is a Chinese text-to-table evaluation dataset, which is a collection of document-table pairs in Chinese. The dataset is designed to evaluate the performance of Large Language Models (LLMs) on the text-to-table task, which involves identifying and\n",
      "\n",
      "CPU times: user 4min 4s, sys: 1min 24s, total: 5min 28s\n",
      "Wall time: 5min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(10):\n",
    "    outputs = model.generate(\n",
    "        **input_ids,\n",
    "        max_new_tokens=50,\n",
    "        temperature=1.0,\n",
    "    )\n",
    "    print(tokenizer.decode(outputs[0]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4be36b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T21:20:53.683779Z",
     "iopub.status.busy": "2024-05-21T21:20:53.683212Z",
     "iopub.status.idle": "2024-05-21T21:21:30.116954Z",
     "shell.execute_reply": "2024-05-21T21:21:30.115967Z"
    },
    "papermill": {
     "duration": 36.489646,
     "end_time": "2024-05-21T21:21:30.146350",
     "exception": false,
     "start_time": "2024-05-21T21:20:53.656704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Usage Before Inference:\n",
      "[2883, 2949] MiB\n",
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "1 Introduction\n",
      "Information extraction (IE) aims to identify and extract structured information from unstructured text.\n",
      "1Codes and data will be publicly available once accepted.\n",
      "Basic IE sub-tasks, such as, named entity recognition and entity linking, generally operate at the\n",
      "sentence level, which ignore models’ ability to understand the document-level meaning. In contrast,\n",
      "Text-to-Table, an emerging sub-task of IE, requires\n",
      "models to understand information within a given\n",
      "document and then generate structured tables. Despite great success in the domain Wu et al. (2022);\n",
      "Li et al. (2023), previous studies are oriented to\n",
      "English, limiting the research in other languages.\n",
      "Recently, large language models (LLMs) have\n",
      "demonstrated powerful performance across various NLP tasks (Zhao et al., 2023; Wang et al.,\n",
      "2023a,b,c). Some studies (González-Gallardo et al.,\n",
      "2023; Gao et al., 2023) have also utilized LLMs for\n",
      "several IE sub-tasks, and found that compared to\n",
      "supervised baselines, the performance of LLMs is\n",
      "sub-optimal. However, their performance of LLMs\n",
      "on Text-to-Table remains largely unexplored. Additionally, the rapid advancements in LLMs have facilitated their widespread adoption in multi-lingual\n",
      "settings, enabling language modeling abilities to be\n",
      "shared across different languages. Consequently, it\n",
      "is theoretically possible to feasible to employ LLMs\n",
      "for text-to-table in other languages, an area that\n",
      "has yet to be thoroughly investigated.\n",
      "Motivated by the aforementioned considerations,\n",
      "we decide to benchmark LLMs on Chinese text-totable. A heuristic approach involves translating existing English datasets into Chinese for subsequent\n",
      "performance evaluation. There are four datasets\n",
      "widely used in text-to-table: E2E (Novikova et al.,\n",
      "2017) is a restaurant domain dataset encompassing\n",
      "51.5K samples about restaurant information like\n",
      "names, addresses, rate scores, etc. Rotowire (Wiseman et al., 2017) is a sports domain dataset derived\n",
      "from NBA basketball games with 4.9K samples\n",
      "about NBA team information. WikiBio (Lebret\n",
      "et al., 2016), compiled from Wikipedia, containing\n",
      "72.6K documents and corresponding structured tables in the biography domain. WikiTableText (Bao\n",
      "arXiv:2405.12174v1 [cs.CL] 20 May 2024\n",
      "et al., 2018) is also derived from Wikipedia, and\n",
      "involves 13.3K multidisciplinary samples spanning\n",
      "fields like finance and politics. However, according to our preliminary analysis, these datasets also\n",
      "suffer from the issues of less diversity or high hallucination, making them unsuitable to benchmark\n",
      "LLMs. Specifically, (1) E2E, Rotowire, and WikiBio exhibit a lack of diversity as they focus on a\n",
      "single domain, violating the core principle of instruction tuning in LLMs, that is, diversity. (2)\n",
      "Although WikiTableText incorporates multiple domains for diversity, our analysis (§ 3.5) indicates\n",
      "that 18.83% of samples exhibit hallucination in\n",
      "the golden tables, that is, containing additional information beyond the provided documents. This\n",
      "arises from treating Wikipedia infoboxes as golden\n",
      "tables, created collaboratively by online users and\n",
      "potentially containing additional basic information.\n",
      "In this paper, we propose the Chinese Text-toTable Evaluation (CT-Eval) dataset, which is constructed through three steps to ensure data diversity\n",
      "and minimize hallucination. To ensure diversity,\n",
      "the first step involves collecting multidisciplinary\n",
      "document-table pairs. We choose the Baidu Baike\n",
      "as the data source, which is a popular Chinese multidisciplinary online encyclopedia. Each page in\n",
      "this source contains text and an infobox summarizing the corresponding key information. Second, to\n",
      "minimize data hallucination, we train an LLM, as a\n",
      "hallucination judger, to filter out task samples with\n",
      "hallucination in their golden tables (infoboxes). Finally, we obtain 88.6K samples with an average\n",
      "length of 911.46 Chinese characters. We split them\n",
      "into 86.6K, 1K and 1K for training, validation and\n",
      "testing. For validation and testing samples, human\n",
      "annotators further clean data hallucination in the\n",
      "golden tables to ensure evaluation reliability.\n",
      "Based on the proposed CT-Eval, we benchmark\n",
      "various mainstream LLMs in both zero-shot (for\n",
      "both open- and closed-source LLMs) and finetuning (only for open-source LLMs) scenarios. Our\n",
      "experiments reveal that (1) GPT-4 achieves the best\n",
      "zero-shot performance among all LLMs. However,\n",
      "its performance remains a discernible disparity\n",
      "compared to human judgment. (2) After fine-tuning\n",
      "on the training set of CT-Eval, all open-source\n",
      "LLMs demonstrate a significant performance improvement, outperforming zero-shot GPT-4 by a\n",
      "large margin, indicating the effectiveness of CTEval. In-depth analyses of LLM-generated tables\n",
      "reveal the persistence of hallucination issues in\n",
      "both zero-shot and fine-tuned LLMs, highlighting\n",
      "a challenge in using LLMs as text-to-table systems.\n",
      "Future work could not only evaluate LLMs’ performance on text-to-table via our CT-Eval benchmark\n",
      "dataset, but also leverage its training data to improve LLMs’ text-to-table ability via fine-tuning.\n",
      "2 Related Work\n",
      "2.1 Text-to-table Tasks\n",
      "Wu et al. (2022) pioneer the text-to-table task.\n",
      "Given the absence of a dedicated text-to-table\n",
      "dataset, Wu et al. (2022) repurpose existing tableto-text datasets, i.e., WikiBio (Lebret et al., 2016),\n",
      "E2E (Novikova et al., 2017), Rotowire (Wiseman et al., 2017) and WikiTableText (Bao et al.,\n",
      "2018), for text-to-table tasks by reversing their\n",
      "input-output pairs. They fine-tune BART (Lewis\n",
      "et al., 2019) to perform text-to-table in a sequenceto-sequence manner, and find that the fine-tuned\n",
      "BART outperforms the pipeline baselines using\n",
      "relation extraction and named entity extraction.\n",
      "STable (Pietruszka et al., 2022) employs two pretrained language models (PLMs) (T5 (Raffel et al.,\n",
      "2020) and TILT (Powalski et al., 2021)) for text-totable, and designs a permutation-based decoder to\n",
      "enhance the PLMs’ table generation ability. Subsequently, Li et al. (2023) find that the predefined row\n",
      "order in golden tables introduces bias into text-totable models. Consequently, they train table header\n",
      "and table body generators separately to produce\n",
      "final tables. While these studies achieve notable\n",
      "success, they primarily explored text-to-table performance before the LLM era. In addition, their\n",
      "evaluation datasets generally focus on a single domain, and adapt from the table-to-text datasets, resulting in hallucination issues. Thus, they are unsuitable for benchmarking LLMs in text-to-table.\n",
      "2.2 Large Language Models\n",
      "The advent of advanced LLMs,e.g., ChatGPT (OpenAI, 2022), GPT-4 (OpenAI, 2023), marks a pivotal\n",
      "moment that propels the field of NLP into a boom\n",
      "phase. Zhong et al. (2023) show that LLMs can\n",
      "achieve decent performance on benchmarks like\n",
      "GLUE (Wang et al., 2018), which spans eight representative NLP understanding tasks. Concurrently,\n",
      "several studies have scrutinized the performance\n",
      "of LLMs across various IE tasks. For example,\n",
      "Gao et al. (2023) test the capability of ChatGPT\n",
      "in event extraction. Similarly, González-Gallardo\n",
      "et al. (2023) employ ChatGPT on historical entity\n",
      "recognition. However, the results of these IE subtasks consistently show that LLMs underperform\n",
      "compared to state-of-the-art supervised approaches.\n",
      "The text-to-table task we focused on is more complex than the previous basic IE sub-tasks, however,\n",
      "it remains an unexplored area for evaluation on\n",
      "LLMs.\n",
      "3 CT-Eval\n",
      "In this section, we first discuss the data source for\n",
      "building CT-Eval (§ 3.1). Then, we give the details\n",
      "of how to control the hallucination in the preliminary collected data, including LLM hallucination\n",
      "judger (§ 3.2) and human cleaning processes (§ 3.3).\n",
      "Finally, we formulate the text-to-table task (§ 3.4)\n",
      "and provide the details of data statistics (§ 3.5).\n",
      "3.1 Data Source\n",
      "Following the success of WikiTableText (Bao et al.,\n",
      "2018), we also choose a multidisciplinary online\n",
      "encyclopedia as the data source to ensure data diversity. After carefully comparing existing Chinese\n",
      "online encyclopedias, we choose Baidu Baike2\n",
      ",\n",
      "which is one of the Chinese encyclopedias with\n",
      "the most entries in the world.\n",
      "We obtain the Baidu Baike data from the dumps\n",
      "provided by Xu et al. (2017). The data contains\n",
      "over 9M entity pages, each of which includes an\n",
      "infobox and the corresponding textual description,\n",
      "forming a text-to-table sample. Utilizing this data,\n",
      "we implement the following rule-based strategy\n",
      "for preliminary data cleaning: (1) Each page must\n",
      "contain at least one infobox; otherwise, the golden\n",
      "table is missing. (2) The number of tabular cells in\n",
      "the infobox should exceed three to ensure validity.\n",
      "(3) The length of the textual description should\n",
      "surpass 200 tokens. After that, 200K documenttable pairs are remaining for further processing.\n",
      "3.2 LLM Data Cleaning\n",
      "After the initial cleaning process, the hallucination issue persists due to Baidu Baike’s submission\n",
      "rules, wherein most of the page text and infoboxes\n",
      "are edited and maintained by individuals. Thus,\n",
      "the contents in the infoboxes may not always align\n",
      "precisely with the textual documents. For instance,\n",
      "some infoboxes may include additional knowledge\n",
      "unrelated to the text, potentially misleading the\n",
      "model from learning the text-to-table task.\n",
      "2\n",
      "https://baike.baidu.com/\n",
      "§ There is a document and a golden table that\n",
      "summarize the information contained in this\n",
      "document. Please help me identify whether the\n",
      "golden table contains additional information\n",
      "beyond the document. Give the reason first and\n",
      "then provide your judgment.\n",
      "Figure 1: Illustration of judgment prompt.\n",
      "To control the hallucination in the task samples,\n",
      "we decide to employ an LLM as a hallucination\n",
      "judger to filter out samples exhibiting hallucination.\n",
      "While using GPT-4 directly as the LLM hallucination judger is a straightforward approach, utilizing\n",
      "official APIs can be costly. Therefore, we first\n",
      "randomly select 5K samples from the data before\n",
      "cleaning. Then, we use GPT-4 to assess whether the\n",
      "golden tables contain additional information. The\n",
      "judgment prompt is illustrated in Figure 1, where\n",
      "the LLM is tasked with evaluating hallucination in\n",
      "a chain-of-thought manner. Next, we use the GPT4 judgment results to train an open-source LLM,\n",
      "employing the trained model to evaluate the remaining samples. Samples containing hallucinations are\n",
      "discarded. Given the capacity for understanding\n",
      "lengthy documents of existing Chinese LLMs (Bai\n",
      "et al., 2023b), we select ChatGLM3-6B-32k3\n",
      "as\n",
      "the open-source LLM hallucination judger. Finally,\n",
      "there are 88.6K samples after the data cleaning\n",
      "by the LLM hallucination judger. These samples\n",
      "totally cover 28 domains, e.g., physics and religion.\n",
      "3.3 Human Data Cleaning\n",
      "We split the LLM-cleaned samples into the training, validation, and test sets with 84.6K, 1K and\n",
      "1K samples, respectively. In the validation and test\n",
      "sets, we balance the number of samples in each\n",
      "domain to ensure a comprehensive evaluation. Furthermore, to mitigate the hallucination issue in the\n",
      "validation and test sets, we employ human annotators to manually cleanse their golden tables.\n",
      "In this phase, we employ five human annotators\n",
      "and one data expert, all of whom are native Chinese\n",
      "speakers with advanced educational qualifications.\n",
      "The data expert is a researcher with extensive experience in IE research. We first organize a tutorial\n",
      "for the five annotators to ensure alignment of annotation requirements. This includes clarifying the\n",
      "concept of the hallucination issue, emphasizing the\n",
      "additional information present in tables that cannot\n",
      "be directly extracted from the corresponding documents. Next, for each document-table pair in the\n",
      "3\n",
      "https://huggingface.co/THUDM/chatglm3-6b-32k\n",
      "CT-Eval\n",
      "Industry\n",
      "Informatics\n",
      "Physics\n",
      "Astronomy\n",
      "Traffic Engineering\n",
      "Biology\n",
      "Medical Science\n",
      "Culture\n",
      "Religion\n",
      "History\n",
      "Geography\n",
      "Education\n",
      "Business\n",
      "Finance\n",
      "Management\n",
      "Movie and Book\n",
      "Science Fiction\n",
      "Music\n",
      "Animation\n",
      "Dwelling\n",
      "Traveling\n",
      "Career\n",
      "Healthy\n",
      "Food\n",
      "Cosmetics\n",
      "Profile\n",
      "Security\n",
      "Military\n",
      "STEM\n",
      "24.0%\n",
      "Social Science\n",
      "60.9%\n",
      "Life\n",
      "Support\n",
      "11.6%\n",
      "O.\n",
      "3.5\n",
      "%\n",
      "Figure 2: Domain distribution in CT-EVAL\n",
      "validation and test sets, three annotators are asked\n",
      "to remove the hallucination information from tables. The data expert reviews 10% of the manually\n",
      "cleaned samples from each annotator. If the accuracy rate falls below 95%, the respective annotator\n",
      "will be required to redo the annotation. Finally,\n",
      "for each sample, if the three manually cleaned results are consistent, the results are saved as the final\n",
      "data. Otherwise, the results are decided by a group\n",
      "meeting among all annotators and the data expert.\n",
      "3.4 Task Overview\n",
      "Given a document D = {w1, w2,..., w|D|}, where\n",
      "wi\n",
      "is the i-th word in D, the text-to-table task aims\n",
      "to extract key information from D and outputs a table T = {c0,0, c0,1,..., c0,n, c1,0, c1,1,..., c1,n,...,\n",
      "ci,j,..., cm,n}, where c0,k(k ∈ {0, 1,..., n}) represents the column header and ck,0(k ∈ {0, 1,..., n})\n",
      "denotes as the row header. ci,j (i > 0, j > 0) represents the text of the cell in the i-th row and j-th\n",
      "column in the table with m rows and n columns.\n",
      "3.5 Data Statistics\n",
      "We compare CT-Eval with the previous datasets\n",
      "across several metrics including language, number of entries, average length, number of domains,\n",
      "hallucination rate, and average number of cells.\n",
      "As shown in Table 1, compared to previous\n",
      "datasets that mostly focus on one domain, CT-Eval\n",
      "covers 28 domains, offering a valuable resource\n",
      "to evaluate the text-to-table capabilities of LLMs\n",
      "across multiple domains. These 28 domains can\n",
      "be categorized into four branches: STEM, social\n",
      "science, life support and others. To provide a comprehensive understanding of domain coverage in\n",
      "CT-Eval, we present the distributions of each domain in Figure 2. Notably, the “Social Science”\n",
      "Original Item\n",
      "Translation\n",
      "材料 做法 厨师一点通\n",
      "炖牛肉时，可以放几个\n",
      "山楂进去，这样牛肉会\n",
      "烂得快些，而且有股山\n",
      "楂的清香。\n",
      "1.牛肉洗净放入清水锅\n",
      "中煮至七成熟，捞出切\n",
      "成方块\n",
      "2. …\n",
      "材料：\n",
      "牛肉，胡萝卜，\n",
      "白萝卜，大料，\n",
      "香叶\n",
      "东坡牛肉\n",
      "Original Item\n",
      "Ingredients Cooking method Cooking Tips\n",
      "When stewing beef, you\n",
      "can put a few hawthorn\n",
      "into it, so that the beef\n",
      "will rot faster and have a\n",
      "hawthorn fragrance.\n",
      "1. Wash the beef and\n",
      "put it into a pot of\n",
      "water to boil until\n",
      "it is seven mature,\n",
      "fish out and cut it\n",
      "into squares…\n",
      "2. …\n",
      "Ingredients:\n",
      "Beef, carrots, white\n",
      "radish, Chinese\n",
      "spices, allspice\n",
      "Dongpo\n",
      "Beef\n",
      "“标题”: “东坡牛肉”, “ ”章节“: [{”标题“: ”东坡牛肉的做法“},\n",
      "内容“: ”葱段、姜片、蒜头、辣椒、香叶、大料、盐、白糖、番茄酱、排骨\n",
      "酱、辣椒酱、高汤（用煮牛肉的汤即可）。\n",
      "做法：\n",
      "1、先来处理一下牛肉，\n",
      "将牛腩在冷水中多泡一会…“}\n",
      "Translation\n",
      "“Title”: “Dongpo Beef”, “ ”Chapter“: [{”Title “: ”Dongpo Beef Practice“}, :\n",
      "Ingredients：Diced green onion, ginger, garlic, chili pepper, coriander, dashi, salt,\n",
      "sugar, tomato sauce, rib sauce, chili sauce, broth. Cooking method:1, first, let's deal\n",
      "with the beef, soak the brisket in cold water for a while longer...\"}\n",
      "Figure 3: Text-to-table example from CT-EVAL.\n",
      "branch exhibits the highest proportion, encompassing domains such as culture and religion. Conversely, the “Other” branch, which includes topics\n",
      "related to cosmetics and profiles, represents the\n",
      "smallest portion. The “STEM” (science, technology, engineering, and mathematics) branch features\n",
      "data related to topics such as physical sciences and\n",
      "astronomy. The “Life Support” branch is highly\n",
      "relevant to everyday life, comprises 11.60% of the\n",
      "data, and primarily focuses on domains such as\n",
      "dwelling and health. An illustration of a data sample from CT-Eval is depicted in Figure 3. Each\n",
      "document-table pair, akin to the example shown,\n",
      "consists of a textual description and a golden table\n",
      "with varying numbers of columns and rows.\n",
      "To assess the quality of our dataset compared\n",
      "to previous ones, we randomly select 200 samples\n",
      "from the training, validation and test sets of CTEval, and previous E2E, Rotowire, WikiBio, and\n",
      "WikiTableText, respectively. Then, we compute the\n",
      "hallucination rate for each dataset through human\n",
      "annotation. Specifically, three annotators proficient\n",
      "in both English and Chinese judged whether the\n",
      "golden tables contained hallucination information\n",
      "following the guidance outlined in Section 3.3. The\n",
      "hallucination rate for each dataset is determined\n",
      "by the average proportion of hallucinated samples\n",
      "judged by all three annotators. We observe that\n",
      "the hallucination rate of the CT-Eval training set\n",
      "is 6.83%, similar to Rotowire (6.17%). Moreover,\n",
      "with the help of our human cleaning, the hallucination rates in our test and validation sets decrease to\n",
      "1.00% and 1.50%, respectively, significantly lower\n",
      "Dataset Language N. Entries Avg. Length N. Domains Hallu. R Avg. Cells\n",
      "E2E English 42,061 90.58 Restaurant 4.17% 4.46\n",
      "Rotowire English 3,398 1311.01 Sports 6.17% 40.49\n",
      "WikiBio English 582,659 416.71 Biography 8.67% 4.19\n",
      "WikiTableText English 10,000 59.76 Multiple Subclasses 18.83% 4.25\n",
      "CT-Eval(train) Chinese 84,603 911.46 28 Subclasses 6.83% 11.40\n",
      "CT-Eval(val.) Chinese 1,000 813.32 28 Subclasses 1.00% 10.78\n",
      "CT-Eval(test) Chinese 1,000 845.22 28 Subclasses 1.50% 10.86\n",
      "Table 1: Data Statistics of CT-Eval and previous text-to-table datasets. “N. Entries” denotes the number of entries.\n",
      "“Hallu. R” indicates hallucination rate. “Avg.Cells” indicates the average number of table cells\n",
      "than those in other datasets. Therefore, the reliability of our dataset can be verified. Regarding\n",
      "domains, the E2E, Rotowire and WikiBio datasets\n",
      "focus on single domains, whereas WikiTableText\n",
      "and CT-Eval encompass multiple domains to ensure data diversity. Concerning the length of the input documents, we note that previous E2E and WikiTableText primarily contain short documents with\n",
      "an average length of under 100 words, whereas documents in our dataset and Rotowire exceed words\n",
      "or characters, presenting more complex inputs for\n",
      "text-to-table models. In summary, CT-Eval stands\n",
      "as the sole dataset fulfilling criteria of data diversity,\n",
      "lengthy documents, and low hallucination.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is CT-Eval?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "CT-Eval is a Chinese text-to-table evaluation dataset, which is a benchmark for evaluating the performance of Large Language Models (LLMs) in extracting structured information from unstructured text. Specifically, it is a dataset designed to assess the ability of L\n",
      "GPU Usage After Inference:\n",
      "[4017, 7417] MiB\n",
      "Difference in GPU Memory Usage:\n",
      "[1134, 4468] MiB\n"
     ]
    }
   ],
   "source": [
    "clear_cache()\n",
    "\n",
    "# GPU usage before inference\n",
    "memory_before = get_gpu_memory()\n",
    "print(\"GPU Usage Before Inference:\")\n",
    "print(memory_before, \"MiB\")\n",
    "\n",
    "# Inference code here\n",
    "outputs = model.generate(\n",
    "    **input_ids,\n",
    "    max_new_tokens=50,\n",
    "    temperature=1.0,\n",
    ")\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "# GPU usage after inference\n",
    "memory_after = get_gpu_memory()\n",
    "print(\"GPU Usage After Inference:\")\n",
    "print(memory_after, \"MiB\")\n",
    "\n",
    "# Calculate the difference in memory usage\n",
    "memory_difference = [after - before for before, after in zip(memory_before, memory_after)]\n",
    "print(\"Difference in GPU Memory Usage:\")\n",
    "print(memory_difference, \"MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad88ac77",
   "metadata": {
    "papermill": {
     "duration": 0.026493,
     "end_time": "2024-05-21T21:21:30.199823",
     "exception": false,
     "start_time": "2024-05-21T21:21:30.173330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1256.884846,
   "end_time": "2024-05-21T21:21:33.718226",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-21T21:00:36.833380",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "04212b1122aa4cba9d5f5a458c61cc02": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0778772d2b5b445b9b90992c9915083c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ac9cc18422e34c19b15dff0aedfc28ab",
       "max": 654.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c99554b3c1a2450e8abcdf7dbfaedec0",
       "value": 654.0
      }
     },
     "07ed60c13a6145c492a6b565873e5153": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "0a720fc9af894025821f209627b31751": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0d550c75c280499fbce8bfa8f12d18d5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "0fc931ad92254a1da6ea717448b5020d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_78c46f6d4ad5434e8651d999e7fc7222",
       "placeholder": "​",
       "style": "IPY_MODEL_c896b0a18e9141b68122cb406cfef669",
       "value": "tokenizer.json: 100%"
      }
     },
     "10426588f2a64bcd9d13c381242d7765": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3c722a3a8a254dccaf2455d14b4e5fba",
       "max": 50977.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6e36b2c9f0294c75b83bb3dc085eec4f",
       "value": 50977.0
      }
     },
     "1071ec689a944ce9bd01de6a9a50127b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "12f2ae94b54c430da23aabfd75967564": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "163e20db82e64ac3bd7b710f300a2114": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "16cb0945ab0c4131a71ff92f3b8f73ef": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "18179aedfb4b46459d3224943b4ab5e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_04212b1122aa4cba9d5f5a458c61cc02",
       "placeholder": "​",
       "style": "IPY_MODEL_aa957f74a0fa4b199e86e5187d4751ba",
       "value": " 5.00G/5.00G [00:18&lt;00:00, 274MB/s]"
      }
     },
     "18f56726e3f0449a9c470cb6bad41209": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f2b4d2670e3948dca6a3f776678850fc",
       "placeholder": "​",
       "style": "IPY_MODEL_1071ec689a944ce9bd01de6a9a50127b",
       "value": " 4/4 [00:51&lt;00:00,  9.41s/it]"
      }
     },
     "1967f8c07dc24c95b51dfff19dc8b4fa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1b8cfd9c220d423589b7eb19aa98f25e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1d153dd40ee14723a787562ee52f6ebb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1eec993e6e4d411092700f157fba8c37": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7a8ab201934442d18450ce238ef09d66",
       "placeholder": "​",
       "style": "IPY_MODEL_ee59a4e4286a4d1f8081b77e2c10accf",
       "value": "Downloading shards: 100%"
      }
     },
     "2221a570bfd7455086a92ad8a8642237": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "22e53c9bb09b438c94ff25b574574374": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "234f8b85feb14d0989b9b2a93bed651b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2438a1d7401542d4897e3bafe8e595b3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "252a60dd28f44333bae75b37da097be9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "26b2614167d94885992650030b425b93": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d1420c557eb040d7968344b4971d515d",
        "IPY_MODEL_d2aca44412b44249903668df2084d778",
        "IPY_MODEL_9517bdca5ddd44d38bff6ba507f4e896"
       ],
       "layout": "IPY_MODEL_7511bb309b0e4125b5c9880414b92084"
      }
     },
     "2c51c3ce5fa8449a803b59671533661c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2ceb133e8f9f4266816a5dbdf12e9add": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2dc2dc0c0d0a40a3a1095a1f8656383e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2e844e843716487bad84056502e3bd11": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b485eccda9a0406f856944684412fb86",
       "placeholder": "​",
       "style": "IPY_MODEL_b0a7c65cd383434fb2c2ea2551e1cf79",
       "value": " 4.92G/4.92G [00:23&lt;00:00, 273MB/s]"
      }
     },
     "2fceca5898f5439aaf905e4dafabde2a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ea6cc738d7004a1a9af37f249383cac2",
       "max": 187.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_12f2ae94b54c430da23aabfd75967564",
       "value": 187.0
      }
     },
     "30e8e9094e964a1d8dda89c4edf8f976": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "34a6fad76c2541dda722d8547082cd5a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "35b97004e34e4644896ab6868cabfcb3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "35c8ec23dbe04017a6b9b150a94fb2d6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "36066fa0985547edb32f77523fdc568b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "3773ad7660a945659c9fa020e737c94e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "397362848a174a1aaa545d2d0a029fcd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3aae5791148f43d881eaa816065b59c1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3c722a3a8a254dccaf2455d14b4e5fba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3dd4ed04c94f41ddbfeee9ef893e963d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3aae5791148f43d881eaa816065b59c1",
       "max": 4.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_07ed60c13a6145c492a6b565873e5153",
       "value": 4.0
      }
     },
     "4020ca6adb9449e9832b3865ed15f59b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6e64aaed730b45c89b6d5cd79824ba99",
       "placeholder": "​",
       "style": "IPY_MODEL_5deaef1b45f04a2aa15f54d7f00ab81f",
       "value": " 51.0k/51.0k [00:00&lt;00:00, 3.18MB/s]"
      }
     },
     "418c6e305ce44e2a834fdb29fa445bb0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_dc47d06d2e944683af90041122a0ffaf",
       "placeholder": "​",
       "style": "IPY_MODEL_a952ea4add70469fa4607a20d515bcfb",
       "value": " 4/4 [00:16&lt;00:00,  3.71s/it]"
      }
     },
     "43be4379941748219078abe7e411cfe8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4daf7243ee53418693590f3caf46f325",
        "IPY_MODEL_6920ec1f3ac84bf9a5a3628e94a00e7c",
        "IPY_MODEL_18f56726e3f0449a9c470cb6bad41209"
       ],
       "layout": "IPY_MODEL_d9417fea4faa4777a2eb68e852d860f4"
      }
     },
     "455305a2598e4976b343c57359e75cce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "45d8c11dd94a4ea586ed7b45de17cb67": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4aacf5128659482987aa8617235277b2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4c6e5f59ecc94b7ea138f5b92d28f968": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_607f079ffa0446a09cac9221fffbda64",
        "IPY_MODEL_a6482b4cb7d74720801e98163d85eeb0",
        "IPY_MODEL_a0d350257de7468b8043f5d52ea89603"
       ],
       "layout": "IPY_MODEL_d27e2021b9494fe6932148809fb122e0"
      }
     },
     "4dad83436fb647dc8ee2db69ad622031": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_bc8d729d63934c78a5607c56ce13633d",
        "IPY_MODEL_10426588f2a64bcd9d13c381242d7765",
        "IPY_MODEL_4020ca6adb9449e9832b3865ed15f59b"
       ],
       "layout": "IPY_MODEL_5102186b15d94787acea2b9876a8f330"
      }
     },
     "4daf7243ee53418693590f3caf46f325": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_86208b01e35a46749b41f12e9de48cd4",
       "placeholder": "​",
       "style": "IPY_MODEL_234f8b85feb14d0989b9b2a93bed651b",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "5102186b15d94787acea2b9876a8f330": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "531b3a632afa454ba37fed08e0773282": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "534442a946db423bbb1436dc0376f725": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "53f726374005473797aae8106c8687ca": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "54db929edc0a4352a2efa4ace0d47f34": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_78b71de1559e42e5b9552c442b26b9d0",
        "IPY_MODEL_56aaaae3cc3145d8b2c01b8961bb280e",
        "IPY_MODEL_65a76bb0bfc4494385505218d5fbf16c"
       ],
       "layout": "IPY_MODEL_ab3ec807fe1e40a195ab2744279fb43e"
      }
     },
     "55b4ebcdbd4b4b2fa548eb376a76fb15": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f95d40a465584e9090584172a513ef10",
       "max": 9085698.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_35c8ec23dbe04017a6b9b150a94fb2d6",
       "value": 9085698.0
      }
     },
     "56aaaae3cc3145d8b2c01b8961bb280e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a50739b0475d4e7f92dbc0b8686681b3",
       "max": 23950.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8879657b099e4c849685235f2c80c7a3",
       "value": 23950.0
      }
     },
     "588c6f2ac597422bab87c707ddb8fa81": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "592543d135fe458a84253c23755371be": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_252a60dd28f44333bae75b37da097be9",
       "placeholder": "​",
       "style": "IPY_MODEL_839e8579b39d48749ba21914978363f5",
       "value": "config.json: 100%"
      }
     },
     "59bea925fcad4e49a0f82e1964f146f7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_30e8e9094e964a1d8dda89c4edf8f976",
       "placeholder": "​",
       "style": "IPY_MODEL_1d153dd40ee14723a787562ee52f6ebb",
       "value": " 4.98G/4.98G [00:22&lt;00:00, 267MB/s]"
      }
     },
     "5deaef1b45f04a2aa15f54d7f00ab81f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "5f52ef7c0dab40b78d102f4d36acb074": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5f9d1783736f42258645fb1b54e9c42c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "607f079ffa0446a09cac9221fffbda64": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0a720fc9af894025821f209627b31751",
       "placeholder": "​",
       "style": "IPY_MODEL_531b3a632afa454ba37fed08e0773282",
       "value": "special_tokens_map.json: 100%"
      }
     },
     "65a76bb0bfc4494385505218d5fbf16c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7a4238360e254c7b8c7785c48539b571",
       "placeholder": "​",
       "style": "IPY_MODEL_c513b754f52a4d18bbc86dd86244c04e",
       "value": " 23.9k/23.9k [00:00&lt;00:00, 1.95MB/s]"
      }
     },
     "69198fc2ab2a46c78576f0b79129f17e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6920ec1f3ac84bf9a5a3628e94a00e7c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_bc1d2f2fe7564643ac78b329f52ccba6",
       "max": 4.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_edb44606b6a5459c86bfc1a49849b1a6",
       "value": 4.0
      }
     },
     "6e36b2c9f0294c75b83bb3dc085eec4f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6e64aaed730b45c89b6d5cd79824ba99": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "72932046ccfa43ae82175b4d3302d19c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "73f89043c5ce47c68d49df6e52e4acec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_534442a946db423bbb1436dc0376f725",
       "placeholder": "​",
       "style": "IPY_MODEL_1b8cfd9c220d423589b7eb19aa98f25e",
       "value": " 187/187 [00:00&lt;00:00, 15.3kB/s]"
      }
     },
     "7511bb309b0e4125b5c9880414b92084": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7670a4efebbd4bb094dca298a2933ae3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f7138c4ad36d405ba9f5b60991ec6bf7",
       "max": 4915916176.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_78f3c31072114e84b84873183559cfe4",
       "value": 4915916176.0
      }
     },
     "7695540426ea4198a5653bd7048ba291": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e89d3ae6be85410aba38d95ef59a09cb",
       "max": 4976698672.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_45d8c11dd94a4ea586ed7b45de17cb67",
       "value": 4976698672.0
      }
     },
     "78b71de1559e42e5b9552c442b26b9d0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8ddedc2546864b8fac9007611794e9b5",
       "placeholder": "​",
       "style": "IPY_MODEL_bdd3e5ce16ec4dfc8c68cd08081f6ef0",
       "value": "model.safetensors.index.json: 100%"
      }
     },
     "78c46f6d4ad5434e8651d999e7fc7222": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "78f3c31072114e84b84873183559cfe4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7a4238360e254c7b8c7785c48539b571": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7a8ab201934442d18450ce238ef09d66": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8040874352fe409cb21a506d751410ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "839e8579b39d48749ba21914978363f5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "86208b01e35a46749b41f12e9de48cd4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "881d7fa2723f4670acc2ab41de3b712a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1967f8c07dc24c95b51dfff19dc8b4fa",
       "placeholder": "​",
       "style": "IPY_MODEL_72932046ccfa43ae82175b4d3302d19c",
       "value": "generation_config.json: 100%"
      }
     },
     "8879657b099e4c849685235f2c80c7a3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "8ddedc2546864b8fac9007611794e9b5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8e41baff9eed4eac8c265ad7637958db": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1eec993e6e4d411092700f157fba8c37",
        "IPY_MODEL_f8063675476f4fde998ed535636f338f",
        "IPY_MODEL_d38b214c95ae41bbb7c13f672eb1c115"
       ],
       "layout": "IPY_MODEL_2ceb133e8f9f4266816a5dbdf12e9add"
      }
     },
     "8f28bef7223b433783ee12a9e56a46c4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "90774a4511414330a5df0e8ccc6edee6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c24ade5d3aec4be1a0bcbeb2cd39e0ff",
        "IPY_MODEL_3dd4ed04c94f41ddbfeee9ef893e963d",
        "IPY_MODEL_418c6e305ce44e2a834fdb29fa445bb0"
       ],
       "layout": "IPY_MODEL_b415d35503c2487dbaff0ad5eee5042d"
      }
     },
     "9253280d05e540b2a1e11505148edeb6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_fc5b8edfa0eb4bafbce461fd93243d9b",
        "IPY_MODEL_c1e18ccecb2d41a6b571bccdcc8443d2",
        "IPY_MODEL_18179aedfb4b46459d3224943b4ab5e4"
       ],
       "layout": "IPY_MODEL_397362848a174a1aaa545d2d0a029fcd"
      }
     },
     "9517bdca5ddd44d38bff6ba507f4e896": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f4cccf84db924db48f4887b9d8a2e8ba",
       "placeholder": "​",
       "style": "IPY_MODEL_2221a570bfd7455086a92ad8a8642237",
       "value": " 1.17G/1.17G [00:04&lt;00:00, 269MB/s]"
      }
     },
     "a0d350257de7468b8043f5d52ea89603": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_16cb0945ab0c4131a71ff92f3b8f73ef",
       "placeholder": "​",
       "style": "IPY_MODEL_36066fa0985547edb32f77523fdc568b",
       "value": " 73.0/73.0 [00:00&lt;00:00, 5.68kB/s]"
      }
     },
     "a446846ba4e34d8c926170c4fa286eec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5f52ef7c0dab40b78d102f4d36acb074",
       "placeholder": "​",
       "style": "IPY_MODEL_0d550c75c280499fbce8bfa8f12d18d5",
       "value": " 9.09M/9.09M [00:00&lt;00:00, 28.6MB/s]"
      }
     },
     "a50739b0475d4e7f92dbc0b8686681b3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a6482b4cb7d74720801e98163d85eeb0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_588c6f2ac597422bab87c707ddb8fa81",
       "max": 73.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8f28bef7223b433783ee12a9e56a46c4",
       "value": 73.0
      }
     },
     "a952ea4add70469fa4607a20d515bcfb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "aa957f74a0fa4b199e86e5187d4751ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ab3ec807fe1e40a195ab2744279fb43e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ac9cc18422e34c19b15dff0aedfc28ab": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "acb38c154ca940eba782e3f6a10b9762": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b0a7c65cd383434fb2c2ea2551e1cf79": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b2aef5e638854885bbd37fb1ef5ad057": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b375a867b65945a5aef30d747b45d908": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b415d35503c2487dbaff0ad5eee5042d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b485eccda9a0406f856944684412fb86": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bc1d2f2fe7564643ac78b329f52ccba6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bc8d729d63934c78a5607c56ce13633d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3773ad7660a945659c9fa020e737c94e",
       "placeholder": "​",
       "style": "IPY_MODEL_eea6fd59869749dc9f4ceb39093fa1bd",
       "value": "tokenizer_config.json: 100%"
      }
     },
     "bdd3e5ce16ec4dfc8c68cd08081f6ef0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c1e18ccecb2d41a6b571bccdcc8443d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_acb38c154ca940eba782e3f6a10b9762",
       "max": 4999802720.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b2aef5e638854885bbd37fb1ef5ad057",
       "value": 4999802720.0
      }
     },
     "c24ade5d3aec4be1a0bcbeb2cd39e0ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ff6b7188aec04e218428581014e5aad7",
       "placeholder": "​",
       "style": "IPY_MODEL_fb08e8b48d9c486d9473999ddb6fbb41",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "c513b754f52a4d18bbc86dd86244c04e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c69bedb5c4d74f44b383109b7469dc47": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f2edd3ca80874f83927e638b9f20bd26",
        "IPY_MODEL_7695540426ea4198a5653bd7048ba291",
        "IPY_MODEL_59bea925fcad4e49a0f82e1964f146f7"
       ],
       "layout": "IPY_MODEL_163e20db82e64ac3bd7b710f300a2114"
      }
     },
     "c767150ffbe54411950d25040b1d756b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c896b0a18e9141b68122cb406cfef669": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c99554b3c1a2450e8abcdf7dbfaedec0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d1420c557eb040d7968344b4971d515d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e414bb5822cc4e349d40e6a44d188639",
       "placeholder": "​",
       "style": "IPY_MODEL_455305a2598e4976b343c57359e75cce",
       "value": "model-00004-of-00004.safetensors: 100%"
      }
     },
     "d27e2021b9494fe6932148809fb122e0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d2aca44412b44249903668df2084d778": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e0c0e124f30a4025b783d313baac4556",
       "max": 1168138808.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_35b97004e34e4644896ab6868cabfcb3",
       "value": 1168138808.0
      }
     },
     "d38b214c95ae41bbb7c13f672eb1c115": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_fedf25e110844fce9000b9c4016d767b",
       "placeholder": "​",
       "style": "IPY_MODEL_22e53c9bb09b438c94ff25b574574374",
       "value": " 4/4 [01:09&lt;00:00, 14.92s/it]"
      }
     },
     "d5e87e4d6455490080277ff1a48e972a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d6bf0890ca054903a29841b355e70c0f",
        "IPY_MODEL_7670a4efebbd4bb094dca298a2933ae3",
        "IPY_MODEL_2e844e843716487bad84056502e3bd11"
       ],
       "layout": "IPY_MODEL_f718cd102f7b4e0aaf14a176ffab8045"
      }
     },
     "d6bf0890ca054903a29841b355e70c0f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2c51c3ce5fa8449a803b59671533661c",
       "placeholder": "​",
       "style": "IPY_MODEL_d82e2efd78b5457085d32533ce0fe47e",
       "value": "model-00003-of-00004.safetensors: 100%"
      }
     },
     "d82e2efd78b5457085d32533ce0fe47e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d9417fea4faa4777a2eb68e852d860f4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dc47d06d2e944683af90041122a0ffaf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e0c0e124f30a4025b783d313baac4556": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e414bb5822cc4e349d40e6a44d188639": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e89d3ae6be85410aba38d95ef59a09cb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ea6cc738d7004a1a9af37f249383cac2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ead7ef6f172d403e9e7e8612e9410cd9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_592543d135fe458a84253c23755371be",
        "IPY_MODEL_0778772d2b5b445b9b90992c9915083c",
        "IPY_MODEL_f8be56e619ff41a985bf94812d111b2d"
       ],
       "layout": "IPY_MODEL_34a6fad76c2541dda722d8547082cd5a"
      }
     },
     "ec376bb70d314fbfb53a0c7172d85ef3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0fc931ad92254a1da6ea717448b5020d",
        "IPY_MODEL_55b4ebcdbd4b4b2fa548eb376a76fb15",
        "IPY_MODEL_a446846ba4e34d8c926170c4fa286eec"
       ],
       "layout": "IPY_MODEL_c767150ffbe54411950d25040b1d756b"
      }
     },
     "edb44606b6a5459c86bfc1a49849b1a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ee59a4e4286a4d1f8081b77e2c10accf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "eea6fd59869749dc9f4ceb39093fa1bd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f2b4d2670e3948dca6a3f776678850fc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f2edd3ca80874f83927e638b9f20bd26": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_53f726374005473797aae8106c8687ca",
       "placeholder": "​",
       "style": "IPY_MODEL_f4ce52260a4748939515e11413edc849",
       "value": "model-00001-of-00004.safetensors: 100%"
      }
     },
     "f4cccf84db924db48f4887b9d8a2e8ba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f4ce52260a4748939515e11413edc849": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f7138c4ad36d405ba9f5b60991ec6bf7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f718cd102f7b4e0aaf14a176ffab8045": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f8063675476f4fde998ed535636f338f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2438a1d7401542d4897e3bafe8e595b3",
       "max": 4.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8040874352fe409cb21a506d751410ec",
       "value": 4.0
      }
     },
     "f8be56e619ff41a985bf94812d111b2d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5f9d1783736f42258645fb1b54e9c42c",
       "placeholder": "​",
       "style": "IPY_MODEL_69198fc2ab2a46c78576f0b79129f17e",
       "value": " 654/654 [00:00&lt;00:00, 56.6kB/s]"
      }
     },
     "f95d40a465584e9090584172a513ef10": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fb08e8b48d9c486d9473999ddb6fbb41": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "fc5b8edfa0eb4bafbce461fd93243d9b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2dc2dc0c0d0a40a3a1095a1f8656383e",
       "placeholder": "​",
       "style": "IPY_MODEL_b375a867b65945a5aef30d747b45d908",
       "value": "model-00002-of-00004.safetensors: 100%"
      }
     },
     "fde883abf9a24945ad31cd4f2475e55c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_881d7fa2723f4670acc2ab41de3b712a",
        "IPY_MODEL_2fceca5898f5439aaf905e4dafabde2a",
        "IPY_MODEL_73f89043c5ce47c68d49df6e52e4acec"
       ],
       "layout": "IPY_MODEL_4aacf5128659482987aa8617235277b2"
      }
     },
     "fedf25e110844fce9000b9c4016d767b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ff6b7188aec04e218428581014e5aad7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
